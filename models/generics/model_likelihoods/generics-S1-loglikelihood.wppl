// time ~/webppl-fork/webppl generics-S1-loglikelihood.wppl --require ../node_modules/utils generic 1

var chain = last(process.argv) // load index as last command line index


var iterationTests = [
  200000, 300000, 400000, 500000
]
var totalIterations = iterationTests[chain % iterationTests.length];
// penultimate argument is the semantics
// generic = uncertain threshold
// some = fixed threshold at lowest threshold value
var targetUtterance = process.argv[process.argv.length - 2]

var responseDictionary = { "agree-key": 1, "disagree-key": 0 };

var dataPath = "../../../data/generics/endorsement/"
var priorFile = dataPath + "naturalGenerics-prior-trials-n57.csv",
 		endorsementFile = dataPath + "truth-judgments-n100.csv",
		endorsementCatchFile = dataPath + "truth-judgments_catch-trials.csv"

var d_prior = dataFrame(utils.readCSV(priorFile).data, ["prevalence"]),
 		d_endorsement = dataFrame(utils.readCSV(endorsementFile).data),
		d_catch = dataFrame(utils.readCSV(endorsementCatchFile).data, ["pass"]);

var data = {
	prior: map(function(d){
		extend(d, {
			avoided_endval: avoidEnds(d.prevalence/100)
		})
	}, d_prior),
	endorsement: filter(function(di){
		var catchData = _.filter(d_catch, {workerid: di.workerid});
		return catchData[0].pass == 1;
	}, map(function(d){
			extend(d, {
				binaryResponse: responseDictionary[d.response]
			})
	}, d_endorsement))
};

// test that removing Ss who failed catch trial works properly
assert.ok(
	levels(data.endorsement, "workerid").length == sum(_.map(d_catch, "pass"))
)

var utterancePrior = Infer({model: function(){return uniformDraw([targetUtterance,"silence"])}});

var meaning = function(utt,state, theta) {
  return utt === "generic" ? state > theta :
         utt === "generic is false" ? state<=theta :
         utt === "silence" ? true :
         utt === "some" ? state > _.min(thetaBins) :
         utt === "most" ? state >= 0.5:
         utt === "all" ? state >= 0.99:
         true
}

var properties = levels(data.endorsement, "Property")

// should be 21 properties
assert.ok(
  properties.length == 21
)

var nullParams = {a:1, b:100}, nullDistribution = Beta(nullParams);

var addGuessing = function(dist, noise){
  return Infer({model: function(){
    return flip(noise) ? uniformDraw([0,1]) : sample(dist)
  }})
}

// PRIORS
var UnitUniform = Uniform({a: 0, b: 1});
var speakerOptimalityPrior = Uniform({a:0, b:10}),
    priorThetaPrior = UnitUniform,
    priorBetaGPrior = UnitUniform,
    priorBetaDPrior = Uniform({a: 0, b: 100}),
    prevalenceParamsPriors = {
      g: Uniform({a: 0, b: 1}),
      d: Uniform({a: 0, b: 100})
    },
    noiseParamPrior = UnitUniform;


var model = function(){

	var speakerOptimality = {
		s1: sample(speakerOptimalityPrior)
	};

  var noiseParam = sample(noiseParamPrior);
  // var globalLikelihoods = speakerOptimalityPrior.score(speakerOptimality.s1) +
                          // noiseParamPrior.score(noiseParam);

  // query.add(["logLikelihood","globals","NA", "NA"], globalLikelihoods)

	foreach(properties, function(p){

		var propertyData = {
			endorsement: _.filter(data.endorsement, {Property: p}),
			prior: _.filter(data.prior, {Property: p})
		}
    // make sure there's data
    assert.ok(
      (propertyData.prior.length > 0) && (propertyData.endorsement.length > 0)
    )

		// prior parameters
		var theta = sample(priorThetaPrior)
      //  thetaLike = priorThetaPrior.score(theta);
		var betaParams = {
			g: sample(priorBetaGPrior),
			d: sample(priorBetaDPrior)
		}
      // betaLike = priorBetaGPrior.score(betaParams.g) +
      //            priorBetaDPrior.score(betaParams.d);

  //  query.add(["logLikelihood","itemPrior", p, "NA"], thetaLike + betaLike)


		var priorParams = betaShape(betaParams);
    // display("before prevalence prior factor")
		// observe structured prior data
		mapData({data: propertyData.prior}, function(d){
			// display(d.roundedPrevalence)
			// display(Delta({v: 0}).score(d.roundedPrevalence))
			// display(Beta(priorParams).score(d.roundedPrevalence))
			var scr = util.logsumexp([
				 Math.log(theta) + Beta(priorParams).score(d.avoided_endval),
				 Math.log(1-theta) + nullDistribution.score(d.avoided_endval)
				//  Math.log(1-theta) + Delta({v: 0}).score(d.roundedPrevalence)
			 ])
			 factor(scr)
		})
    // display("after prevalence prior factor")

		var statePrior = Infer({model: function(){
			sample(flip(theta) ? DiscretizedBeta(priorParams) : DiscretizedBeta(nullParams))
		}});

		/// RSA model
		var listener0 = cache(function(utterance) {
		  Infer({model: function(){
		    var state = sample(statePrior)
				var theta = targetUtterance === "generic" ? sample(thetaPrior) : -99;
		    var m = meaning(utterance, state, theta)
		    condition(m)
		    return state
		 }})}, 10000)

		var speaker1 = cache(function(speakerBeliefs) {
			Infer({model: function(){
		    var utterance = sample(utterancePrior);
		    var L0 = listener0(utterance);
				factor(speakerOptimality.s1 * L0.score(speakerBeliefs))
		    return utterance === targetUtterance ? 1 : 0
			}})}, 10000)

		var categories = levels(propertyData.endorsement, "Category");

		// make sure subsetting works properly
		assert.ok(categories.length > 0)

		foreach(categories, function(k){

			var categoryData = {
				endorsement: _.filter(propertyData.endorsement, {Category: k}),
				prior: _.filter(propertyData.prior, {Category: k})
			};

      // make sure there is data
      assert.ok(
        (categoryData.prior.length > 0) && (categoryData.endorsement.length > 0)
      )
      // display(_.map(categoryData))
			// var theta_target = uniformDrift({a: 0, b: 1, width:0.2})

      var withinParams = {
        g: sample(prevalenceParamsPriors.g),
        d: sample(prevalenceParamsPriors.d)
      };

    //   var itemPrevalanceLike = prevalenceParamsPriors.g.score(withinParams.g) +
    //                            prevalenceParamsPriors.d.score(withinParams.d);
     //
    //  query.add(["logLikelihood","itemPrevalanceLike", p, k], itemPrevalanceLike)

			var withinShape = betaShape(withinParams);
			// var withinKind_params = {
			// 	a: uniformDrift({a: 1, b: 100, width: 2}),
			// 	b: uniformDrift({a: 1, b: 100, width: 2})
			// };

			mapData({data: categoryData.prior}, function(d){
				// display(p+k+ "d = " + d.avoided_endval + " " + Beta(withinShape).score(d.avoided_endval))
				observe(Beta(withinShape), d.avoided_endval)
			})
      // display("after prevalence factor")

			var speakerBeliefs = sample(DiscretizedBeta(withinShape));

			// var speakerBeliefs = Infer({model: function(){
			// 	return categorical({
			// 			vs:bins,
			// 			ps:map(function(b) {
			// 				return probability(Binomial({n:100,
			// 					p: withinKind_prev}), Math.round(b*100)) +
      //     Number.EPSILON
			// 				}, bins )
			// 		})
			// }});
      // display(speakerBeliefs)
			var s1prediction = speaker1(speakerBeliefs);
      var s1withNoise = addGuessing(s1prediction, noiseParam);

			var responseData = _.map(categoryData.endorsement, "binaryResponse")

      var logLike = sum(map(function(d){
        return s1withNoise.score(d)
      }, responseData))

      query.add(["logLikelihood","genericsData", p, k], logLike)

		})

	})

	return query
}

// 1500 iterations / min (including burn-in)
// var totalIterations = 100000,
var lag = 10;
var mhiter = totalIterations/lag, burn = totalIterations / 2;
var effChain = (chain - chain%iterationTests.length ) / terationTests.length

var outfile = 'results-marginalLikelihood-testIterations-S1-'+'smntcs_'+targetUtterance+"-"+ totalIterations+'_burn'+burn+'_lag'+lag+'_chain'+effChain+'.csv'

var posterior = Infer({
  model: model,
	method: "incrementalMH",
  samples: mhiter, burn: burn, lag: lag,
  verbose: T,
  verboseLag: mhiter / 60,
	stream: {
		path: "results/" + outfile,
		header: ["type", "param", "property", "category", "val"]
	}
})

"written to " + outfile;
