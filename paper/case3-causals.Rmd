# Case Study 3: Causal Language

Learning to that an event consistently tends to result in another event is a critical kind of generalization for understanding how the world works. 
Adults and children can infer causal relationships from just a few examples [@Gopnik2000; @Schulz2004] and computational models have been used to formalize the problem of *elemental causal induction*, or learning a causal relationship from observational data [@Griffiths2005; @Kemp2010a].
Having induced a causal relationship from observational data, people can communicate their causal knowledge using language. 
Indeed, when causal relations are nonobvious, causal language helps children as young as 2-years learn those relations [@Bonawitz2010].


<!-- Such *token causation*  -->
<!-- serves as a foundation for an intuitive theory of causality [@Goodman2011; @Kemp2010a]. -->
<!-- Recognizing an event as *causal* is a challenging problem [e.g., @Gerstenberg2015how]. -->
<!-- Inducing a generalization  -->
<!-- Communicating generalizations about causally related events is highly valuable because  -->
<!-- relevant data can be prohibitively expensive and no one individual can run all the experiments that she wants to run in a life time.  -->
<!-- The history of civilizations has shown that it is incredibly useful to divide labor: Some members can investigate modifications to the food preparation procedures while others can examine different ways of cutting stone.  -->
<!-- Our framework for communicating generalizations makes predictions about statements conveying genericity about a category of causal events.  -->
<!-- In this domain, an instance of the category is a single causal event, or *token causality* (e.g., adding a particular amount of a particular kind of yeast to an actual piece of dough).  -->
<!-- <!-- [Gerstenberg] -->
<!-- Inducing the existence of an abstract cause from observational evidence also depends in complex ways on an observer's background theory of causality [e.g., @Griffiths2005; @Schulz2008a]. -->
<!-- Language provides a way to bypass the observational data needed to make a causal induction. -->

In this paper, we posit that feature-probability priors are a mediating representation between abstract conceptual structure and generalizations in language.
In this last set of experiments, we explicitly test the relationship between the feature-probability priors and endorsements of generalizations about causes.
Also, by designing the experiments in the domain of causal language, we further demonstrate the generality of our theory of communicating generalizations.

## Experiment 3a: Manipulating priors for causes

In this experiment, we manipulate and measure participants' background knowledge in order to check whether or not manipulation of participants' background knowledge was successful. 
Experiment 3b (*causal endorsement*) will then follow a mostly-identical experimental procedure.

<!-- examine causal domains that vary as to whether or not probabilistic causes are plausible and whether or not a background cause of the event is intuitively present. -->
<!-- We introduce participants to the results of several experiments about each domain and see how different experimental results update participants' beliefs about causal power in these domains. -->

<!-- People have theories about how different casusal systems work.  -->
<!-- Physical causation tends to have a deterministic bend to it: If one billiard ball hits a second with a certain force, the second will respond by moving with a certain force.  -->
<!-- If the experiment is repeated exactly, we would expect the same results. -->
<!-- In other domains, probabilistic causes are plausible: Giving a sick animal a certain medication may sometimes make it better and other times it won't.  -->
<!-- Finally, some events could have multiple possible causes: Testing whether or not a drug makes an animal *blink* might be difficult because animals will tend to *blink* even without intervention.  -->



### Method

```{r causal-priors-data}
d.cas.priors <- read.csv(paste(project.path,
                           "data/causals/priors/pilot-causals-7-prior-trials.csv",
                           sep= ""))

n.subj.cas <- length(unique(d.cas.priors$workerid))

ave.minutes.cas <- round(mean(unique(d.cas.priors %>% select(workerid, rt))$rt) / 1000 / 60, 1)
```

```{r causal-priors-runModel, cache = T}
causal.distributions <- levels(factor(d.cas.priors$distribution))
p <- causal.distributions[1]
m.cas.marginal.prevalence <- data.frame()
m.cas.endorse.priors.summary <- data.frame()

for (p in causal.distributions){
  priorData <- 100*filter(d.cas.priors, distribution == p)$response
  
  m.cas.endorse.priors <- webppl(genericEndorsementPriorModel, data = priorData, data_var = "data",
         model_var = "model", inference_opts = list(method = "MCMC", samples = 5000, burn = 2500, verbose = T))
  
  m.cas.marginal.prevalence <- bind_rows(m.cas.marginal.prevalence, m.cas.endorse.priors %>% 
    spread(Parameter, value) %>%
    rowwise() %>%
    mutate(
      a = g * d,
      b = (1 - g) * d,
      stable = rbinom(n = 1, size = 1, prob = phi),
      prevalence = ifelse(stable == 1, 
                             rbeta(n = 1, shape1 = a, shape2 = b),
                             rbeta(n = 1, shape1 = 1, shape2 = 100)),
      distribution = p
    )
  )
  
  m.cas.endorse.priors.summary <- bind_rows(m.cas.endorse.priors.summary,
                                            m.cas.endorse.priors %>% 
                                              mutate(distribution = p) %>%
                                              group_by(distribution, Parameter) %>%
                                              summarize(MAP = estimate_mode(value),
                                                        cred_upper = hdi_upper(value),
                                                        cred_lower = hdi_lower(value))
                                              )
}

```



#### Participants 

We recruited `r n.subj.cas` participants from Amazon's Mechanical Turk.
Participants were restricted to those with U.S. IP addresses and who had at least a 95\% work approval rating.
The experiment took on average `r ave.minutes.cas` minutes and participants were compensated \$0.50 for their work.


<!-- The different kinds of system are described in more detail below. -->
<!-- Participants are then told that the team of scientists is conducting experiments with different possible causes (e.g., different kinds of foods). -->

#### Materials

Participants were told a story of a scientific experiment testing different substances to produce an effect (either to make animals sleepy or plants grow tall). 
Our cover stories were constructed so that the potential cause could have some plausible intuitive mechanism that could give rise to the proprety (e.g., a naturally occurring herb causing animals to be sleepy). 
The two cover stories can be seen in Table \@ref(tab:causalItems).

Participants were then shown "previous experimental results", which followed one of four distributions, shown in Figure \@ref(fig:causals-conditions). 
In two of the conditions, participants saw results that were approximately all the same across the various experiments. 
In one of these conditions, all causes produced a strong effect (average efficacy approximately 98\%; the "common deterministic" condition).
In the second of these conditions, all causes produced a weak effect (average efficacy approximately 20\%; the "common weak" condition).
These distributions had the feeling that either there was some background cause which gave rise to the effect (e.g., a simple base rate) or that all substances were equally as efficacious at producing the effect.

The two other conditions used distributions in which some experiments resulted in either no or very few succeses (i.e., produced 0s or 1s or 2s), and others that either had strong or weak effects as above. 
These are the "rare deterministic" and "rare weak" conditions, because the very existence of a greater than zero effect was "rare".
These distributions had the feeling that the effect was somewhat rare and that only certain substances produced any effect.

\begin{table}[]
\centering
\label{tab:causalMaterials}
\resizebox{\textwidth}{!}{
\begin{tabular}{lll}
                   & Plants                                                                                                                                                                                                  & Animals                                                                                                                                                                                                                               \\
Cover story        & On this planet, there is a plant called feps and your team wants to figure out how to make these plants grow tall. Your team runs experiments trying to make feps grow tall with different fertilizers. & On this planet, there are animals called cheebas and your team of scientists wants to figure out how to make these animals sleepy. Your team runs experiments trying to make cheebas sleepy with different naturally occurring herbs. \\
Evidence statement & Your team gave fertilizer B to 100 different feps. Of those 100 treated, 2 feps grew tall.                                                                                                              & Your team gave herb C to 100 different cheebas. Of those 100 treated, 98 cheebas were made sleepy.                                                                                                                                   
\end{tabular}
}
\caption{Cover stories and evidence statements for the two sets of materials used in Expt. 3a\&b}
\end{table}


```{r causalExpt, fig.cap="Overview of Experiment 3. A: Results of previous experiments are shown one at a time. One of the results were lost. B: Particiapnts are asked to review results before continuing. C: Prior elicitation task. D: Causal endorsement task."}
knitr::include_graphics("figs/causal-expt.pdf", dpi = 108)
```

#### Procedure

Each participant saw only one cover story with one distribution of previous experiments (i.e., the experiment was a single trial).
<!-- In the first part of the trial, participants learned about different substances' ability to produce an effect ("previous experimental results"). -->
<!-- Participants read a cover story and clicked a button to show the results of successive previous experiments. -->
<!-- For each previous experiment, participants were presented with an *evidence statement* as well as a numerical representation of the results shown in a table above the text. -->
<!-- The table was filled in gradually with each successive experiment.  -->
<!-- The results of one experiment was said to be lost, and a "?" was placed in the table (Figure \@ref(fig:causalExpt)).  -->
<!-- (These lost results would be found in the *causal endorsement* task, Expt. 4b). -->
<!-- Participants were instructed to take a final look at the results of all of the previous experiments before continuing.  -->
<!-- Once participants clicked the "continue" button, they were brought to a new screen where they would make predictions about new substances that have yet to be tested (Figure \@ref(fig:causalExpt)). -->
<!-- Participants saw a list of new substances and slider bars that ranged from 0 to 100.  -->
<!-- The second phase serves as a measurement of participants' feature-probability priors. -->

Participants were told that they were an astronaut-scientist on a distant planet trying to figure out how some system works (how to make a certain kind of animal sleepy with different herbs or how to make a plant grow tall with different fertilizers). The story for the "sleepy animals" condition read:

\begin{quotation}
You are an astronaut-scientist exploring a distant planet. 
On this planet, there are animals called cheebas and your team of scientists wants to figure out how to make these animals sleepy.
Your team runs experiments trying to make cheebas sleepy with different naturally occurring herbs.
The results are shown below:
\end{quotation}

Participants then must click a button to show the results of the experiments.
Experiment results appear one at a time (upon a click), and are described in an *evidence statement* (e.g., "Your team gave herb A to 100 different cheebas. Of those 100 treated, 98 cheebas were made sleepy.") as well as displayed in a table showing the number of successes per number of attempts (always 100 per experiment). 
Eleven "experiments" are shown, though the results of one experiment were said to be lost and a "?" was placed in the table (Figure \@ref(fig:causalExpt)). 
(These lost results would be found in the *causal endorsement* task, Expt. 3b).
After participants view the results of the 10 "experiments" (and 1 missing experiment), they are told to review the results of the experiments before continuing. 

Upon clicking the continue button, the table of "experiment results" is removed and participants are told more experiments were conducted.
Participants were asked to predict the results of the next 5 experiments.
Participants were given 5 slider bars ranging from 0 - 100, and asked to predict the next five substances (e.g., herbs M, N, P, Q, and R).

After responding, participants then completed an attention check survey where they were asked what the team of scientists were investigated (choosing a response from a drop-down menu with 12 options) and to input one of the numerical results they saw on the previous screen. 
This attention check served to confirm that participants had encoded some part of both relevant aspects of the experiment (the domain and the frequencies). 


```{r figure-causals-priors, fig.caption = "Four background distributions participants were exposed to in Expt. 3 (stimuli) and priors reconstructed from participants' responses (model). ", fig.width = 8, fig.height = 3, eval = F}
sample.dists <- bind_rows(
  data.frame(
    dist = "rare weak",
    x = c(rbeta(n = 3000, shape1 = 2, shape2 = 10),
          rbeta(n = 7000, shape1 = 2, shape2 = 50)
          )
  ),
  data.frame(
    dist = "common weak",
    x = rbeta(n = 10000, shape1 = 2, shape2 = 10)
  ),
  data.frame(
    dist = "rare deterministic",
    x = c(rbeta(n = 3000, shape1 = 50, shape2 = 2),
          rbeta(n = 7000, shape1 = 2, shape2 = 50)
          )
  ),
  data.frame(
    dist = "common deterministic",
    x = rbeta(n = 10000, shape1 = 50, shape2 = 2)
  )
) %>%
    mutate(Distribution = factor(dist, 
                               levels = c("common deterministic", "rare deterministic", "common weak", "rare weak"),
         labels = c("Common Deterministic", "Rare Deterministic",
                    "Common Weak", "Rare Weak")))
  
figure.causals.priors <- bind_rows(
  sample.dists %>%
    mutate(src = "stimuli") %>%
    select(-dist),
  m.cas.marginal.prevalence %>%
    filter(distribution %in% c("common_deterministic", "rare_deterministic", "common_weak", "rare_weak")) %>%
    mutate(Distribution = factor(distribution, 
                                 levels = c("common_deterministic", "rare_deterministic", "common_weak", "rare_weak"),
           labels = c("Common Deterministic", "Rare Deterministic",
                      "Common Weak", "Rare Weak"))) %>%
    select(Distribution, prevalence) %>%
    mutate(src = 'model') %>%
    rename(x = prevalence)
) %>%
  mutate(src = factor(src, levels = c("stimuli", "model"))) %>%
  ggplot(., aes(x = x, fill = Distribution, color = Distribution, alpha = src, lty = src
                ))+ 
  geom_density( adjust = 1.3, 
               size = 1.2,
               aes( y = ..scaled..))+ #
  scale_x_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 1))+
  #scale_fill_manual(values = c("#636363", "#d7191c"))+#, "#2b83ba"))+
  scale_fill_solarized()+scale_color_solarized()+
  #scale_color_manual(values = c("#636363", "#d7191c"))+
  scale_alpha_manual(values = c(0.1, 0.4))+
  scale_linetype_manual(values = c(2, 1))+
  ylab("Scaled Prior Probability")+
  xlab("Target probability")+
  facet_wrap(~Distribution, scales = 'free', nrow = 1)+
  guides(fill = F, color = F)+
  theme(legend.position = 'bottom', legend.direction = 'horizontal')
```


```{r s1-model}
s1.model <- '
var probability = function(Dist, x) {
    return Math.exp(Dist.score(x));
}
var targetUtterance = "generic";

var utterancePrior = Infer({model: function(){
  return uniformDraw([targetUtterance,"silence"])
}});

var betaShape = function(params){
  return {
      a: params.g * params.d,
      b : (1-params.g) * params.d
  }
}

var thetaPrior = Infer({model: function(){
 return uniformDraw([
   0.01, 0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,
   0.5, 0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95
 ])}
});

var bins = [
  0.01,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,
  0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.99
];

var meaning = function(utt,state, theta) {
  return utt=="generic"? state > theta :
         utt=="generic is false"? state<=theta :
         utt=="silence"? true :
         utt=="some"? state>0.01:
         utt=="most"? state> 0.5:
         utt=="all"? state >= 0.99:
         true
}
var alpha = 2;
var mixture = data.prior[0].phi;
var priorParams = betaShape(data.prior[0]);

var statePrior = Infer({model: function(){
  var component = flip(mixture);
  return component ?
    categorical({
      vs: bins,
      ps: map(function(b) {
        return probability(Beta(priorParams), b) + Number.EPSILON
      }, bins )
    }) :
    categorical({
      vs: bins,
      ps: map(function(b) {
        return probability(Beta({a:1,b:100}), b) + Number.EPSILON
      }, bins )
    })
}});

var listener0 = cache(function(utterance) {
  Infer({model: function(){
    var state = sample(statePrior)
    var theta = utterance == "generic" ? sample(thetaPrior) : -99
    condition(meaning(utterance, state, theta))
    return state
 }})}, 10000)

var speaker1 = function(state) {
  Infer({model: function(){
    var utterance = sample(utterancePrior);
    var listener_posterior = listener0(utterance);
    factor(alpha * listener_posterior.score(state))
    return utterance
  }})
}

probability(speaker1(data.state[0]), "generic")
'
```

```{r causal-endorsement-predictions, cache = T, eval = F}
causal.priorParams <- m.cas.endorse.priors.summary %>%
    filter(distribution %in% c("common_deterministic", "rare_deterministic", "common_weak", "rare_weak"),
           Parameter != 'marginalPrevalence') %>%
  select(distribution, Parameter, MAP) %>%
  spread(Parameter, MAP)
experimental.frequencies <- c(0.2, 0.7)
causal.endorsement.predictions <- data.frame()
for (p in causal.priorParams$distribution){
  for (s in experimental.frequencies){
    
    inputData = list(prior = filter(causal.priorParams, distribution == p), 
                     state = s)
    
    s1.rs <- webppl(s1.model, data = inputData, data_var = "data")

    causal.endorsement.predictions <- bind_rows(
      causal.endorsement.predictions,
      data.frame(state = s, distribution = p, prob = s1.rs)
    )
  }
  print(p)
}
```


<!-- #### Materials  -->

<!-- The kind of causal events in question were selected to correspond roughly to different positions in a theoretically meaningful space defined by the model. -->
<!-- Research in elemental causal induction suggests people use causal models that correspond to a mixture probability distributions, where one component of the distribution is a consequence to the intrinsic causal force while the other component is a result of some extrinsic background cause. -->
<!-- So we designed materials that corresponded to domains where various parameters of that mixture model could be manipulated.  -->

<!-- A second dimension of man ipulation corresponds to the extent to which *determinism* is expected in the domain.  -->
<!-- For example, people's theories of physical causation tends to be more deterministic (i.e., favoring probabilities 1 and 0) then causation in the social or psychological domain.  -->


### Results

The distributions that resulted from participants predicting the causal efficacy of the new substances are shown in Figure \ref{fig:figure-causals}.
These distributions nicely recapitulate the distributions supplied in the different experimental conditions, suggesting that the manipulation does indeed change participants' representations of what probabilities are likely to occur in each experimental condition. 
We hypothesize that with the distributions altered, endorsements of generalizations will similarly be affected.

```{r empirical-causal-prior-densities, eval = F, fig.width = 5, fig.height = 3, fig.caption="Empirical densities from the prediction task in the prior manipulation experiment (Expt. 3a)."}

d.cas.priors %>%
  filter(distribution %in% c("common_weak", "rare_deterministic", "rare_weak")) %>%
      mutate(Distribution = factor(distribution, 
                               levels = c("rare_deterministic", "common_weak", "rare_weak"),
         labels = c("Rare Deterministic",
                    "Common Weak", "Rare Weak"))) %>% 
ggplot(., aes(x = response, fill = Distribution, color = Distribution))+ 
  geom_density( adjust = 0.8, alpha =0.8, 
               size = 1.2,
               aes( y = ..scaled..))+
  scale_x_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 1))+
  scale_fill_manual(values = c("#d01c8b", "#f1b6da", "#4dac26", "#b8e186"))+
  scale_color_manual(values = c("#d01c8b", "#f1b6da", "#4dac26", "#b8e186"))+
  ylab("Scaled Prior Probability")+
  xlab("Target probability")+
  facet_wrap(~Distribution, scales = 'free')+
  guides(fill = F, color = F)
```


## Experiment 3b: Causal Endorsements

In this experiment, we tested whether the manipulated priors of Expt. 3a are causally related to the endorsement of causal statements.

### Method

Most of the experiment was identical to that of Experiment 3a.


```{r causal-endorsement-data}
d.caus.endorse.catch.20 <- read.csv(paste(project.path, 
                                       "data/causals/endorsement/",
                                       "causals-8-20-catch_trials.csv", sep = ""))

d.caus.endorse.catch.70 <- read.csv(paste(project.path, 
                                       "data/causals/endorsement/",
                                       "causals-8-70-catch_trials.csv", sep = "")) %>%
  mutate(workerid = workerid + 1 +  max(d.caus.endorse.catch.20$workerid) )

d.caus.endorse.catch <- bind_rows(d.caus.endorse.catch.20, d.caus.endorse.catch.70)

d.caus.endorse.catch <- d.caus.endorse.catch %>% 
  mutate(passBoth = ifelse(pass_numeric + pass_story == 2, 1, 0))

d.caus.endorse.20 <- read.csv(paste(project.path, 
                                "data/causals/endorsement/",
                                "causals-8-20-trials.csv", sep = ""))

d.caus.endorse.70 <- read.csv(paste(project.path, 
                                "data/causals/endorsement/",
                                "causals-8-70-trials.csv", sep = "")) %>%
  mutate(workerid = workerid + 1 + max(d.caus.endorse.20$workerid))

d.caus.endorse <- bind_rows(d.caus.endorse.20, d.caus.endorse.70)


n.subj.cas.endorse <- length(unique(d.caus.endorse$workerid))
ave.minutes.cas.endorse <- round(mean(unique(d.caus.endorse %>% select(workerid, rt))$rt) / 1000 / 60, 1)

n.subj.cas.endorse.failed <- length(filter(d.caus.endorse.catch, passBoth == 0)$workerid)


d.caus.endorse.summary <- left_join(
  d.caus.endorse, 
  d.caus.endorse.catch %>% select(workerid, passBoth)
  ) %>%
  filter(passBoth == 1) %>%
  group_by(distribution, frequency) %>%
  summarize(k = sum(response), n = n()) %>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         low  = qbeta(.025, a, b),
         high = qbeta(.975, a, b),
         MAP_h = (a-1)/(a+b-2),
         mean = a / (a + b))
```

#### Participants

We recruited `r n.subj.cas.endorse` participants from Amazon's Mechanical Turk.
Participants were restricted to those with U.S. IP addresses and who had at least a 95\% work approval rating.
None of the participants had participated in Experiment 3a. 
The experiment consisted of one trial and took on average `r ave.minutes.cas.endorse` minutes; participants were compensated \$0.25 for their work.

#### Procedure and materials

The materials were the same as in Experiment 3a,
The first part of the experimental trial was the same as in Expt. 3a (the table of "previous experiments"; Figure \@ref(fig:causalExpt)).
Upon continuing past the first part of the trial, the table of results and background story were removed from the screen and the participant is told that the results of the "lost experiment" were found. 
The results are reported to the participant in terms of how many out of 100 of the attempts were successful. 
Participants saw 1 of 2 reported frequencies: 20\% or 70\% (randomized between-subjects).
Participants were then asked to judge the causal sentence (e.g., "Herb X makes the animals sleepy"). 
Partipants responded by either clicking "Yes" or "No". 

After responding, participants completed the same attention check as Expt. 3a.

```{r causals-endorsement-fig, fig.width = 4, fig.height = 3, eval = F}

ggplot(d.caus.endorse.summary, aes( x = factor(frequency), y = MAP_h, ymin = low, ymax = high, fill = distribution,
                        group = distribution))+
  geom_bar(stat='identity', position = position_dodge(), width = 0.3, color = 'black')+
  geom_errorbar(width = 0.2, position = position_dodge(0.3), color = 'black')+
  theme(axis.text.x = element_text(angle = 0))+
  geom_hline(yintercept = 0.5, lty = 3, color = 'black')+
  scale_y_continuous(limits = c(0, 1), breaks = c(0,0.5, 1))+
  scale_fill_solarized()+
  xlab("Target frequency")+
  ylab("Proportion Causal Endorsement")
```


```{r causals-fullmodel}
n_chains <- 1
n_samples <- 5000
burn <- n_samples / 2
lg <- 5
i <- 1

model_prefix <- "pilot-results-causals-jointModel-S1-"

m.cas.samp <- data.frame()
for (i in seq(1, n_chains)){
  mi <- fread(paste(project.path,  "models/causals/results/", 
                    model_prefix, "smntcs_causal-",
                    n_samples, "_burn", burn, "_lag", lg, "_chain", i, ".csv", sep = ""))
  m.cas.samp <- bind_rows(m.cas.samp, mi %>% mutate(chain = i))
}

# n_chains <- 1
# n_samples <- 1000
# burn <- n_samples
# lg <- 5
# i <- 1
# m.hab.fixed.samp <- data.frame()
# for (i in seq(1, n_chains)){
#   mi <- fread(paste(project.path,  "models/habituals/results/", 
#                     model_prefix, "smtncs_some-",
#                     n_samples, "_burn", burn, "_lag", lg, "_chain", i, ".csv", sep = ""))
#   m.samp.i <- mi %>% mutate(chain = i)
#   m.hab.fixed.samp <- bind_rows(m.hab.fixed.samp, m.samp.i)
# }

# m.hab.somemodel.endorsement <- m.hab.fixed.samp %>%
#   filter(type == 'predictive') %>%
#   rename(habitual = B, time_period = D, binned_freq = E) %>%
#   group_by(habitual, time_period, binned_freq) %>%
#   summarize(MAP = estimate_mode(val),
#             cred_upper = hdi_upper(val),
#             cred_lower = hdi_lower(val))

m.cas.fullmodel.endorsement <- m.cas.samp %>%
  filter(type == 'predictive') %>%
  group_by(dist, frequency) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))
```


```{r causal-model-insets, fig.width = 8, fig.height = 3.5, cache = T}
m.cas.fullmodel.prior.parameters <- m.cas.samp %>%
  filter(type == "prior") %>%
  rename(variable = item, parameter = roundedFreq) %>%
  group_by(dist, variable, parameter) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))

# use MAP estimates to generate L(h | causal) & L(h | silence) predictions

m.cas.fullmodel.prior.parameters.tidy <- m.cas.fullmodel.prior.parameters %>%
  ungroup() %>%
  select(dist, variable, parameter, MAP) %>%
  mutate(param = paste(variable, parameter, sep = "_")) %>%
  select(-variable, -parameter) %>%
  spread(param, MAP) %>%
  rename(mix = mixture_NA, 
         stable_mean = stableFreq_mean, 
         stable_concentration = stableFreq_sampleSize) %>%
  mutate( a = stable_mean * stable_concentration, 
          b = (1 - stable_mean) * stable_concentration)

cas.listener.predictions <- data.frame()
  
for (p in m.cas.fullmodel.prior.parameters.tidy$dist){
 priorParams <- m.cas.fullmodel.prior.parameters.tidy %>% filter(dist == p) 
 inputData = list(prior = list(params = data.frame(a = priorParams[["a"]],
                                                   b = priorParams[["b"]]),
                               mix = priorParams[["mix"]]), 
                  utt = "generic")
 l0.rs <- webppl(l0.model, data = inputData, data_var = "data")
 cas.listener.predictions <- bind_rows(
   cas.listener.predictions, 
   l0.rs %>% select(Parameter,value) %>% mutate(distribution = p)
   )
}

inset.color.order <- c( "#268bd2", "#dc322f","#d33682","#859900")


causals.endorsement.insets <- cas.listener.predictions %>% 
    mutate(Parameter = factor(Parameter, 
                              levels = c("state_Prior","state_Posterior"),
                            labels = c("Listener Prior (Posterior given Silence)",
                                       "Listener Posterior given Causal")),
    Distribution = factor(distribution, 
                               levels = c("common_deterministic", "rare_deterministic", "common_weak", "rare_weak"),
         labels = c("Common Deterministic", "Rare Deterministic",
                    "Common Weak", "Rare Weak"))) %>%
  ggplot(., aes( x = value, fill = Distribution, color = Distribution, lty = Parameter, alpha = Parameter ))+
  geom_density(aes(y = ..scaled..), adjust = 4, size = 1)+
  facet_wrap(~Distribution, nrow = 1)+
  scale_fill_manual(values =inset.color.order )+
  scale_color_manual(values = inset.color.order)+
  scale_alpha_manual(values = c(0.1, 0.8))+
  scale_linetype_manual(values = c(2, 1))+
  scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
  scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
  xlab("Feature-probability") +
  ylab("Scaled probability density")+
  theme(legend.position = "bottom", legend.title = element_blank())+
  guides(fill = F, color = F)
```

```{r}
md.caus.endorse <- left_join(
  m.cas.fullmodel.endorsement,
    d.caus.endorse.summary %>%
    select(distribution, frequency, MAP_h, low, high) %>%
    rename(dist = distribution)
)

n.caus.endorse <- length(md.caus.endorse$MAP_h)
r2.caus.rsa <- compute_r2(md.caus.endorse, "MAP_h", "MAP")
mse.caus.rsa <- compute_mse(md.caus.endorse, "MAP_h", "MAP")

```



```{r figure-causals-endorsement, fig.width=7, fig.height=3.5, cache = T}
figure.causals.endorsement <- bind_rows(
  m.cas.fullmodel.endorsement %>%
    mutate(src = "model"),
  d.caus.endorse.summary %>%
    select(distribution, frequency, MAP_h, low, high) %>%
    rename(dist = distribution, MAP = MAP_h, cred_lower = low, cred_upper = high) %>%
    mutate(src = 'data')
) %>%
  ungroup() %>%
  mutate(
    src = factor(src, levels = c( "data", "model")),
    dist = factor(dist, 
           levels = c("rare_weak", "common_weak", "rare_deterministic","common_deterministic"),
           labels = c(
                       "Rare Weak", "Common Weak","Rare Deterministic","Common Deterministic"))
    ) %>%
  ggplot(., aes( x = dist, y = MAP, ymin = cred_lower, ymax = cred_upper, fill = dist,
                        group = src, alpha = src))+
  geom_bar(stat='identity', position = position_dodge(), width = 0.3, color = 'black')+
  geom_linerange(position = position_dodge(0.3), alpha = 1)+
  #geom_line(position = position_dodge(0.3))+
  #theme(axis.text.x = element_text(angle = 90))+
  geom_hline(yintercept = 0.5, lty = 2, color = 'black', alpha = 0.4)+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 1))+
  scale_alpha_manual(values = c(1, 0.3))+
  #scale_fill_solarized()+
  scale_fill_manual(values = c("#859900","#d33682","#dc322f", "#268bd2"))+
  xlab("")+
  ylab("Proportion Causal Endorsement")+
  facet_grid(.~frequency)+
  coord_flip()+
  guides(fill = F)+
  theme(legend.position = 'bottom', legend.direction = 'horizontal', legend.title = element_blank())
```

```{r figure-causals, fig.width=11, fig.height=8, cache = T, fig.cap="Endorsing generalizations about causes. Top: Human elicited endorsements and model fit predictions for four manipulated prior distributions and two observed frequencies. Bottom: Feature-probability priors and listener posteriors upon hearing the generalization for the experimentally manipulated priors. These distributions are inferred using both data sources from Expt. 3 (analagous to the Bayesian data analytic approach outlined in Figure \\ref{fig:genericsModelDiagram})."}
grid.arrange(figure.causals.endorsement, causals.endorsement.insets, ncol = 1,
             layout_matrix = cbind(c(1,1,2)))

# ,
#              habituals.endorsement.insets, ncol = 2,
#              layout_matrix = cbind(c(1,1,3), c(2,2,3)))
```

### Results

`r n.subj.cas.endorse.failed` participants were excluded from the analysis for failing to answer both of the attention check questions correctly, leaving a total of `r n.subj.cas.endorse.failed-n.subj.cas.endorse.failed` responses for analysis.
As in our other analyses of endorsement responses, we computed the Bayesian Maximum A-Posteriori (MAP) estimate and 95\% highest probability density interval of the true population probability of endorsing the statement, assuming a uniform prior. 
These are shown for the different experimentally-manipulated priors and frequencies in Figure \ref{fig:causals-endorsement}.

As predicted by our model, endorsements for a causal statement were sensitive to the background distribution of other causes and the causal power of the target cause.
When many other causes produced the effect very reliably (*common deterministic* condition), very few participants endorsed the causal statement for a cause with causal power of 0.2, and were at chance when the causal power was 0.7 (Figure \ref{fig:figure-causals-endorsement; top, blue).
By contrast, when many other causes failed to produce the effect and those that did were not very reliable (*rare weak* condition; green in figure), at least half of participants endorsed the causal statement for a cause with causal power of 0.2, and were at ceiling when the causal power was 0.7.
The other two conditions (*rare deterministic* and *common weak*) led to endorsements intermediate between these two conditions.
Our model predicted these effects again with strong quantitative accuracy ($r^2(`r n.caus.endorse`) = `r r2.caus.rsa`$; MSE = $`r mse.caus.rsa`$).

## Discussion

In our third case study, we applied our same model to generalizations about causal events.
In this domain, we succesfully manipulated participants' beliefs about the expected causal power in a domain (Expt. 3a).
This was done using both unimodal (*common weak*, *common strong*) and bimodal (*rare weak*, *rare strong*) distributions. 
In Expt. 3b, we showed that these manipulated priors influenced endorsements of the corresponding causal statements.
In addition to further demonstrating the generality of this theory, these experiments show that the *feature-probability prior* $P(h)$ is causally related to endorsements of generalizations in language.
To our knowledge, these are the first experiments to demonstrate how the probabilities of other categories having a feature can influence endorsing a generalization about a different category.

In these experiments, we used two cover stories that described plausible causal events: herbs making animals sleepy and fertilizers making plants grow tall. 
We chose these items because there was a plausible causal mechanism that could give rise to the property and these causal events could have ambiguous causal power associated with them (e.g., it's plausible that there are herbs that only weakly make animals sleepy and it's also plausible that there are herbs that almost deterministically make animals sleepy).
These two features of the domains make them particularly amenable to manipulation.
That is, people's abstract theories about these domains are flexible enough to permit such manipulations.

It's likely that there exist domains for which abstract, intuitive theories might interfere with the experimentally-supplied "experimental data" to form a hybrid belief distribution.^[
If this were happening in our domains, we would expect this to show up in the results of Expt. 3a. Participants' predictions about the likely causal power of new causes would be expected to show a mixture of their abstract, intuitive theories and the experimentally supplied data.
]
For example, physical causal systems (e.g., billiard balls hitting each other) could strongly induce near-deterministic notions of causal power, analagous to our "deterministic" priors conditions. 
Causal systems that demonstrate surprising or *a priori* unlikely effects (e.g., liquids melting concrete) could induce rarity about the existence of a non-zero causal power, analgous to our "rare" prior conditions. 
Our theory would predict in these cases that differences in endorsement would be attributable to differences in the belief distribution over causal power. 
