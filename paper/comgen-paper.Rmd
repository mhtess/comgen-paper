---
title             : "The language of generalization: Probability, vagueness, and interaction"
shorttitle        : "The language of generalization"

author: 
  - name          : "Michael Henry Tessler"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Bldg. 420, Rm. 316, Stanford, CA 94305"
    email         : "mtessler@stanford.edu"
  - name          : "Noah D. Goodman"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
    
header-includes:
  - \usepackage{tabularx}
  - \usepackage{multicol}
  - \usepackage{wrapfig}
  - \usepackage{caption}
  - \usepackage{booktabs}
  - \usepackage{amsmath}
  - \usepackage{graphicx}
  - \usepackage{subcaption}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}

author_note: >
  This manuscript is currently in prep. Comments or suggestions should be directed to MHT.

abstract: > 
    Generalizations are central to human understanding and language provides simple ways to convey them (e.g., "Birds fly").
    The language of generalization is ubiquitous in everyday discourse and child-directed speech. 
    Yet the meaning of these expressions is logically puzzling (e.g., not all birds fly) and has resisted precise formalization.
    The major issue in formalizing generalization in language is determining which such statements are true or which are false.
    Using a probabilistic model of pragmatic reasoning, we explore the hypothesis that the meaning of these linguistic expressions is *simple but underspecified*, and that general communicative principles can be used to establish a more precise meaning in context. 
    To test this theory, we examine endorsements of generalizations about three different domains: generalizations about categories (*generic language* e.g., "Birds fly"), events (*habitual language* e.g., "John runs"), and causes (*causal language* e.g., "Staring at the sun makes you go blind"). 
    Across these diverse domains, we find that our model explains the wide variance in human endorsements of language conveying generalization, while simpler models fall short.
    Ours is the first formal theory that makes precise, quantitative predictions about human understanding of the language of generalization.
    The results suggest the central phenomena emerge from the interaction of underspecified meaning with diverse beliefs about properties. 
  
keywords          : "genericity, generalization, generics, pragmatics, semantics, bayesian modeling"
wordcount         : "20000"

bibliography      : ["generics.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
---

\newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}

\newcommand{\mht}[1]{{\textcolor{Blue}{[mht: #1]}}}
\newcommand{\ndg}[1]{{\textcolor{Green}{[ndg: #1]}}}
\newcommand{\red}[1]{{\textcolor{Red}{#1}}}

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=6, fig.height=5, fig.crop = F, fig.path='figs/',
                      echo=FALSE, warning=FALSE, cache=T, message=FALSE, sanitize = T)

```


```{r libraries, cache = F}
library(papaja)
library(formatR)
library(rwebppl)
library(xtable)
library(tidyverse)
library(forcats)
library(langcog)
# library(dplyr)
library(data.table)
library(coda)
library(ggthemes)
library(ggrepel)
library(jsonlite)
library(gridExtra)
library(lme4)
library(knitr)
library(kableExtra)
library(cowplot)
library(magick)
theme_set(theme_few())
estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}
hdi_upper<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}
hdi_lower<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}
logmeanexp <- function(x){
  x.num <- as.numeric(x)
  xstar = max(x.num)
  return(xstar + log(mean(exp(x.num - xstar))))
}

compute_r2 <- function(df,v1, v2, sigfigs = 3){
  return(format(cor(df[[v1]], df[[v2]])^2, digits = sigfigs))
}

compute_mse <- function(df, v1, v2, sigfigs = 3){
  return(format(mean( (df[[v1]]-df[[v2]])^2), digits = sigfigs))
}

project.path <- "../"
options("scipen"=10) 
```

<!-- 
possible titles:
- Communicating generalizations
- The language of generalization
- The language of generalization: Probability, vagueness, and interaction
-->

# Introduction 

Learning that an object tends to have a property, an entity tends to exhibit a behavior, or a cause tends to produce an effect can be crucial to thrive in our open-ended world.
Yet such knowledge can be difficult to acquire: Relevant observations may be costly (e.g., learning that a plant is poisonous) or rare (e.g., understanding that lightning strikes tall objects).
It thus is important that language allow us to communicate such *generalizations* to each other.
By sharing generalizations, we flourish collectively without individually needing to taste potentially-poisonous plants or personally witness many lightning strikes.
Furthermore, communicating generalizations from one generation to the next may be a foundational mechanism behind the faithful transmission of knowledge necessary for cumulative cultural evolution [@Tomasello1999; @Henrich2015].

<!-- Generalizations can be acquired from the world by drawing inductive inferences from observations [@Baldwin1993; @Xu2008; @Dewar2010]. \ndg{cite older stuff. eg hume} -->
<!-- Cooperative teachers can facilitate inductive learning by supplying helpful observations to the learner [@Csibra2009; @Tomasello1999; @Butler2012]. -->
<!-- At around a child's first birthday, however, the learner gains access to a new, human-unique source of data: language. -->
<!-- Minimally, language fosters generalizations by bringing attention to objects being of the same kind through the use of labeling [e.g., "This is a dog"; @Gelman1986; @Gopnik2000; @Graham2004; @Markman1989; @Schulz2008]. -->
<!-- Objects have many properties though; when speakers attach properties to kind-labels (e.g., "Dogs bark"), listeners hear a generalization. -->
<!-- \ndg{this par doesn't really do much} -->

The language of generalization is often studied via *generic language* (or, generics): simple statements  about categories [e.g., "Dogs are friendly"; @Carlson1977; @Cohen1999; @Leslie2007; @Nickel2008].
Generics are ubiquitous in everyday conversation as well as in child-directed speech [@Gelman2008]. 
In contrast to *particular statements* (e.g., "Rufus is friendly") which describe individual entities or situations and can be verified through observation, generic statements refer to inherently unobservable categories (e.g., the category of \textsc{dogs}) and convey information that extends beyond the present context, a fact which children as young as 2 appreciate [@Cimpian2008].
<!-- Additionally, generics are the primary way by which speakers discuss social categories, and thus are key to propagating stereotypes [@Gelman2004; @Rhodes2012; @Leslie2015] and impacting motivation [@Cimpian2010]. -->
Generics are just one case of the language of generalizations, however; events can be described in generalizations (*habitual language*; e.g., "Mary swims after work.") as well as causal relationships (*causal language*; e.g., "Staring at the sun makes you go blind.").
It is believed that every language can express generalizations [@Behrens2005; @Carlson1995] and that such language is central to the growth of conceptual knowledge [@Gelman2004].

<!-- Generic statements ascribing properties to categories are just one kind of generalization found in language.  -->
<!-- Individual entities can also be talked about in generalization, using so-called  -->
<!-- <!-- Recent evidence suggests talking about the other in generalization (using so-called *generic You*) can help make meaning from difficult situations   [e.g., "You never know what will happen on a blind date."; @Orvell2017]. -->
<!-- Communicating about causality can also take the form of communicating a generalization about a causal event  -->
<!-- What these statements have in common is that they convey generalizations, or exhibit *genericity* [@Carlson1977; @Carlson1995]. -->

Despite their ubiquity and relative simplicity, there have been few formal models of this language and none that have made precise, quantitative predictions to match human intuitions. 
The major challenge in formalizing the language of generalizations is in determining *what makes the statements true or false*.
When we hear "Dogs bark", it sounds like a universal statement, as in "All dogs bark".
But the property does not apply to all dogs: Basenji dogs are barkless^[
http://strangesounds.org/2014/09/basenji-dogs-basenjis-dont-bark-howl.html
], for example.
The generalization could signal something weaker like "Most dogs bark".
However, "Most mosquitos do not carry malaria" (in fact, only a tiny fraction do), but "Mosquitos carry malaria" is intuitively true.
<!-- *most mosquitos* do not carry malaria, while the statement "Mosquitos carry malaria" is intuitively true. -->
The only quantifier that would permit such a statement would be "some" (e.g., "Some mosquitos carry malaria").
But weakening the generalization to "some" is too permissive: "Some Robins are female" (in fact, half of them are), but asserting "Robins are female" seems odd.
Even more perplexing: Those very same female robins lay eggs, and "Robins lay eggs" is a perfectly fine thing to say.

These observations have led to the conclusion that the prevalence of the feature (or the probability of a member of the category having the property e.g.,  $P(x \text{ barks} \mid x \text{ is a dog})$) is not central to the meaning of generalizations in language. 
Conceptual accounts of generics emphasize the structure of *generic knowledge* [@Prasada2000], and view generic utterances as the way of expressing special, mental relationships between kinds and properties [@Leslie2008; @Prasada2012]. 
A paradigm case is the statement "Bishops move diagonally", which is true but not because bishops tend to move diagonally but rather because those are the rules of the game.
It's been suggested that the mind has cognitive primitives that directly dictate what generalizations are true or false.
One such primitive is the ability to priviledge *characteristic features*: For an animal, their mode of reproduction is a characteristic feature. Whatever their mode of reproduction, regardless of the actual prevalence of the feature, will make a true generic statement (e.g., "Robins lay eggs").
*Horrifying* or *striking* is also priviledged and creates true generics, which is why "Mosquitos carry malaria" is felicitous.

Examining a wider range of examples makes one realize that generics convey more than just rich, conceptual relations [@Nickel2008; @Nickel2016; @Sterken2015]. 
Purely arbitrary relationships sound like true generics true.
<!-- Arbitrary relationships are also expressed with generics:  -->
For example, "Barns are red" because barn-owners tend to paint their barns red; were barn-owners to decide tomorrow to switch to blue, then "Barns are blue" would be true.
Ravens and toasters have no priviledge relationship to each other, but the statement "Ravens are bigger than toasters" is still a true genaralization.
Conceptual accounts must then be supplemented with the proviso that, in some situations, the actual prevalence of the feature does influence human intuitions about generics.

What we are left with is hybrid theories, which try to classify different kinds of generalizations and the various inferences they support [e.g., @Prasada2013].
But insofar as there is a single class of linguistic expressions that convey generalizations, there should be something common to them all: a general, unified semantic core.
The test of a unified semantic core is its ability to explain what makes the statements true or false.

<!-- It is thus argued that there are qualitatively different kinds of generic statements (i.e., "striking", "characteristic", "statistical"), each with their own conditions on what makes them true. -->
<!-- This diversity is interesting, but what is common across generics? -->
<!-- Is there some core meaning to *genericity*, a general unified principle that explains what makes generic statements true or false? -->
<!-- Finally, formalizing such a theory is crucial to make explicit the relevant assumptions of the psychological or linguistic hypothesis meant to match human intuitions; this, in turn, makes the theory more testable.  -->

In this paper, we propose a semantic core for the language of generalization.
To do so, we look to probability, the universal currency of belief and a useful representation for human generalization from observations [@Shepard1987; @Tenenbaum2001].
Given that certain probabilities are the result of inductive generalization, it would make sense that these are the object of communication in the language of generalization. 
We then consider the background knowledge that listeners bring to bear when interpreting generalizations, and formalize an endorsement model based on this interpretation hypothesis.
Though concepts such as *striking properties* are not explicitly formalized in our model, our general cognitive modeling framework provides natural avenues for future integration.
Our endorsement model is the first formal theory that makes quantitative predictions about what makes generalizations in language true or false; the assumptions of the theory, and in turn, the conditions under which the theory may be meaningfully tested are made explicit by its instantiation in a formal model.

<!-- We formalize such a semantics in a general cognitive modeling framework, and thus incorporate generalizations in language into a larger class of models designed to formalize concepts and common-sense reasoning [@Goodmanconcepts]. -->

<!-- Specifically, we formalize an account where the core meaning of a generalization in language is a *simple but underspecfied* function of probability and where context helps resolve the underspecification. -->
<!-- In its simplest form, context takes the shape of interlocutors' shared beliefs about the relevant properties or categories under discussion [cf., @Grice1975; @Clark1996; @Levinson2000]. -->
<!-- We then ask what a rational speaker---who can only either endorse the generalization or stay silent---would say in different contexts; this is our formal model of endorsement. -->
<!-- We show how the decision to endorse depends in systematic ways on the shared prior knowledge of the interlocutors. \mht{"interlocutors" may be confusing here...} -->
<!-- We test this theory in three distinct case studies---generalizations about categories, events, and causes---by both measuring and manipulating prior knowledge, showing that it is strongly and causally related to the endorsement of generalizations in the ways predicted by our model. -->


<!-- We show that in each domain, our general theoretical framework explains endorsements about generalizations conveyed in language, while simpler models fall short.  -->

<!-- We think prevalence may have been cast aside too quickly for two main reasons: (i) in previous empirical work, prevalence has been rather narrowly defined, as frequency, where in fact it can be more generally seen as probability (ii) the backround knowledge that adult speakers, whose intuitions are consulted to establish a theory of meaning, may bring to the table has not been explicitly considered or formalized in studies and theoretical work into the language of generalization. -->


<!-- We propose a formal, unified account of generalization in language that can make precise, quantitative predictions about these simple and ubiquituous utterances. -->
<!-- We do so by first considering a much more well-studied form of generalization: generalization from observations.  -->
<!-- The Bayesian probability calculus describes inductive generalizations in terms of probability, and this is has proven to be a good model of human generalization . -->


<!-- We formalize this using a rational model of communication --- the Rational Speech Act (RSA) theory [@Frank2012; @Goodman2013; @Franke2015; @Goodman2016]. -->
<!-- Such models of rational communication describe the inferences listeners can draw from language, and the utterances speakers should say given their state of knowledge and communicative goals.  -->
<!-- Heretofore, however, the fragments of language formalized in such models have been limited to *particular statements* that convey information about a specific entity or situation [e.g., in hyperbole: "My watch cost a million dollars"; @Kao2014, or vagueness: "John is tall"; @Lassiter2013]. -->
<!-- The language of generalization does not make reference to entities in the world or concrete situations, but rather refer to unobservable concepts in the head. -->

<!-- We relate known measurements on factors regarding generic language to each other via our model. -->
<!-- We then go on to manipulate the relevant factors, showing they are causally related via our model to generic endorsement and interpretation.  -->

The paper is organized as follows. 
In Section 1, formalize the hypothesis that the core of meaning of a generalization in language is simple but *underspecified*, articulate a thory of relevant background knowledge used to understand a generalization in language, and develop a computational model of endorsement around these components.
The endorsement model's predictions about depend upon the background knowledge and the probability communicated. 
Sections 2 - 4 present our three empirical case studies: generalizations about categories (*generic language*), events (*habitual language*), and causes (*causal language*). 
Across these three case studies, we both measure and manipulate background knowledge and probability communicated and measure its effect on endorsement. 
We find a very strong agreement of our model's predictions to the full spectrum of human elicited endorsements (cumulative $r^2(N) = 0.XX$).
<!-- In the final section of the paper, we discuss the generality of our approach and pave a path to unifying the extant psychological, linguistic, and philosophical literatures on generalization in language. -->
<!-- We also show how our endorsement model is a special case of a speaker model in a rational framework of communication.  -->
This opens the door to understanding generalizations in pragmatic language understanding contexts and building models of learning from language.

# Computational Framework

Generalizations are used to make predictions about properties of instances that an agent has yet to experience [@HumeTHN].
People readily predict that a new dog will have fur, drinking a new cup of coffee will cause jitters, and a new day will find certain people typing.
In each case, we assign a specific exemplar $x$ to a category $k$, and make a prediction that it will have property $f$.
This prediction can be described by a conditional probability: $h = P(x \in f \mid x \in k)$, the probability that $x$ will have $f$ given that it is in $k$, which we will refer to as the **feature-probability**.
The targets of our predictions can vary widely: They may be objects (a dog), events (a coffee drink), or more fine grained types (a person on a given day).
The properties also vary (e.g., having fur, causing jitters, typing). 
Yet the mathematical description of the inductive inference is always given by the conditional feature-probability $h$.

How does one acquire such an inductive belief?
One can observe instances of the category $k$ and note how many of them have $f$.
These observations can support prediction to new instances by integrating them with one's prior expectations about the property [cf., @Gelman2003; @Keil1992].
For example, if you observe creatures that \textsc{have two legs}, you might infer that all creatures of that category have two legs.
Observing new creatures with \textsc{broken wings}, however, leads to a much weaker prediction about other instances of the category having the property [@Nisbett1983].
This kind of background knowledge can be represented formally as a probability distribution over the feature-probabilities $h$: $P(h)$ [@Kemp2008], and observational data $d$ can update one's beliefs via Bayes' Rule: $P(h \mid d) \propto P(d \mid h) \cdot P(h)$

Learning from observational evidence can only take you so far, though. 
To learn about properties that are costly to observe (e.g., *staring at the sun makes you go blind*) or events that are statistically rare (e.g., *lightning strikes tall objects*), one would much rather learn from others.
Fortunately, language has simple ways of encoding generalizations.


<!-- Here, $P(d \mid h)$ is the observer's generative model of the observed data.  -->
<!-- It is conditional on $h$ because the probability of various direct observations depends upon the feature-probability that the observer assumes.  -->
<!-- Generalization from observational evidence is limited, however, in its capacity to describe how the mind builds complex theories of the world or how culture accumulates across generations. -->
<!-- How do we acquire these generalizations? -->
<!-- For abstract, generalizable knowledge to remain in a culture, it must be faithfully transmitted between individuals and across generations [@Tomasello1999], and language provides simple ways to communicate generalizations. -->
<!-- The human capacity to generalize from observations, deriving $h$ from example $x$s, has been studied extensively in cognitive and developmental psychology [@Nisbett1983; @Baldwin1993; @Heit2000]. -->
<!-- From a very young age, children draw strong inferences about nonobvious object properties from just a few examples [@Baldwin1993; @Gopnik2000;  @XuTenenbaum2007]. -->
<!-- Such inductive generalization can be described by the Bayesian probability calculus: An observer's initial beliefs or state of knowledge---called a prior belief distribution---is updated through experience (i.e., after making observations) and becomes a posterior belief distribution (what they belief *a posteriori*).  -->
<!-- This posterior distribution becomes the observer's prior for the next observation she makes: it thus informs her predictions about unobserved instances. -->

<!-- As a concrete example, consider a learner investigating the feature \textsc{barks} ($f$) for the category \textsc{dog} ($k$). -->
<!-- $h$ describes the learner's subjective probability that the next dog she encounters will bark^[At the moment, we assume the feature \textsc{barks} is an all-or-none Boolean property. Later, we will see how the presence of this feature itself can be thought of as a generalization over events of barking.], or more abstractly, her belief about the barking capacity of dogs. -->
<!-- That is, $h = P(x \text{ barks} \mid x \text{ is a dog})$. -->
<!-- The learner does not know $h$, but has some idea about what $h$ could be; this is represented by $P(h)$, the learner's prior beliefs about how likely it is that $x$ will bark given that $x$ is a dog. -->

<!-- The observer goes around the world, encountering dogs and taking note of how many bark. -->
<!-- This could be represented as a number of positive examples $d$ out of a number of total examples $n$.  -->
<!-- With the weak assumption that the dogs she has observed were all independent and identically-distributed (*i.i.d.*) random samples from the category of dogs, the likelihood of her observations are given by the Binomial distribution.  -->
<!-- That is, $d \sim  \text{Binomial}(n, h)$, where $d$ is the number of dogs she has seen that bark, $n$ is the total number of dogs she has seen, and $h$ is the feature-probability of barking among dogs. -->
<!-- This is called a likelihood function: It determines how well a given value of $h$ predicts the observed $d$.  -->
<!-- Given a formal description of the learner's state of knowledge before any observations $P(h)$ and the learner's assumptions about how the data she observed were generated (e.g., the *i.i.d.* assumption), what a learner should believe after having seen the data is proportional to the product of these two terms, as given by Bayes' Rule: -->


<!-- $P(h \mid d)$ is then the strength of the learner's belief in the next dog she encounters barking. -->
<!-- This simple idea of integrating observational evidence with prior knowledge is surprisingly powerful in explaining a multitude of diverse phenomena, from perception to word learning to inductive reasoning [e.g., @Tenenbaum2006; @Tenenbaum2011]. -->
<!-- \ndg{what is this par doing for us? delete?} -->
<!-- Generalization from examples shows complex sensitivites to the kind of observations, and even how those observations came to be observed in the first place. -->
<!-- For instance, observing that an indigenous person living on a remote island *is brown and obese* provides considerably more information about the skin color of other islanders than about their weight [@Nisbett1983]. -->
<!-- This subtlety can be modeled by incorporating heiarchical structure into the prior belief distribution of a Bayesian model, allowing the model to represent and reason flexibly about different kinds of properties [@Kemp2008]. -->
<!-- Observations that are generated by an intentional agent can lead to similarly subtle inferences, like deconfounding otherwise ambiguous causal evidence [@Goodman2009]. -->
<!-- Yet generalizations from observational evidence is limited in its capacity to describe how the mind builds complex theories of the world or how culture accumulates across generations. -->
<!-- Relying upon observations would be particularly problematic, for example, to learn about properties that are costly to observe (e.g., *staring at the sun makes you go blind*) or events that are statistically rare (e.g., *lightning strikes tall objects*). -->
<!-- How do we acquire these generalizations? -->
<!-- For abstract, generalizable knowledge to remain in a culture, it must be faithfully transmitted between individuals and across generations [@Tomasello1999], and language provides simple ways to communicate generalizations. -->

## Generalization from language

Generalization from observations can be described as a predictive probability; it is intuitive then to posit that same quantity will be at the core of a semantic theory of the language of generalization.
To connect to the semantic tools of *truth values* [@Montague1973], we must relate the scalar quantity of probability to a Boolean value.
One simple way is with a *threshold semantics*: An utterance is true if the relevant quantity is above a threshold.
A threshold semantics for the language of generlizations would then apply to the *feature-probability* $h$:
\begin{equation}
\denote{generalization}(h, \theta) = \{h>\theta\}
\label{eq:semantics}
\end{equation}
Various attempts of this kind have been made before [e.g., @Cohen1999].
We distinguish our proposal from previous ones by emphasizing the cognitive underpinnings of $h$, as well as the formalization into a model that makes precise, quantitative predictions about human endorsements.  
$h$ describes a latent belief and should not be confused with *frequency*, which can inform a belief but is not itself the belief (due to background knowledge, see above).

<!-- We emphasize that this account is based on a probability $h$. -->
<!-- The literature on generic language has often focused on *prevalence*---either the actual or believed proportion of members of a category with the property---which we distinguish from the feature-probability $h$.^[ -->
<!--   There has also been substantial discussion of *cue validity* , which we come back to below. -->
<!-- ] -->
<!-- Prevalence, an ostensibly observable quantity, can inform a person's beliefs (e.g., via Bayesian belief updating) but it is not the belief itself.  -->
<!-- Probability is a measure of belief.  -->

### Literal interpretation model

Quantifier statements can be described using a similar threshold semantics: $\denote{some}(x) = \{h > 0\}; \denote{all} = \{h = 1\}$.
So what threshold should be used for the generalizations? 
What is $\theta$?
Unlike quantified language, we posit that the language of generalizations is *underspecfied* with respect to the actual truth-functional threshold $\theta$.
That is, we hypothesize that $\theta$ is contextually-determined, in a way analagous to how gradable adjectives like "tall" have contextually-determined thresholds [e.g., what counts as "tall" for a four-year-old is different than what counts as "tall" for an adult; @Kennedy2007; @Lassiter2013].

We formalize this underspecified semantics in a probabilistic model, where the underspecifed threshold $\theta$ is sampled from a uninformed prior distribution over possible thresholds $P(\theta)$ [cf., @Lassiter2013].
After making explicit the relevant background knowledge $P(h)$, the model becomes:

\begin{eqnarray}
Interpret(h, \theta \mid u) &\propto& {\delta_{\denote{u}(h, \theta)} \cdot P(h) \cdot P(\theta)} \label{eq:L0}
\end{eqnarray}

Equation \ref{eq:L0} ($Interpret$) is a literal interpretation model, that updates its beliefs about the feature-probability $h$ according to the truth-functional meaning of the utterance $u$. 
Formally, the truth-functional meaning is represented by the Kronecker delta function  $\delta_{\denote{u}(h, \theta)}$ that returns probabilities proportional to $1$ when the utterance is true (i.e., when $h > \theta$) and $0$ otherwise.

\begin{eqnarray}
\delta_{\denote{u_{gen}}(h, \theta)} &\propto  & \begin{cases}
1 & \text{if } h > \theta \\
0 & \text{otherwise}
\end{cases}\label{eq:delta}
\end{eqnarray}

<!-- Finally, $h$ is sampled from a prior belief distribution $P(h)$, potentially informed by the property in question and discussed further below.  -->

### Endorsement model

Given the above minimal model of literal interpretation, how do we decide when a statement conveying a generalization is true or not?
We consider the notion of felicity or endorsement from the information-theoretic perspective: Would a generalization convey sufficient information to warrant the cost of producition? 
We formalize this by considering how well the generalization would convey the feature-probability $h$ to the $Interpret$ model (Eq. \ref{eq:L0})^[A more general version of this model can relax the assumption that the endorsement model has access to a specific feature-probability $h'$ that it wants to communicate.
Rather, the endorser may have a distribution over $h'$ for the category $k$, corresponding to the endorser's uncertain beliefs about the feature-probability. 
In this situation, we would define the endorsement model decision to be with respect to the expected value of the informativity, which integrates over the endorser's belief distribution:
$Endorse(u \mid k) \propto \exp{(\lambda \cdot {\mathbb E}_{h\sim P_{k}} \ln{ \int_{\theta} Interpret(h, \theta \mid u)} \diff \theta )}$
For the empirical case studies described below, these two versions of the model make almost identical predictions.
].

\begin{equation} 
Endorse(u \mid h = h') \propto \exp{(\lambda \cdot \ln{ \int_{\theta} Interpret(h = h', \theta \mid u)  \diff \theta } )}
\label{eq:S1}
\end{equation}

The endorsement model simulates what the interpretation model would believe upon hearing an utterance $u$): $Interpret(h = h', \theta \mid u)$.
$Endorse$ is not interested in communicating $\theta$ (in fact, it does not know $\theta$), and so integrates over the values of $\theta$.
We assume the decision to endorse is an approximately rational one, with degree of rationality given by $\lambda$.
The proportionality in Eq. \ref{eq:S1} implies normalization over a set of alternative utterances. 
We interpret the endorsement task (e.g., "true" vs "false"; "agree" vs. "disagree") as supplying two alternatives: *Endorse* or *not*, where *not* we take to be a silent utterance that conveys no information content [cf., @Franke2014cogsci; @Degen2014]: $\denote{null}(x, \theta) = \text{True}$^[
  This "null" alternative  to the generalization ("Ks F") can be realized in at least two other ways: the negation of the generalization (i.e., "It is   not the case that Ks F") or the negative generalization (i.e., "Ks do not F").
  All results reported are similar for these two alternatives, and we use the alternative of the "silent" utterance for simplicity.
]


Our endorsement model (Eq. \ref{eq:S1}) provides a straight-forward mapping from a *referent-category feature-probability* $h'$  (e.g., $P(x \text{ lays eggs} \mid x \text{ is a robin})$) to an endorsement probability for the corresponding generalization (e.g., "Robins lay eggs"). 
It does this assuming a threshold semantics based on probability but does not require specifying the threshold *a priori* (note that there is not $\theta$ is Eq. \ref{eq:S1}).
This model can be seen as a special case of a Rational Speech Act model [@Frank2012; @Goodman2016], where there is only a single level of recursion and the space of possible utterances is maximally constrained. 

<!-- Rather, the uncertainty over $\theta$ is resolved *internal* to the cognitive model, by reasoning about the literal semantics and the prior distribution over feature-probabilities $P(h)$. -->

<!-- The endorsement model presented above is a special case of a family of rational models of communication that use an underspecified threshold-meaning for a generalization. -->

<!-- Yet the *simple but underspecified* hypothesis defines a *family* of models (e.g., more levels of recursion: a listener who thinks about a speaker who thinks about a listener). -->
<!-- The primary theoretical interest of this paper is to outline *a* formal account, rather than discuss possible variations within it.  -->
<!-- However, there are interesting theoretical differences to be found within this family of models (e.g., if the inference about the threshold can be thought of as a *pragmatic inference* or just a mere *literal inference*). -->
<!-- We discuss these alternative formulations and provide formal model comparison between these RSA models in the Appendix. -->

<!-- The null alternative returns a posterior interpretation distribution that is identical with the prior $P(h)$. -->

<!-- Following the Rational Speech Act framework, the speaker (called $E$ for endorsement) is assumed to be an approximately rational Bayesian agent who produces utterances according to their utility $U$ with rationality governed by parameter $\lambda$, which acts as a soft-max function. -->
<!-- Because we are interested in determining whether or not *feature-probability* alone is sufficient to formalize generalizations in language, we define the endorsement utility as a function of only the utterance (say the generalization or stay silent) and the *referent-category feature-probability* $h'$ (e.g., $P(x \text{ lays eggs} \mid x \text{ is a robin})$). -->

<!-- The utility of an utterance is defined by how well it conveys the speaker's beliefs ($h'$) to a naive listener $Lit$, after integrating out her residual uncertainty about $\theta$. -->

<!-- The literal semantics for the generalization is the threshold function described above; the literal semantics for the option of staying silent carries no informational content:   -->

<!-- \begin{equation}  -->
<!-- U(u; h') =  \ln{ \int_{\theta} Lit(h = h', \theta \mid u)  \diff \theta } \label{eq:utility} -->
<!-- \end{equation} -->

We implemented the model in the probabilisitic programming language WebPPL [@dippl].
The following simulations were implemented using the R package RWebPPL [@rwebppl] in the RMarkdown document that generated this paper text and figures.
That document, as well as all other models, analysis, data and links to experiments used in this paper can be found at \url{https://mhtess.github.io}.


### Model simulations

The endorsement model defined in the previous section explicitly predicts that the variability in endorsements for generalizations in language can be explained by two factors: (i) the feature-probability communicated in the category under discussion $h'$ in Eq. \ref{eq:S1} (e.g., $P(x \text{ lays eggs} \mid x \text{ is a robin})$) and (ii) the background knowledge about feature-probabilities $P(h)$ (Eq. \ref{eq:L0}).
We simulate how these two factors change both literal interpretation and endorsement using hypothetical values based on intuition about classic examples from the literature on generic language.

<!-- depends upon the probability that utterance vs. silence accurate conveys to a naive listener a. -->
<!-- This probability of successful communication is given by $L(h \mid u, \theta)$ (Eq. \ref{eq:S1}) and depends upon the listener's *a priori* beliefs about the feature-probability in question: $P(h)$ (Eq. \ref{eq:L0}), as well as the referent-category feature-probability $h$ (Eq. \ref{eq:S1}). -->
<!-- Predictions for endorsement depend upon how well the listener's posterior on $h$  upon hearing a generalization matches an $h$ that the speaker wants to convey in comparison to how well the listener's prior matches (which would result from the speaker staying silent). -->
<!-- For purposes of illustration, we present different hypothetical priors on feature-probabilities $P(h)$, together with listener $L$ posterior distributions on $h$ after hearing utterances corresponding to (i) the underspecified threshold semantics as well as (ii) a comparison model where the threshold is fixed near-0, corresponding intuitively to the quantifier *some* (Figure \ref{fig:simulations}). -->


```{r rsaHelpers}
rsaHelpers <- '
var probability = function(Dist, x) {
    return Math.exp(Dist.score(x));
}
var targetUtterance = "generic";

var round = function(x){
  return Math.round(x*1000)/1000
}
var utterancePrior = Infer({model: function(){
  return uniformDraw([targetUtterance,"silence"])
}});

var thetaBins = map(round, _.range(0.01, 0.98, 0.01))
// var thetaBins = [
//    0.01, 0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,
//    0.5, 0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95
 // ]
var thetaPrior = Infer({model: function(){
 return uniformDraw(thetaBins)
}});

var bins = map(round, _.range(0.01, 0.99, 0.01))

// var bins = [
  // 0.01,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,
  // 0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.99
// ];

var meaning = function(utt,state, theta) {
  return utt=="generic"? state > theta :
         utt=="generic is false"? state<=theta :
         utt=="silence"? true :
         utt=="some"? state>0.01:
         utt=="most"? state> 0.5:
         utt=="all"? state >= 0.99:
         true
}

var mixture = data.prior.mix[0];
var priorParams = data.prior.params[0];

var statePrior = Infer({model: function(){
  var component = flip(mixture);
  return component ?
    categorical({
      vs: bins,
      ps: map(function(b) {
        return probability(Beta(priorParams), b) + Number.EPSILON
      }, bins )
    }) :
    categorical({
      vs: bins,
      ps: map(function(b) {
        return probability(Beta({a:1,b:50}), b) + Number.EPSILON
      }, bins )
    })
}});
'
```


```{r rsaInterpretationModels, eval = T, cache = T}
no.utterance.model <- '
var noUtteranceInterpreter = function() {
  Infer({model: function(){
   var state = sample(Beta( flip(mixture) ? priorParams : {a:1,b:100}))
    return { state }
 }, method: "rejection", samples: 20000, burn:5000, verbose: T})}
'

fixed.threshold.model <- '
var fixedThresholdInterpreter = function(threshold) {
  Infer({model: function(){
  // var state = sample(Beta( flip(mixture) ? priorParams : {a:1,b:100}))
  var state = sample(statePrior)
  condition( state > threshold)
    return {
      state: state
  }
 // }, method: "rejection", samples: 20000, burn:5000, verbose: T})}
 }, method: "enumerate"})}
'

uncertain.threshold.model <- '
var uncertainThresholdInterpreter = function() {
  Infer({model: function(){
   var state = sample(Beta( flip(mixture) ? priorParams : {a:1,b:100}))
    factor( Math.log(state) )
    return {
      state: state, 
  }
  }, method: "rejection", samples: 20000, burn:5000, verbose: T})}
'

```

```{r simulationCalls}
fixed.threshold.calls <- '
_.fromPairs(map(function(t){return [t, fixedThresholdInterpreter(t)]}, [0.1, 0.3, 0.5, 0.7, 0.9]))
'
no.utterance.call <- ' 
  noUtteranceInterpreter()
'
uncertain.threshold.call <- '
  uncertainThresholdInterpreter()
'
```

```{r simulationRuns}
priorNames <- c(
  "doesntEatPeople", #"uniformRare", 
  "isFemale", 
  "laysEggs", 
  #"eatsFood", 
  "barks",
  "carriesMalaria",
  "hasSpots"
 # "areSick"
  )
priorShapes <- list(
  isFemale =  list(params = data.frame(a = 10, b = 10), mix = 1), 
  laysEggs =  list(params = data.frame(a = 10, b = 10), mix = 0.4), 
  doesntEatPeople =  list(params = data.frame(a = 50, b = 1), mix = 1), 
  barks =  list(params = data.frame(a = 5, b = 1), mix = 0.4), 
  hasSpots =  list(params = data.frame(a = 5, b = 1), mix = 0.7), 
  carriesMalaria =  list(params = data.frame(a = 2, b = 10), mix = 0.4)
)


sims.fixed.thresholds <- data.frame()
sims.priors <- data.frame()
sims.uncertain.thresholds <- data.frame()

for (p in priorNames){

  inputData = list(prior = priorShapes[[p]])
  rs.fixed.threshold.model <- webppl(paste(rsaHelpers, fixed.threshold.model, fixed.threshold.calls, sep = '\n'), data = inputData, data_var = "data")
  rs.uncertain.threshold.model <- webppl(paste(rsaHelpers, uncertain.threshold.model, uncertain.threshold.call, sep = '\n'), data = inputData, data_var = "data")
  rs.prior <- webppl(paste(rsaHelpers, no.utterance.model, no.utterance.call, sep = '\n'), data = inputData, data_var = "data")

  sims.fixed.thresholds <- bind_rows(sims.fixed.thresholds,
    bind_rows(
      mutate(get_samples(data.frame(rs.fixed.threshold.model$`0.1`) %>% rename(prob = probs), 20000), threshold = 0.1) ,
      mutate(get_samples(data.frame(rs.fixed.threshold.model$`0.3`) %>% rename(prob = probs), 20000), threshold = 0.3),
      mutate(get_samples(data.frame(rs.fixed.threshold.model$`0.5`) %>% rename(prob = probs), 20000), threshold = 0.5),
      mutate(get_samples(data.frame(rs.fixed.threshold.model$`0.7`) %>% rename(prob = probs), 20000), threshold = 0.7),
      mutate(get_samples(data.frame(rs.fixed.threshold.model$`0.9`) %>% rename(prob = probs), 20000), threshold = 0.9)
    ) %>% 
      mutate(PriorShape = p)
    )

  sims.uncertain.thresholds <- bind_rows(sims.uncertain.thresholds, rs.uncertain.threshold.model %>% select(value) %>% rename(state = value) %>% mutate(PriorShape = p))
  sims.priors <- bind_rows(sims.priors, rs.prior %>% select(value) %>% rename(state = value) %>% mutate(PriorShape = p))
}
```


```{r cache = F}
fig.sims.priors <- ggplot(sims.priors, aes(x = state, fill = PriorShape, color = PriorShape))+
    geom_density(aes(y = ..scaled..), size = 0.6, alpha = 0.7)+
    theme_few() +
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    ylab("Normalized probability density") +
    xlab("")+
    scale_color_solarized()+
    scale_fill_solarized()+
    facet_grid(PriorShape~., scales = 'free')+
    theme(strip.text.y = element_blank())
```


```{r cache = F}
fig.sims.fixed.thresholds <- ggplot(sims.fixed.thresholds, aes(x = state, fill = PriorShape, color = PriorShape))+
    geom_density(aes(y = ..scaled..), size = 0.6, alpha = 0.3)+
    theme_few() +
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    xlab("Feature-probability") +
    ylab("") +
    scale_color_solarized()+
    scale_fill_solarized()+
    facet_grid(PriorShape~threshold, scales = 'free')+
    theme(strip.text.y = element_blank(), axis.title.x = element_text(hjust = 1))
```

```{r cache = F}
fig.sims.uncertain.threshold <- sims.uncertain.thresholds %>% mutate(PriorShape = factor(PriorShape, 
                             levels = c("barks", "hasSpots", "doesntEatPeople", "laysEggs", "isFemale", "carriesMalaria"),
                            labels = c("bark", "have spots", "don't eat people", "lay eggs", "are female", "carry malaria"))) %>% 
                              ggplot(., aes(x = state, fill = PriorShape, color = PriorShape))+
    geom_density(aes(y = ..scaled..), size = 0.6, alpha = 0.7)+
    theme_few() +
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    xlab("") +
    ylab("") +
    scale_color_solarized()+
    scale_fill_solarized()+
    facet_grid(PriorShape~., scales = 'free')+
    theme(strip.text.y = element_blank())#element_text(angle = 0, hjust = 0))
```

```{r fig.width = 9, eval = F}
p1 <- fig.sims.priors +
  theme(plot.margin = unit(c(18,2,6,6), "pt")) #+ ggtitle("Feature-probability priors")
p2 <- fig.sims.fixed.thresholds +
  theme(plot.margin = unit(c(6,0,6,0), "pt")) + ylab("")
p3 <- fig.sims.uncertain.threshold+
  theme(plot.margin = unit(c(18,0,6,6), "pt"))+ 
    theme(strip.text.y = element_blank())#+ ggtitle("Interpreter model posterior")

prow <- plot_grid( p1 + theme(legend.position="none"),
           p2 + theme(legend.position="none"),
           p3 + theme(legend.position="none"),
           align = 'vh',
           labels = c("A", "B", "C"),
           hjust = -1,
           nrow = 1,
           #rel_widths = c(1.2, 3, 2)
           rel_widths = c(1.2, 3, 1.2)
           )
#prow
#legend <- get_legend(p1)

#plot_grid( prow, legend, rel_widths = c(3, .3))
```


```{r simluationEndorsementModel, eval = T, cache = T}
s1.model <- '
var interpreter = cache(function(utterance) {
  Infer({model: function(){
    var state = sample(statePrior)
    var theta = utterance == "generic" ? sample(thetaPrior) : -99
    condition(meaning(utterance, state, theta))
    return state
 }})}, 10000)

var alpha = 2

var endorser = function(featureProb){
  Infer({model: function(){
    var endorsement = sample(utterancePrior)
    var L0 = interpreter(endorsement)
    factor(alpha * L0.score(featureProb))
    return endorsement
  }})
}
// console.log(data.featureProb[0])
probability(endorser(data.featureProb[0]), "generic")
'

exampleGenerics <- c(
  "sharksDontEatPeople",
  "robinsAreFemale", 
  "robinsLayEggs", 
  "kangaroosHaveSpots",
  "dogsBark",
  "mosquitosCarryMalaria"
  )

exampleParameters <- list(
  robinsAreFemale =  list(prior = list(params = data.frame(a = 10, b = 10), mix = 1), featureProb = 0.5), 
  robinsLayEggs =  list(prior = list(params = data.frame(a = 10, b = 10), mix = 0.2), featureProb = 0.5), 
  sharksDontEatPeople =  list(prior = list(params = data.frame(a = 50, b = 1), mix = 1), featureProb = 0.8), 
  dogsBark =  list(prior = list(params = data.frame(a = 5, b = 1), mix = 0.4), featureProb = 0.9), 
  kangaroosHaveSpots =  list(prior = list(params = data.frame(a = 5, b = 1), mix = 0.7), featureProb = 0.1), 
  mosquitosCarryMalaria =  list(prior = list(params = data.frame(a = 1, b = 30), mix = 0.1), featureProb = 0.1)
)

s1.simulations <- data.frame()
for (p in exampleGenerics){

    s1.rs <- webppl(paste(rsaHelpers, s1.model, sep = "\n"), data = exampleParameters[[p]], data_var = "data")
    s1.simulations <- bind_rows(s1.simulations,
                                   data.frame(example = p, endorsement = s1.rs, featureProb = exampleParameters[[p]]["featureProb"] ))
    
}

```

```{r}
s1.simulations.relabeled <- s1.simulations %>%
  mutate(example = factor(example, 
                          levels = c("dogsBark", "kangaroosHaveSpots", "sharksDontEatPeople", "robinsLayEggs", "robinsAreFemale", "mosquitosCarryMalaria"),
                            labels = 
                            c("Dogs bark", "Kangaroos have spots", "Sharks don't eat people", "Robins lay eggs", "Robins are female", "Mosquitos carry malaria")
                            #c("Dogs", "Kangaroos", "Sharks", "Robins ", "Robins", "Mosquitos")
                          ))

# s1.simulations.scatter <- ggplot(s1.simulations.relabeled, aes( x = featureProb, y = endorsement))+
#   #geom_jitter(width = 0.1, shape = 21, size = 5)+
#   #ylab("Human endorsement probability")+
#   #xlab("Frequency of target")+
#   geom_label_repel(data = s1.simulations.relabeled, 
#                    aes( label = example, fill = example),
#                        color = 'white',
#     box.padding = unit(0.0, "lines"),
#     #point.padding = unit(1e-06, "lines"),
#     segment.color = 'grey37')+
#   scale_fill_solarized()+
#   scale_x_continuous(limits = c(0, 1), 
#                      breaks = c(0, 0.5, 1))+
#   scale_y_continuous(limits = c(-0.01, 1.01), 
#                      breaks = c(0, 1),
#                      labels = c("False", "True"))+
#   coord_fixed(ratio = 1)+
#   guides(fill = F)+
#   ylab("Endorsement model prediction")+
#   xlab("Hypothesized feature-probability")



s1.simulations.scatter <- ggplot(s1.simulations.relabeled, aes( x = featureProb, y = endorsement, fill = example))+
  geom_col(color = 'black')+
  # geom_label_repel(data = s1.simulations.relabeled, 
  #                  aes( label = example, fill = example),
  #                      color = 'white',
  #   box.padding = unit(0.0, "lines"),
  #   #point.padding = unit(1e-06, "lines"),
  #   segment.color = 'grey37')+
  scale_fill_solarized()+
  scale_x_continuous(limits = c(0, 1), 
                     breaks = c(0, 1))+
  scale_y_continuous(limits = c(-0.01, 1.01), 
                     breaks = c(0, 1),
                     labels = c("False", "True"))+
  #coord_fixed(ratio = 1)+
  coord_flip()+
  facet_grid(example~.)+
  guides(fill = F)+
  ylab("Endorsement")+xlab("Referent feature-probability")+
  theme(strip.text.y = element_text(angle = 0, hjust = 0)
        #axis.title.x = element_text(hjust = 0)
        #axis.title.y = element_text(vjust = 0)
        )
  #ylab("Endorsement model prediction")+
  #xlab("Hypothesized feature-probability")
```


```{r simulations, fig.cap = "Computational model behavior. A: Prior distributions over feature-probabilities for 6 example generalizations. B: Different fixed thresholds (columns) give rise to different posterior interpretations. C: The interpreter model averages over all thresholds, producing posterior distributions that strongly depend upon the priors. D: Endorsement model predictions, assuming a particular referent feature-probability. Shapes of the priors were chosen to intuitivly correspond to properties labeling the distributions.", fig.width = 12, cache=F}
p1 <- fig.sims.priors +
  theme(plot.margin = unit(c(18,2,6,6), "pt")) #+ ggtitle("Feature-probability priors")
p2 <- fig.sims.fixed.thresholds +
  theme(plot.margin = unit(c(6,0,6,0), "pt")) + ylab("")
p3 <- fig.sims.uncertain.threshold+
  theme(plot.margin = unit(c(18,2,6,6), "pt"))+ 
    theme(strip.text.y = element_blank())#+ ggtitle("Interpreter model posterior")
p4 <- s1.simulations.scatter + theme(plot.margin = unit(c(18,0,6,6), "pt"))
  
prow <- plot_grid( p1 + theme(legend.position="none"),
           p2 + theme(legend.position="none"),
           p3 + theme(legend.position="none"),
           p4,
           align = 'vh',
           labels = c("A", "B", "C", "D"),
           hjust = -1,
           nrow = 1,
           #rel_widths = c(1.2, 3, 2)
           rel_widths = c(1.2, 3, 1.05, 1.7)
           )
prow
```


```{r eval = F}
all.simulations.refactored <- all.simulations %>%
  mutate(Distribution = paste(Distribution, utterance, sep = "_")) %>%
  filter(Distribution != "Some", Distribution != "Most") %>%
  mutate(Distribution = factor(Distribution, levels = 
                                c( "Prior_generic", "Posterior_Generalization_generic"),
                                labels = c( "Prior/Silence", "Generalization")
                             # c("Prior_some",  "Posterior_Generalization_generic", "Posterior_Generalization_some"),
                             #    labels = c("Prior/Silence",  "Generalization (Uncertain threshold)", "Fixed threshold")
                             ),
         PriorShape = factor(PriorShape, 
                             levels = c("barks", "hasSpots", "doesntEatPeople", "laysEggs", "isFemale", "carriesMalaria"),
                            labels = c("bark", "have spots", "don't eat people", "lay eggs", "are female", "carry malaria")
                            # levels = c("uniform", "barks","laysEggs","carriesMalaria","isFemale"),
                             #labels = c("uniform", "bark","lay eggs","carry malaria","are female")
                                  #"eatsFood","areSick", "uniformRare", 
                                  #"carriesMalaria"
                                  )
         )

l0.simulations.distributions <- ggplot(all.simulations.refactored, aes(x = value, #y = prob, 
                #lty = Distribution, 
                color = PriorShape, fill = PriorShape, group = Distribution, alpha = Distribution, linetype = Distribution))+
    geom_density(aes(y = ..scaled..),
                 size = 0.6,  adjust = 1.5)+
    #geom_bar(stat = 'identity', position = position_dodge(), color = 'black')+
    theme_few() +
    scale_color_solarized()+
    scale_fill_solarized()+
    scale_alpha_manual(values = c(0.7, 0.3))+
    scale_linetype_manual(values = c(1, 3))+
    #scale_fill_manual(values = c("#636363", "#abdda4", "#2b83ba", "#d7191c"))+
    #scale_color_manual(values = c("#636363", "#abdda4", "#2b83ba", "#d7191c"))+
   #scale_fill_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    #scale_color_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    #scale_linetype_manual(values = c(3, 4, 2, 1))+
    scale_x_continuous(breaks = c(0, 0.5, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    xlab("Feature-probability h") +
    ylab("Scaled prior probability density") +
  facet_wrap(~PriorShape, scales = 'free')+
  guides(fill = F, color = F)+
  theme(strip.text.y = element_text(angle = 0),
          legend.position = c(0.92, 0.35))


  
l0.simulations.distributions <- ggplot(all.simulations.refactored, aes(x = value, #y = prob, 
                color = PriorShape, fill = PriorShape, group = Distribution, alpha = Distribution, linetype = Distribution))+
    geom_density(aes(y = ..scaled..),
                 size = 0.6,  adjust = 1.5)+
    theme_few() +
    scale_color_solarized()+
    scale_fill_solarized()+
    scale_alpha_manual(values = c(0.7, 0.3))+
    scale_linetype_manual(values = c(1, 3))+
    scale_x_continuous(breaks = c(0, 0.5, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    xlab("Feature-probability h") +
    ylab("Scaled prior probability density") +
  facet_wrap(~PriorShape, scales = 'free')+
  guides(fill = F, color = F)+
  theme(strip.text.y = element_text(angle = 0),
          legend.position = c(0.92, 0.35))


```

```{r simulationsOld, eval = F, fig.cap = "Model behavior. Prior distributions over feature-probabilities (Left; grey) and listener model posterior distributions upon hearing an utterance with an uncertain threshold (Middle; red) and a fixed threshold (Right; blue), for five different schematic prior distributions. The uncertain threshold (Middle; red) is hypothesized to correspond to utterances that convey generalizations, while the fixed threshold (Right; blue) is a control model. Shapes of the priors were chosen to intuitivly correspond to properties labeling the distributions.", fig.width = 14, fig.height=6, cache=F}
grid.arrange(l0.simulations.distributions, 
             s1.simulations.scatter, ncol = 3,
             layout_matrix = cbind(c(1,1), c(1,1), c(2, 2)))

```

Figure\ \@ref(fig:simulations) (left; filled colors) shows six prior distributions over feature-probabilities, representing the relevant background knowledge about each property.
These distributions can be thought of as distributions over categories, displaying only the feature-probability for the property in question.
For example, many different kinds of animals *do not bark*; thus, there is substantial probability mass near 0 for the prior distribution over the feature-probability of \textsc{barks} (top left; solid pink distribution). 
In addition, among the categories that do have instances that bark (e.g., \textsc{dogs}, \textsc{seals}, ...), the probability is quite high that an instance will bark; thus, the feature-probability distribution has substantial mass at relatively high values, with the maximum probability-density occuring around 0.9 (though with quite a bit of uncertainty around this value).

The lighter distribution is the interpretation distribution given by Eq. \ref{eq:L0}. 
This gives the literal interpretation of a generalization about a previously unknown category (e.g., "Feps bark.") given the background knowledge about the property.
For instance, the interpretation of a generalization about the property \textsc{barks} is that the feature-probability is quite high, near 1 with some uncertainty (top left; lighter pink distribution).
This would correspond to a rather strong interpretation (i.e., "Almost all feps bark").
This is not the case for the \textsc{lays eggs} distribution (a la "Feps lay eggs"; bottom left, green). 
Given the background knowledge about that property----not that many animals lay eggs, and among those for which the feature is present, the probability is about 0.5 (because it's only present in the females)----the interpretation is much more conservative: "About half of feps lay eggs".
The model can even derive very weak interpretations, as is shown in the \textsc{carries malaria} distribution (bottom right, purple).
Again, the literal interpretation depends upon the background knowledge in a deep way.

The endorsement model's predictions for six generic sentences are shown in Figure\ \@ref(fig:simulations) (right).
Given a feature-probability of a referent category $h'$ (e.g., $P(x \text{ lays eggs} \mid x \text{ is a robin})$; Fig.\ \@ref(fig:simulations) right x-axis) and the background knowledge of the interpreter (Fig.\ \@ref(fig:simulations) left), the endorsement model (Eq. \ref{eq:S1}) predicts an endorsement probability (Fig.\ \@ref(fig:simulations) right y-axis).
We see the model correctly predicts "Dogs bark", "Robins lay eggs", and "Mosquitos carry malaria" are all true, despite the referent feature-probabilities varying substantially between these three examples.
Intruigingly, the model predicts "Robins are female" (which has the same feature-probability as "Robins lay eggs") should receive intermediate endorsement: It is neither true nor false.
To understand this prediction, examine the interpretation distribution for the \textsc{are female} distribution: It is identical with the prior (Figure\ \@ref(fig:simulations) left; blue distributions).
The endorsement model decides if it is better to produce the generalization or stay silent (which results in the interpretation distribution being the prior). 
In this case, there is no difference in interpretation, and thus, the model is completely ambivalent about whether or not to endorse the statement.
This can be contrasted with cases such as "Sharks don't eat people" and "Kangaroos have spots", which the model thinks would be misleading to say (i.e., better to say nothing).
\mht{For further discussion about the behavior of the model, and these six examples, see Appendix?}

These simulations show how the model predicted endorsement depends upon the background knowledge about the property $P(h)$ as well as the feature-probability $h'$ communicated. 
We chose reasonable, hypothetical values for each of these, and saw that the endorsement model recapitulated standard intuitions about the truth or falsity of classic examples from the generics literature.
In what follows, we test this theory for a wide range of generalizations, including generalizations of different types (categories, events, and causes).
We do this by both measurning and manipulating background knowledge and feature-probability communicated and relate it to human elicited endorsements of generalizations.

<!-- that the behavior of the uncertain-threshold semantics is distinct from both the prior distribution over probability $h$, as well as the inferences derived from a fixed-threshold semantics. -->
<!-- For these simulations, we had specify both the feature-probability priors $P(h)$ and the referent feature-probability $h'$.. -->
<!-- In the rest of this paper, we both meausure and manipulate $P(h)$ and examine the behavior of the speaker model in three case studies: generalizations about categories, events, and causes. -->

<!-- These different semantic models make different predictions easily seen when considering a uniform prior distribution over $h$ (Figure\ \@ref(fig:simulations); top row). -->
<!-- The fixed-threshold model rules out only the lowest possible feature-probabilities ($h \sim 0$) and returns a posterior very similar to the prior. -->
<!-- The behavior of the generalization is distinct from a fixed-threshold model: The higher the feature-probability $h$, the more likely it is after hearing the generalization.^[ -->
<!-- The intuition behind the model's behavior can be gleaned by imagining what the listener model $L$ would believe given different thresholds, and averaging over those possibilities. -->
<!-- If the threshold $\theta$ is very high, only the highest feature-probability $h$ would pass the threshold. -->
<!-- If the threshold $\theta$ is slightly lower but still high, the highest feature-probabilitys $h$ would still pass the threshold, as would some that are slightly lower. -->
<!-- As the threshold $\theta$ takes on lower and lower values, more and more $h$'s would surpass it. -->
<!-- When $\theta$ is very low, almost all $h$ are consistent with it (akin to fixed-threshold model). -->
<!-- The listener averages over these possibilities: $h$'s with higher values pass more thresholds and the result is that higher $h$'s are more likely *a posteriori*. -->
<!-- ] -->

<!-- Next, we consider a few classic cases from the literature on generics to demonstrate the kind of inferences our listener model would draw from different generic statements. -->
<!-- The strength of a listener's inference about $h$ (i.e., how prevalent is the feature?) depends in systematic ways on the prior belief distribution over $h$. -->
<!-- In Figure\ \@ref(fig:simulations) (row 2), we show a schematic prior distribution that is bimodal with peaks near 0\% and some high feature-probability values; this prior could be over the feature-probability of *barking* because many kinds of creatures do not bark (i.e., have $h \sim 0$; e.g., cats), but *for those that do have members that bark*, many of them bark (i.e., have some high $h$). -->
<!-- The fixed threshold model (right column) struggles to differentiate those that generally do not have the feature from those that usually have the feature, returning a posterior distribution very similar to the prior.^[ -->
<!-- We model "those that do not generally have the feature" as a Beta distribution with an expected value (mean) close to zero.  -->
<!-- The fact that this component of the prior is not just a Delta function at 0\% reflects the intuitive possibility that some members of a kind who do not generally have the feature (e.g., a cat who barks) could somehow acquire the feature from accidental or transient causes (e.g., a strange genetic mutation). -->
<!-- ] -->
<!-- The uncertain threshold model, however, easily rules out the categories that do not generally have the feature and strongly signals the property is widespread in the category. -->

<!-- How does the model handle the puzzling case of "Robins lay eggs" vs. "Robins are female"? -->
<!-- In each case, we must consider the feature-probability prior:  $P(h_\text{lay eggs})$ and $P(h_\text{are female})$. -->
<!-- For being female, no matter what animal category a person might think of, the feature is prevalent in the exact same proprotion (i.e., 50\% of X are female; Figure\ \@ref(fig:simulations) bottom row). -->
<!-- For laying eggs, it really depends: If a person thinks of a falcon, then 50\% of that category lays eggs; if a person thinks of a bear, then 0\% of them lay eggs. -->
<!-- This dependency is reflected in the corresponding $P(h)$, which is a bimodal distribution with peaks near-0\% and near-50\% (Figure\ \@ref(fig:simulations) row 3). -->
<!-- For both priors, the interpretation upon hearing the generalization is the same (i.e., roughly 50\% of the category has the feature; Figure\ \@ref(fig:simulations) rows and 5, middle red panel). -->
<!-- The difference in endorsement comes from the comparison to the prior: For being female, the listener is very likely to arrive at 50\% under her prior, but for laying egges, this is not the case.  -->
<!-- In other words, "Robins are female" has little very informational content as the listener's posterior is nearly identical with her prior (i.e., it doesn't matter if the speaker says the generalization or stays silent). -->
<!-- "Robins lay eggs", however, has a lot of informational content. -->

<!-- Finally, we look at how the uncertain threshold produces an interpretation for the statement "Mosquitos carry malaria", which intuitively should correspond to "Some mosquitos carry malaria". -->
<!-- The prior $P(h_\text{carry malaria})$ assumes that most creatures do not carry malaria and among those that do, the feature is not very widespread. -->
<!-- Here, we see that listener's posterior upon hearing the generalization (the uncertain threshold) is similar to what she learns upon hearing "Some mosquitos carry malaria" (the fixed threshold; Figure\ \@ref(fig:simulations) row 4).  -->
<!-- Thus, a speaker who believes that *very few* mosquitos carry malaria will still endorse "Mosquitos carry malaria". -->


```{r child = 'case1-generics.Rmd'}
```

```{r child = 'case2-habituals.Rmd'}
```

```{r child = 'case3-causals.Rmd'}
```

# General Discussion

It is a remarkable fact that so much is learned from ideas expressed vaguely in words.
Generalizations in language (e.g., "John runs.", "Dogs are friendly.", "Staring at the sun makes you go blind.") are a premier example of how simple statements---statements understood by even the youngest language users---can display complex sensitivities to context.
We have argued that the core meaning of such linguistic expressions is, in fact, simple, but underspecified. 
To our knowledge, this is the first formal theory of generalizations in language that makes precise quantitative predictions about human behavioral data. 

As a formal theory of generalizations in language, we aim to unify significant swaths of language that are, on their surface, quite different from one another: generalizations about categories, events, and causes.
<!-- What these expressions have in common is *genericity*, the fact that these statements convey generalizations.  -->
We have been able to formalize a unified, core meaning using the underlying scale of *feature-probability*.
We showed that all three kinds of generalizations exhibit the same kind of sensitivity to context (Expt. 1, 2a \& 2b, 3).
Our framework naturally accounts for a number of classic examples from the semantics of *generic language* (Expt. 1).
We provide an avenue for top-down moderators on language use, by elucidating the scale as defined by a subjective, predictive probability, not mere frequency (Expts. 2c \& 2d).
Finally, we have shown that listeners' prior knowledge about the feature-probability scale is causally related to their endorsement of statements of genericity (Expt. 3).
In addition to unifying seemingly disparate parts of language, this paper provides a theoretical framework for asking further questions about genericity, which we outline below

<!-- The theory we present here goes beyond previous accounts in a number of useful ways. -->
The foremost contribution of our theory is that it is accompanied by a formal model, which makes precise quantitative predictions about endorsments.
The formal Bayesian model provides a clean separation of the semantics of a generalization (operationalized in terms of the likelihood function) from background, world knowledge (operationalized in terms of the prior).
This formal separation is key improvement beyond previous accounts. 
Previous accounts have either tried to account for context-sensitivity by baking it into the semantics [e.g., @Cohen1999] or positing a semantics that cannot be separated from world knowledge [e.g., @Leslie2008].
Our model formalizes both the semantics of generalizations and the structure of world knowledge; this separation allows us to articulate a route for precise quantitative inlfuence of conceptual structure on generic language [c.f., @Leslie2008].
<!-- Our separation of world knowledge from the semantics of generics provides a substantial, but well-circumscribed role for conceptual structure to influence generic production and comprehension. -->
We do this without having to posit that conceptual structure and semantics are one in the same.
This is makes it possible to connect with theories of formal semantics as well as applied Natural Language technologies that seeks to draw inferences from information conveyed in generalizations in text [e.g., @Herbelot2011].
<!-- Statistical accounts in particular [@Cohen1999; @Nickel2008] -->

<!-- @Nickel2016 notes that genericity is particularly challenging to study because it intersects with many other phenomena in natural language (e.g., gradable adjectives in "Giraffes are tall"). -->
<!-- We agree, and add to this that genericity intersects with *world knowledge* in subtle ways, and it is critical to accurately describe and measure world knowledge before making claims about the semantics of a generalization. -->
<!-- We used the most basic tests to validate our model: Production was examined in terms of *endorsement*, which can be seen as a special case of production when the only available alterantive utterances are the assertion or its negation. -->
<!-- Comprehension was examined in terms of *implied prevalence*, which in our theory is most basic inference a hearer can make upon hearing a generalization. -->
<!-- Though these are highly abstracted tests of the model, the modeling framework in principle allows for elaboration to a more complete production and comprehension model.  -->
<!-- The model is a communicative model, which includes a component corresponding to *comprehension* of genericity (i.e., a listener model).  -->

In the rest of this discussion, we elaborate some key features of the theory that warrant future investigation, sketch out arguments for other known philosophical puzzles in genericity, and situate our theory with respect to extant theories of genericity.
<!-- We conclude by sketching out arguments for other known philosophical puzzles in genericity. -->

## A step towards communicating generalizations
<!-- Implicit aspects of language use: The comparison class and question under discussion -->

In this paper, we proposed a model for interpreting and endorsing generalizations in language by formalizing a simple semantic theory in a probabilistic model with structured, prior knowledge.
Our pair of models can be seen as a special case of a Rational Speech Act model, a rational framework of pragmatic language understanding [@Frank2012; @Goodman2016].
Our endorsement model, which we use to generate predictions for our three case studies, can be seen as a special case of a speaker model who can only produce one of two utterances: the generalization or silence. 
Our interpreter model can be seen as a kind of "literal listener", who does not do any sort of pragmatic or communicative reasoning. 
Our modeling framework could thus naturally be extended to more faithful models of pragamtic communication of generalizations, to explore when and how people choose to communicate their abstract knowledge when they have more options than in a two-alternative forced-choice task.
The view of our endorsement model as a minimal communicative model highlights aspects of our modeling approach for further future development. 

### The comparison class

Our model of an interpreter, or naive listener, has access to implicit statistics of the event or property in question, represented by the prior belief distribution over the feature-probabilities $P(h)$.
The feature-probability priors for a property, event, or cause are constructed by considering *other possible* categories having the property, people doing the action, or causes producing the effect.
Collectively, these other kinds, people, or causes form *comparison classes* against which the referent-category is evaluated.
Thus, the feature-probability prior $P(h)$ is constructed with respect *the comparison class* $C$: $P_C(h)$.

The existence of comparison classes is uncontroversial in the study of *vague* or *underspecified* language [e.g., gradable adjectives like *tall* and vague quantifiers like *many*; @Bale2011; @Solt2009].
Adult judgments of the felicity of gradable adjectives like *tall* or *dark* depend upon fine-grained details of the statistics of the comparison class [@Qing2014; @Schmidt2009; @Solt2012] in ways directly analagous to the context-sensitivity of the language of generalizations explored in this paper.
Thus, we conclude generic, habitual, and causal language can be seen as a special case of *vague language*. 

The construction of a comparison class adds another layer of flexibility into the use of generalizations in language. 
Objects can be conceptualized and categorized in multiple ways, giving rise to multiple, logically-possible comparison classes (e.g., a robin is a bird, is an animal, is an object). 
We have begun doing work investigating how pragmatic reasoning can help constrain this open-ended problem [@Tessler2017].
Even if interlocutors coordinate on a particular comparison class, they may disagree about the feature-probabilities of other *other categories* in the class.
That is, our model makes the prediction that the acceptance or rejection of the statement "Humans cause global warming" or "Muslims are terrorists" depends upon a speaker's beliefs about how *other forces* (e.g., plate tectonics) influence the climate or how often other social categories (e.g., neo-nazis) commit acts of terrorism.
Indeed, we saw in Expt. 3 how the statistics of other causes can influence the endorsement of a single generalization (e.g., "Herb C makes animals sleepy").
Understanding how knowledge of the comparison class influences language use is an exciting new area for future research in the language of generalizations.

### Question under discussion

Rational models of communication, of the kind we have developed in this paper, are required to make explicit a speaker's assumptions about the goal of communication, otherwise known as the Question Under Discussion or QUD [@roberts1996qud]. 
We have made a very minimalist assumption, that the goal of communication (at least, for purposes of endorsement) is the feature-probability.
However, the choice in constructing a comparison class can be thought of as the result of addressing a particular QUD.
For example, the comparison classes used in our case studies were with respect to the target category (*other animals*, *other people*, *other possible causes*); thus, our model addresses the implicit QUDs of "*what* has feature?", "*who* does action?", or "*what* causes effect?".
Were the QUDs to be reversed (e.g., "what features does category have?", "what does person do?", "what effects does cause bring about?"), the comparison classes would need to be constructed with respect to the feature (e.g., *other features*, *other actions*, *other effects*).

The QUD is thus another layer of flexibility that makes understanding generalizations so challenging.
The same sentence can be used to address multiple QUDs [@KrifkaGenericBookFocus].
The statement "Lawyers care about the law." can be interpreted with a category-wise comparison class (i.e., "Lawyers *(as opposed to doctors, firefighters, ...)* care about the law."; imagine in a pedagogical context) or a feature-wise comparison class (i.e., "Lawyers care about the law *(as opposed to justice, doing what's morally right, ...)*" e.g., when said sardonically).
In this work, we focused on category-wise comparison classes for methodological convenience. 
<!-- In our three case studies, the *category-wise* comparison classes are fixed to *other people*, *other causes*, or *other kinds*, even as we vary the features. -->
<!-- *Feature-wise* comparison classes are likely to be modulated by the feature itself. -->
Future work should investigate the feature-wise reading of generalizations, and explore what cues listeners adopt for one or the other interpretation.

<!-- In the minimal contexts employed in our experiments, this is a reasonable assumption. -->
<!-- There is a different assumption we could have made, however: Another clear way to construct the comparison class is with respect to other features.  -->
<!-- This *feature-wise* comparison class would correspond to addressing the implicit QUDs: *what does \textsc{person} do?*, *what effects does \textsc{cause} bring about?*, and *what features does \textsc{kind} have?* -->



<!-- Throughout our experiments in this work, we have focused on sentences about animals.  -->
<!-- In addition to being the main focus of past theoretical and empirical work, focusing on animals is methodologically convenient as the comparison class for generics about animals is quite naturally \emph{animals}. -->
<!-- When we look beyond generics about animals, deciding what goes into a comparison class becomes less clear. -->
<!-- There are some hints that the comparison class can be derived with respect to the property [@Keil1979], but may involve pragmatic reasoning as well. -->
<!-- For example, the statement "iPhones are useful" could be in comparison to other forms of technology (like a desktop computer), while "iPhones are heavy" could really only be informative relative to other handheld devices. -->

<!-- The incorporation of a comparison class into the study of generic language might help elucidate other puzzles concerning generics. -->
<!-- Recent work in philosophy and linguistics, for instance, suggest generic language is context-sensitive [@Nickel2008; Sterken2015]. -->
<!-- @Nickel2008 points out that \emph{Dobermans have floppy ears} may be true in the context of a discussion of evolutionary biology but that \emph{Dobermans have pointy ears} is true in a discussion of dog breeding. -->
<!-- Our theory provides a hint from where to begin to understand this context sensitivity: the comparison class. -->
<!-- Different conversational contexts could bring to mind different comparison classes, in a way analogous to the context-sensitivity of  gradable adjectives (e.g., \emph{tall}).  -->
<!-- Hearing that "Abigail is tall" means different things is Abigail is 20 years old or if she is 4.  -->
<!-- Future work will be needed to explore whether a pragmatic inference approach is also relevant to establishing the comparison class, and what background knowledge about properties, categories, and context is relevant. -->

## Structured prior knowledge

<!-- -- add tomasello about learning about how to take turns, as a generalization about an event type which is the event of how the game is supposed to be played -->

Previous psychological and philosophical work on generics has looked beyond feature-probabilities and focused on conceptual distinctions and relations [@Gelman2003; @Prasada2013; @Leslie2007; @Leslie2008].
Prasada has argued for a distinction between \emph{characteristic} properties (e.g., "Diapers are absorbent") and \emph{statistical} properties (e.g., "Diapers are white").
Leslie suggests information that is striking (e.g., "Tigers eat people") is ecologically useful and thus permitted to be a generic.
Gelman outlines how generics tend to express \emph{essential} qualities that are relatively timeless and enduring. 
Where in the probability-based semantics could such conceptual distinctions come into play?

Our approach makes the strong claim that beliefs about predicted feature-probability are the connective tissue between conceptual knowledge and the semantics of generic language.
That is, the effect of conceptually meaningful differences on generic language is predicted to be mediated by differences in corresponding feature-probability distributions.
Modulations of feature-probability distributions can occur for multiple reasons (e.g., by changing the comparison class) as described in the previous section.
We found in Expts. 1 \& 2 that conceptually different kinds of properties and events gave rise to quantitatively different feature-probability priors, even while keeping the comparison class fixed. 
In Expt. 3 we showed that these feature-probability distributions are causally related to endorsement.

It is natural to ask how feature-probabilities might reflect conceptual knowledge.
We found that empirical feature-probability distributions are structured in a mixture-model that plausibly reflects intuitions about causal mechanisms underlying different properties; the differences in shape of these distributions in turn led to variable endorsements of generalizations. 
It is plausible that richer conceptual knowledge also influences these distributions, such as higher-order conceptual knowledge about the nature of properties and categories [@Gelman2003; @Keil1992].
Indeed, conceptual structure in general, including higher-order abstractions, can be captured by probabilisitic causal models [@pearl1988probabilistic; @Gopnik2003theory] and their generalization in probabilistic programs [@Goodmanconcepts].
Future work will be needed to explore whether probabilistic representations of conceptual knowledge can capture the relations identified in other accounts of generics (such as principled, essential, and striking properties), and whether the effect of these relations can then be adequately described via their impact on feature-probability priors.

## Communicating predictive probabilities

This paper puts forth the theory that the language of generalization communicates predictive probabilities from speaker to hearer.
In previous accounts, feature-probability is treated as synonymous with the prevalence or frequency of the feature in the category. 
We make the important distinction that probability can be more general than frequency, and in our account, underlies speakers' subjective, predictions about the future and is subject to top-down moderator.
Expts. 2c \& 2d that showed using participants' \emph{predictions} about the future as the object of communication in our formal model perfectly tracked habitual endorsement, when the present frequency would make the wrong prediction.

Communicating probabilities might seem contrary to a basic tenet in cognitive psychology, that human cognition falls short when reasoning about probabilities [@Tversky1974].
Our theory suggests that the problems with understanding probabilities observed in classic, cognitive psychology paradigms are a problem in understanding the *language used* to convey *explicit probabilties* [cf., @Levinson1995]. 
Rather than conveying *explicit probabilities*, generalizations we argue convey *implicit probabilities*, which are easier to understand and quite useful for statistical reasoning.
Given the relationship to *vague language* (described above), we argue that the utterance "70% of birds fly" is analagous to saying that "John is 6'3"", while saying "Birds fly" is more similar to saying "John is tall".
The latter statements are easier to process, understood at an earlier age, and may be more useful for human reasoning. 
Indeed, young children are actually quite good about reasoning about probabilities, but in ways that are not explicit or tied to language [@Gweon2010; @Dewar2010].

<!-- One might also question the feature of our computational model that the speaker is explicitly trying to communicate $h$, her subjective probability.  -->
<!-- We note that if the prevalence prior is structured, as we have argued throughout, then the Question Under Discussion that our speaker model is addressing can also be formalized to be "What component of the prior does the referent-category belong?". This model with a modified QUD makes almost identical quantitative predictions as the simple model without this abstract QUD.  -->

<!-- ### The Problems with Communicating Generalizations -->

<!-- Thus, in order to define a generalization about a category, there must be some corresponding concrete particular instance of that category from which the generalization is formed.  -->
<!-- For example, if an agent observes $n$ instances of \textsc{dog} ($d_1, d_2, ..., d_n$) -->
<!-- For example, if an observer forms a generalization about \textsc{dogs}, she must some way of determining when she is in an instance of the category \textsc{dogs}: She must be able to *individuate*. -->
<!-- In Hume's words, generalizations are predictions about "instances of which we have had no experience resemble those of which we have had experience" [@HumeTHN]. -->

<!-- Generalizations can be made about almost anything. -->
<!-- Here, we restrict our focus to generalizations about categories (whose linguistic expression is known as *generic language*), which have been the primary focus of psychologists, linguists, and philosophers. -->
<!-- We posit that our analysis should expect to generalizations about events (habitual language) as well as generalizations about causal forces (causal language). -->

<!-- Generics express a relation between a kind K (e.g., \textsc{robins}) and a property F (e.g., \textsc{lays eggs}), such that the property can also be said to be applicable of an individual (i.e., the bird in my backyard lays eggs). -->
<!-- Bare plural statements (e.g., \emph{Robins lay eggs}) tend strongly to yield a generic meaning [@Carlson1977], though other forms can express such a meaning sometimes (e.g., \emph{A mongoose eats snakes.}). -->

<!-- Given that generics express a property that can be applied to individuals, it would seem intuitive that the number of individuals with the property would be what makes the statement true or false. -->
<!-- Counter-examples like \emph{Mosquitos carry malaria} and \emph{Birds lay eggs} v. \emph{Birds are female} stifle such intuition.  -->

<!-- ### Statistical Accounts of Generics -->

<!-- Statistical accounts take the \textbf{property prevalence} to be fundamental: \emph{Birds lay eggs} means \emph{Birds, in general, lay eggs}.  -->
<!-- Of course, birds do not in general lay eggs (it's only the adult, female ones that do). -->
<!-- The primary way of dealing with such issues is to posit domain restrictions ("implicitly, we are only talking about the females") when there are "salient partitions" [@Carlson1995]. -->
<!-- The most fully-developed theory on this front is due to @Cohen1999.  -->
<!-- Let's first introduce some notation: -->

<!-- ### Conceptual Accounts of Generics -->



## Acquiring the language of generalizations

The linguistic outlet for generalizations about categories---so called, *generic language*---has received tremendous attention from psychologists, linguists, and philosophers.
Generic language is one of the earliest emerging forms of complex, compositional language.
Somewhere between two and three years of age, children recognize that generics convey a generalization about a category, not directly tied to concrete instances in a scene [@Cimpian2008].
Generics are common in child-directed and child-produced speech [@Gelman1998; @GelmanEtAl2004] and are believed to be central to the growth of conceptual knowledge [@Gelman2004], 

What is perhaps surprising from a formal perspective is that generic language is far from trivial to characterize. 
If generics convey something about the probabilty of a feature, we would expect them to be in some way comparable to quantifier statments (e.g., "some", "most", "all", ...).
The extreme flexibility of generic meaning (e.g., "Birds lay eggs" vs. "Birds are female") stands in stark contrast to its early emergence in development.
In fact, quantified statements, whose formal meaning is much more straight-forward (e.g., "Some" means more than zero), emerge *later* in development [@Brandone2014; @Gelman2015], and are often confused for generic statements [@Leslie2012a].
This has led some to conclude that the normal tools for describing the semantics of quantified utterances (i.e., a truth-functional threshold) is not approrpriate for generic language [@Leslie2008].

We think rejecting the normal tools of truth-functional semantics for generics would be throwing out the baby with the bathwater.
When we consider the acquisition problem, there are three aspects of meaning that a child must acquire for a truth-functional quantifier semantics $\denote{u}(h, \theta) = \{h :h > \theta\}$: 
(1) the dimension being described (i.e., the probability $h$),
(2) the polarity of the relation (i.e., $>$ vs. $<$), and 
(3) the value of the threshold (e.g., $\theta = 0$ for "some", $\theta = 0.5$ for "most").
Upon learning the meaning has to do with probability and that it stands in a positive relation to some standard $\theta$, a rational learner would adopt a prior distribution over possible thresholds $\theta$, as a form of *lexical uncertainty* [@Bergen2016].
This prior could then be updated with more data to acquire a fixed meaning.
However, for the generic statement, the language learner is done once she acquires aspects (1) and (2).
In our model, $\theta$ is left underspecified in the semantics and is inferred in context.
Thus, in this sketch of an acquisition model, a child would first acquire generic language, and from that, carve out more precise meanings for quantified language.

The uniform prior over the threshold $\theta$ in the truth-functional threshold semantics (as we have assumed in our model) provides a secondary argument for why acquiring our semantics for generics should be easy.
Uniform uncertainty over values in the unit interval $[0, 1]$ for threshold $\theta$ in a threshold-function is mathematically equivalent to a *soft semantics* wherein the degree to which the utterance is true is proportional to the feature-probability $h$ *itself*: 

$$
\int_{0}^{1} \delta_{h > \theta} \diff \theta =  h
$$
This *soft semantics* (corresponding intuitively to a meaning like "the higher the probability, the better the utterance", or simply "more is better") is perhaps the simplest semantics one could imagine.
The difficulty in acquiring the meaning of quantifiers is then a difficulty in recognizing a fixed-threshold semantics as a special case of this *more is better* semantics.
We leave for future work the precise implementation of the acquisition model we sketch out here.

## Other philosophical puzzles

In our experiments, we have tried to include classic, philosophically puzzling examples of generalizations in language in addition to crafting novel stimuli that provide stricter tests of the main hypothesis.
In this section, we consider some other previously-discussed troubling examples of generalizations and sketch out arguments rooted in our theoretical framework to demonstrate how our theory could account for these cases. 
We consider the five following examples:

1. Books are paperbacks.
2. Mary handles the mail from Antarctica.
3. Supreme Court Justices have even social security numbers.
4. Elephants live in Africa and Asia.
5. Women are submissive.

Sentence (1), strikes hearers as *odd* or perhaps *false*, despite the vast majority of books being paperback. 
Why?
A first observation is that the sentence is constructed by pairing a category ("Books") with a property that describes a strict subset ("paperbacks" is short-hand for "paperback books"). 
We expect intuitions about this statement to generalize to other examples expressing an analogous relation (e.g., "Pastas are spaghetti", "Animals are humans").
We hypothesize that the strangeness of these examples has to do an inflexible relationship between the comparison class and the referent feature-probabilty.
Assume that the comparison class is constructed by considering the *predicability* of the property [in the sense of @Keil1979, originally from @Sommers].^[
  Note that this assumption is consistent with the comparison classes we have assumed in our three case studies.
]
Then, "is a paperback" can only be informatively said of individual books, because these are the only objects that can plausibly have the property; that is, world knowledge gives us that an elephant cannot be predicated (either true or falsely) by the property "is a paperback".^[
  Note that this idea is similar to @Cohen1999's notion of "Relative generics", which have truth conditions that are determined by a comparison to an alternative set of Ks (his   Alt(K), our comparison class) which is composed of Ks that could satisfy some F in an an alternative set of Fs.
]
The relevant aggregations of entities, then, are collection of books, potentially including books of different genres or publication eras among other such sub-categories.
Then, given that "books" is the superordinate category of "kinds of books", the referent-category feature-probability cannot be different from the mean of the feature-probability prior.
Finally, given this feature-probability prior and the referent-category feature-probability, our endorsement model would predict "Books are paperbacks" is neither true nor untrue, in the a way analogous to "Robins are female". 
 
```{r booksArePaperbacks, fig.cap="Sketch of an argument for why 'Books are paperbacks' sounds odd. The feature 'is paperback' can only be predicated (in the sense of @Keil1979) of individual books, which are organized into collections (here, by genre). The feature-probability prior is constructed by aggregating these collections of books. Then the feature-probability of 'is paperbook' for the category of 'books' can never be higher than the mean of the prior distribution. The endorsement model would give this sentence a rating of 0.5, corresponding to neither true nor false."}
knitr::include_graphics("figs/books-paperbacks.pdf", dpi = 108)
```


Imagine there is a job in the local beaurocrat's office to handle the mail from Antarctica and this job is assigned to Mary;  to date, however, there has never been any mail from Antarctica [@Cohen1999]. 
That is, the statement "Mary has handled mail from Antarctica" is false.
Sentence (2) is still thought to be true despite zero actual instances of the event.
We think (2) is a boundary case of generalizations expressing predictive probabilities, which we elucidated in Expt. 2c.
In in that study, we experimentally dissociated predictive probability from past frequency, and saw that only a model that took into account predictive probability could explain participants' endorsements of generalizations.
We expect the same to be true here, elucidated by the intuition that: were there to be any mail coming from Antarctica, Mary would handle it. 
Past frequency or actual prevalence in the world often tracks predictive probabilities, but language users' internal models of how the world works can lead them in certain situations to expect something different in the future.

Our focus on predictive probability makes even new prediction about some classic puzzles.
Sentence (3) is the conceptual opposite of sentence (2).
Imagine that currently every Supreme Court Justice has a social security number which is an *even number*. 
Sentence (3) is still considered false, every though the property holds in 100\% of the category [@Cohen1999].
We predict this phenomenon is the result of abstract, intuitive theories guiding us to reject observed frequencies in forming our subjective probabilities.
That is, because observers strongly believe selection for the Supreme Court is not influenced by one's social security number, speakers would assign a roughly 50\% subjective probability that the \emph{next} justice would have an even social security number.
Then, given that the all professions would have roughly the same probability of having employees with even social security numbers, Sentence (3) is again similar to "Birds are female". 
However, were we to learn much more surprising information---for instance, if every Supreme Court Justice in U.S. history had a social security number which was a *prime number*---the shear suspiciousness could compel an observer to revise their theory of the domain (appealing perhaps to a conspiracy), update their subjective probability of future instances, and then accept the generic. 
That is, we predict there are situations where statements like (3) could be considered felicitious generalizations.

Understanding how a semantic representation *composes* is another critical test for a theory of generalizations in language.
@Nickel2008 suggests Sentence (4) is semantically equivalent to: "Elephants live in Africa and elephants live in Asia".
Stastistical account of generalizations have trouble with this kind of conjunction: It cannot be the case that *most* (more than half) elephants live in Africa and *most* (more than half) elephants live in Asia, unless we are to posit *international elephants* (i.e., individual elephants who live part-time in Africa and part-time in Asia), which we would rather not have to posit.
However, this is the only interpretation if the generalization is given a fixed, majority based quantificational account (e.g., if the generic means *most*).
Our theory does not provide a fixed semantics for the generalizations, but an uncertain one, which can be updated as more information comes in.
In fact, with a strong constraint against the existence of *international elephants* but with any further semantic assumptions, our model infers that some elephants live in Africa and other lives in Asia.^[
  An implementation of this example can be found: [http://forestdb.org/models/generics-conjunction.html](http://forestdb.org/models/generics-conjunction.html).
]
Importantly, the model believes that most lives in Africa, when it only hears that "Elephants live in Africa", and only later revises it's beliefs to a weaker "some"-like interpretation after hearing they live in Asia as well.
Our model flexibly accomodates new evidence that might otherwise contradict previous utterances because it maintains uncertainty about the precise interpretation of single utterances.

<!-- Instead, the statement should be glossed as *some elephants live in Africa* and *some elephants live in Asia*, and perhaps taken collectively, *most elephants live in either Africa or Asia* [@Nickel2008].  -->
<!-- This reading of (3) gives it the same semantic content as two statements: "Elephants live in Africa. Elephants live in Asia." -->
<!-- Our model is able to flexibly accomodate this statement by having uncertainty about the truth-conditional threshold.  -->

<!-- With reasonable assumptions about the prior (e.g., that *lives in* is similar to other biological properties from Expt. 1a and assuming international elephants are unlikely *a priori*), upon the hearing "Elephants live in Africa", our model will infer that most elephants live in Africa. -->
<!-- When the model then hears "Elephants live in Asia", the model updates its beliefs to accomodate this second statement,  -->
<!-- This is possible because our model has uncertainty over the semantic thresholds in the generic claim. -->

Finally, sometimes a property is widespread relative to other kinds or groups, and it might be so systemic that we would believe it to be the case in the future, but speakers still might find it avertise to actually endorse the generalization.
Such is the situation with Sentence (5) [@Haslanger2011]. 
Due of a variety of socio-cultural reasons, @Haslanger2011 argues that women tend to be submissive (more so than men, and many other social categories), yet still cautions against asserting (5).
In our **Model Simulations** section, we showed that the genearlization often implies the property is widespread, even with relatively uninformed prior beliefs about the property. 
If the generic conveys the property is in fact more widespread than the speaker herself might believe, this can lead to a distortion of the truth. ^[
  Note that this can straight-forwardly capture the central phenomena of @Cimpian2010.
].
The current formulation of the model is relatively ambivalent about the deviations of the listener's beliefs from the speaker's belief (i.e., the speaker doesn't mind that the feature-probability that she believes to be true will be exaggerated in the mind of the listener).
However, if states of the world (or, listener's beliefs about states of the world) correspond to different subjective values, speakers who take into the account the subjective utilities of the listener may produce language that deviates from what a pure informational transmission speaker might say.
This kind of extension to rational models of communication has already been used to account for a variety of phenomena in *polite language* use [@Yoon2016; @Yoon2017].
We propose adopting such a value-based utility structure for states of the world for language about social categories: If the speaker found states of the world where the probability of the feature is high (e.g., a world where many women were submissive) as undesirable, then a speaker who considered both the informational content of the utterance and the utility in the states of the world implied by that utterance would too find it reprehensible to make such a generalization.
We find this possibilitity intruiging and leave it for future work.

In sum, we've tried to argue that our information-theoretic communicative model couched in general cognitive framework can be used to explain a number of interesting and heretofore puzzling phenomena in genericity as well as make novel predictions about a wider range of language.
The precise implementation of these hypotheses we leave for future work.

<!-- -- Books are paperbacks. (failure of the constrast class) -->
<!-- -- Birds lay eggs and are female vs. Elephants live in Asia and Africa -->
<!-- can use pragmatics to resolve if the property is conjunctive or not.  -->
<!-- -- Women are submissive. (Haslanger, 2011; generics + politeness mechanism, some states have lower value than others) -->
<!-- -- Muslims are terrorists (vs. Mosquitos carry malaria) -->
<!-- -- Mary handles the mail from antarctica (predictive propensity; also highlights domain restriction on events) -->
<!-- - Glass breaks when struck. (so-called "disposition", but here: a generalization about the event of a glass being struck [already we are domain restricted]) -->

## Relationship to other theories

To our knowledge, the present papaer presents the first formal theory of genericity in language that makes accurate and precise quantitative predictions about human behavioral data. 
A number of other theories of genericity have been proposed in the linguistics and psychological literatures.
We now turn to situating our account with respect to these previous theories.
<!-- By and large, these theories are concerned with explaining *generics* or generalizations about categories, so in keeping with the spirit of these authors, we will refer to the object of explanation, without loss of generality, as *generics*. -->
Semantic theories fall into one of two broad camps: Those that appeal to the statistics of the world (e.g., how many Ks have F) and those that appeal to structured, conceptual representations (e.g., *there is something about being K which causes it to F*).
Statistical and conceptual theories express the major contrasting views of the truth conditions of generic statements [@Carlson1995essay].^[We use the terms statistical and conceptual to refer to what @Carlson1995essay referred to as "inductive" and "rules and regulations" views, respectively.]

### Statistical accounts

#### Relative and absolute generics

Our underspecified threshold model has clear antecedents in other statistical accounts, most notably @Cohen1999 's theory of generics as a frequency adverb (e.g., "generally").
For Cohen, the statement "Birds lay eggs" means "Generally, birds lay eggs", and then he takes up the task of explaining what "generally" means.
He partitions generics into two types: *absolute* and *relative* generics.
*Absolute generics* use a fixed, 50\% threshold on feature-probability: $P(F\mid K)>0.5$. 
Roughly speaking, "Dogs have four legs" is true because a given dog is more likely than not to have four legs. 
*Relative generics* depend upon an alternative set of kinds (his notation: $Alt(K)$), analogous to our comparison class.
"Mosquitos carry malaria" is a relative generic: it is true because an arbitrary mosquito is more likely than an arbitrary member of an alternative kind to have the feature.

Our uncertain-threshold model deviates from Cohen's account in two primary ways.
First, though his theory is framed in terms of probabilities, it still utilities a fixed threshold semantics and thus is only able to make predictions about what is "true" and what is "false".
In our theory, there is uncertainty about the core semantic meaning, and this uncertainty (though reduced in context) drives the model to make graded predictions about endorsement.
Second, Cohen draws an *a priori* distinction between *Absolute* and *Relative* generics, and provides different mechanisms for each each.
We used examples of both in our stimuli (Expt. 1) and found that our single model handles both without any addition assumptions, casting doubt on the *a priori* distinction between "absolute" and "relative".^[
  For a related theory that aims to do away with the distinction between "absolute" and "relative"   adjectives, see @Lassiter2015.
]

We note additional machinery that Cohen's theory relies upon (and that is not currently required in our account) to explain the context-sensitivity of generics: contextually restricting the entities that go into the computation of feature-probability (i.e., which robins do we look at to compute the probability of *laying eggs* among *robins*?).
His theory posits that feature-probability is calculuated by only considering entities that *could have some feature* in a contextually-specified alternative set of features (so called $Alt(F)$). 
For example, the property "lays eggs" induces a set of alternatives that are associated with mechanisms of reproduction (e.g., "gives birth to live young", "undergoes mitosis", ...).
Therefore, "Robins lay eggs" is evaluated as an *absolute generic* because only individuals that are female members of kinds are under consideration, because only the female members can plausibly satisfy one of the other aternatives having to do with reproduction (i.e., the alternative features in $Alt(F)$).
The inferential machinery behind Cohen's domain restriction (i.e., what comrpises $Alt(F)$?) relies upon conceptual information, but the details remain obscure [@Carlson1995essay].^[
  However, see @Cohen2004 for a discussion of how his semantic constraints relate to different kinds of generics and different kinds of conceptual representational frameworks found in cognitive science.
]
However, we note that this mechanism may be implicit in our own theory: the prior distribution over the feature-probability of *lays eggs* presumably includes aspects of people's theories about reproduction (i.e., that only females lay eggs).
Thus, there may exist a refactorization of the uncertain threshold model to a fixed threshold where the listener has uncertainty about the relevant domain restriction. 

<!-- Of course, the inferential machinery behind this sort of domain restriction (i.e., what is $Alt(F)$?) requires further theorizing to explain.  -->

#### "Normal" accounts

The primary alternative view under the statistical banner draws on the intuition that generics express something normative in the world [@Asher1995; @Pelletier1997; @Nickel2008].
For example, "Dogs have four legs" is a good generalization event though, regrettably, not all dogs have four legs.
However, were the world to function *normally*, then *all* dogs would have four legs (i.e., dogs would not be involved in freak-tractor accidents).
The idea that our beliefs about what is normal in the world influences our judgments about generalizations has intuitive appeal for rejecting accidentally true generalizations (e.g., "Supreme Court justices have even social security numbers"; described above) and tereotyped language (e.g., "Boys are good at math").
Our theory does not directly formalize "what is normal", though we argue that a speaker's beliefs about what is *probable* [which may relate to what is *normal*; see @Icard2017] plays a role in endorsing and interpreting generalizations.
We showed in Expt. 2c that a speaker's beliefs about what is likely to be the case in the near future matters above and beyond what is currently true in the world for endorsing generalizations.
Our notion of feature-probability is as a predictive probability, which incorporates background knowledge in order to make predictions about the future. 

#### Underquantification

A proposal similar to our account concerning underspecification of generics has been made in the computational linguistics literature [so-called *underquantification*; @Herbelot2011].
In their model, generics express an explicit quantified relation, specifically either "Some", "Most", or "All".
The task of the computational linguist, then, is to construct a set of features that accurately predicts (relative to human judgments) the quantified relationship expressed by the generic [analagous to a quantifier version of the *implied prevalence* task used in @Gelman2003b; @Cimpian2010]. 
Our semantic theory can be seen as a generalization of the *underquantification* to a continuous interval of possible meanings.
This distinction is relevant for the acquisition of the language of generalizations; we do not think that quantified relations (e.g., "Some", "All") are in any way primary or special. 
<!-- Formally, whereas the *underquantification* account posits that a generic is *ambiguous* between various meanings, we argue that the generic is *vague*, and has a continuum of possible meanings.  -->
Additionally, by using a vague, underspecified threshold, our formulation naturally extends to other scales and other kinds of generalizations (e.g., habitual language), where quantified relations like "Most" or "All" are not directly applicable. 

#### Generic as indexical

@Sterken2015 has recently proposed an intruiging account of the context-sensitive of generic language by treating generics as a kind of *indexical* (e.g., "this" or "I").
The variability of endorsements is then attributed to mechanisms of domain restriction (described above with @Cohen1999) and context-sensitive "quantificational force".
Our uncertain-threshold semantics may be a precise formalization of @Sterken2015's "quantificational force" (and a generalization of *underquantification*, see above), and we have argued above that domain restriction is not a necessary mechanisms for the cases considered in this paper. 
We leave for future work exploring how the context-sensitivity of an indexical may differ from the kind of context-sensitivity in our model.

### Conceptual accounts

Conceptual accounts of generics emphasize the structure of generic knowledge [@Prasada2000], and view generic utterances as the way of expressing special mental relationships between kinds and properties [@Leslie2008; @Prasada2012].
These accounts start with the perspective that generics express rich, conceptual relationships between kinds and properties.
That is, "Bishops move diagonally" not because of a statistical relationship, but rather, because those are the rules of the game; if you start moving the bishop other ways besides diagonally, you cease to be playing chess.
If a lion lacked the capacity to roar, we might consider a less good example of being a lion because "lions roar".

The most influential conceptual account of generics is from @Leslie2007.
For Leslie, generics are tied to the cognitive system's "default mode of generalization", which infants seem ready to make use of by drawing strong generalizations from small amounts of data [e.g., @Baldwin1993].
Leslie's "default mode" comes equipped with the ability to single-out \emph{striking properties} (e.g., properties which are dangerous or appalling) as particularly useful aspects of the world to know about.
Hence, "Mosquitos carry malaria" is true because malaria can kill you and is useful to know about.

This accounts predicts that generics that convey striking properties should be endorsed even when they are relatively rare in the category (i.e., low referent-category feature-probability; e.g., "Sharks attack swimmers").
Indeed, naturalistic examples (e.g., "Sharks attack swimmers") and artificial examples (e.g., "Lorches have purple feathers")  supplemented with striking information (e.g.,"These feathers are as sharp as needles and can easily get lodged in you, causing massive bleeding") are endorsed when the feature-probability is quite low (e.g., 30\% of lorches have purple feathers; @Cimpian2010; see Expt. 1 for naturalistic cases).
In pilot work, using a paradigm similar to the prior manipulation paradigm employed in the causal language experiments (Expt. 3a), we have observed that the prior distribution over the feature-probabilities $P(h)$ change when supplied information about the dangerousness of the feature.
That is, our pilot work suggests that experiments that experimentally induce beliefs about the dangerousness of a feature also manipulate the feature-probability priors. 
Thus, our model would also predict these effects, but by way of the feature-probability priors rather than some direct connection between generics and striking features.
<!-- However, we do not dispute that extra-linguistic factors can play a role in endorsements of generalizations (e.g., a speaker may not *want* a hearer to believe the generalization, which she herself knows to be true, or visa versa; more on this below). -->

Of course, not all generics convey striking or appalling information (e.g., "Birds lay eggs").
Leslie's default mode, thus, also distinguishes *negative counter-instances* of a property (e.g., a bird that doesn't lay eggs, such as a male bird) from *positive counter-instances*  (e.g., a hypothetical bird that bears live young)
The conceptual account argues that generics are much less reasonable when *positive* counter-instances exist. 
For example, "Birds are female" seems weird because *being male* is a *positive* counter-instance of *being female*, but since there are no birds that bear live young (i.e., no *positive counter-instances*),  "Birds lay eggs" is fine.
In the current version of our theory, both kinds of counter-instances influence the referent-category feature-probabilities, though only positive counter-instances could impact the statistical details of the comparison class (in particular, when constructed with respect to the feature; see discussion about the Comparison Class).
Thus, the notion of *positive* and *negative* counter-instances may result from a particular way of looking at the constructs posited in our model, namely feature-probabilities within- and across- categories or features.

Finally, generics are not limited to conveying rich, conceptual information; they are also used to describe facts of the world. 
"Ravens are bigger than toasters" is a true statement, but not because of a rich relationship between ravens and toasters.
"Barns are red" because most barns are red.
If farmers around the world decided to paint their barns green, the things they would be painting would still be barns; in that world, we might say "Barns are green".
Thus, conceptual accounts must be supplemented with the proviso that in ceratin situations, the statistics of the world are relevant. 

Our view is that subjective probabilities can be formed either from worldly observations or because of the structure of people's mental representations.
In the last fifteen years, there has been tremendous progress in formalizing 
Rich, structural mental representations can be formalized in probabilistic models, which can be used to derive quantitative predictions about inferences derived from that knowledge [@Tenenbaum2011]. 
For example, @Kemp2008 introduced a model for learning different kinds of structural relationships between objects and properties (e.g., taxonomies, social cliques, dominance relations).
@Goodmanconcepts argue that concepts and intuitive theories can be constructed from elementary random primitives using insights from *probabilistic programmming*. 

In this paper, we take a minimalist approach to modeling conceptual structure, incorporating only what is necessary into the interpreter's background knowledge, the prior on feature-probabilities $P(h)$.
In our Bayesian data analysis of background knowledge, we use a mixture model that reflects basic knowledge about kinds and properties (e.g., that most kinds don't have most properites).  
It's plausible that more richly structured knowledge of the kind described by @Prasada2013 and @Leslie2007 can be incorporating into the general probabilsitic framework that we use here.
<!-- In Expt. 1a, we discovered that a mixture model is necessary to account for intuitions about the feature-probability across different categories. -->
<!-- This minimally structured model  -->

The key point of divergence then between the conceptual accounts and our view concerns the core, semantic meaning of a generic statement.
Conceptual accounts try to identify the core meaning of a generalization directly with aspects of conceptual structure.
For us, the semantics concerns a probability, which is the byproduct of a conceptual structure, but it is not the conceptual structure itself. 
We believe this provides a more unifying view of generics, giving a single metric as the basic currency of generic meaning: probability. 


<!-- @Prasada2006 and later @Prasada2013 distinguish between \emph{principled}, \emph{statistical}, and \emph{causal} relations within concepts. -->
<!-- Striking generics (e.g. \emph{Mosquitos carry malaria}) show characteristics of \emph{causal connections} (operationalized using the phrase \emph{There is something about Ks that cause them to F}). -->
<!-- Generics like \emph{Birds lay eggs} (in which only a minority have the property) exhibit *principled* connections (operationalized by endorsement of the phrase \emph{In general, Ks have F}). -->
<!-- The fact that generics about different properties license different kinds of inferences is taken as evidence that the generics themselves represent different kinds of relations. -->
<!-- Statistical information takes a backseat to the conceptual structure. -->


<!-- It is also the first attempt in psychology to try to unify significant swaths of language that are seemingly quite different from one another: generalizations about events, causes, and categories. -->

<!-- -- Leslie (2008): Default inferences, could be similar to the cognitive model described, but we don't make strong metaphysical commitments (e.g., to "negative" properties) -->

<!-- -- "default generalization" -- very close, the listener is doing a generalization, but not generalization based on evidence, but by reasoning about the threshold -->



<!-- Generic language is the simple and ubiquitous way by which generalizations are conveyed between people. -->
<!-- Yet the dramatic flexibility of generic language has confounded psychologists, linguists and philosophers who have tried to articulate what exactly generic statements mean.  -->
<!-- We evaluated a theory of generic language derived from general principles of language understanding using a simple, but uncertain, basic meaning---a threshold on property prevalence. -->
<!-- Our formal model is a minimal extension of the RSA theory of language understanding, together with an underspecified threshold semantics. -->
<!-- The model was able to explain two major puzzles of generics: their extreme flexibility in truth conditions and the contrastingly strong interpretation of many novel generics. -->
<!-- Both of these phenomena were revealed to depend in systematic ways on prior knowledge about properties. -->
<!-- This prior knowledge was revealed through Bayesian model analysis to be structured, providing a promising bridge to conceptual accounts of generic language. -->
<!-- To understand the nature of the underlying prevalence scale, we showed that generic language is about speakers' expectations of future prevalence, and not necessarily what the current state of the world is like.  -->
<!-- Across all experiments, the formal model predicted the quantitative details of participants' judgments with high accuracy. -->

<!-- There have been numerous demonstrations arguing that statistics (e.g., property prevalence) are insufficient to explain generic meaning [@Gelman2002; @Gelman2007; @Cimpian2010; @Cimpian2010c; @Khemlani2012; @Prasada2013]. -->
<!-- In these experiments, the prevalence considered is only the prevalence of the property for the referent-category [e.g., the percentage of birds that lay eggs; @Khemlani2012; @Prasada2013], what we have referred to as \emph{within-kind prevalence}.  -->
<!-- Indeed, this simple statistic also fails to explain our data (Figure \ref{fig:commongenerics}b, right). -->
<!-- Our formal model of pragmatics, by contrast, considers not only within-kind prevalence, but a listener's prior beliefs about prevalence across kinds in order to arrive at a meaning for a generic utterance. -->
<!-- By establishing the validity of a semantics based on prevalence alone, we provide a formalism to learn about categories from generic statements.  -->
<!-- Further, since prevalence is a probability, our model can take information conveyed with a generic and be naturally extended to make predictions about entities in the world or support explanations of events or behavior. -->

<!-- ## Generic Identification -->

<!-- Throughout this paper we treated the bare plural construction as a generic utterance with a threshold semantics: $\denote{\text{K F}}(x, \theta)=x>\theta$. -->
<!-- The bare plural construction can also indicate a specific plural predication. -->
<!-- For example, "Dogs are on my lawn" picks out a specific group of dogs, while "Dogs have fur"  does not [@Carlson1977]. -->
<!-- The problem of \emph{identifying} a generic meaning from a bare plural construction is itself a challenging problem because generic meaning can be signaled using a diverse array of morphosyntactic cues. -->

<!-- @Declerck1991 suggests that generic and non-generic bare plurals can be treated in the same way, and that pragmatic considerations alone may resolve interpretative differences.  -->
<!-- Indeed it does appear that knowledge of the properties under discussion (e.g., the state of being on a front lawn; the state of having fur) could facilitate the generic identification process. -->
<!-- Other pragmatic factors, like knowledge of the identity of the speaker (e.g., a teacher vs. a veterinarian), can also disambiguate generic and non-generic meaning [@Cimpian2008]. -->
<!-- Recent work suggests that utterances that fail to refer to specific entities or events could pragmatically imply generic meaning [@Crone2016cogsci]. -->
<!-- Incorporating these insights about generic identification into an information-theoretic, communicative perspective is a natural extension of this work. -->



# Conclusion

It might seem paradoxical that a part of language that is so common in communication and central to learning should be vague. 
Shouldn't speakers and teachers want to express their ideas as crisply as possible?
To the contrary, underspecification can be efficient, given that context can be used to resolve  uncertainty in meaning [@Piantadosi2012].
In our work, context takes the form of shared beliefs about the property in question. 
By leveraging this common ground, genericity provide a powerful way to communicate and learn generalizations, which would otherwise be difficult or costly information to learn through direct experience.

The dark side of this flexibility is the potential for miscommunication or deceit: A speaker might assert a generalization that she herself would not accept, conveying a too-strong generalization to a naive listener.  
Our model predicts this potential particularly for properties which, when present, are widespread in a category; biological properties are believed to have this distribution, but many properties of social categories may as well [@Cimpian2011a; @Cimpian2012b; @Rhodes2012].
Disagreements are also predicted when interlocutors fail to share background assumptions, which could be differences in the prior distributions on feature-probabilities or the comparison class.
Core aspects of political disagreement then (e.g., as to whether "Humans cause global warming") could be derived from differences in the estimated causal responsibility of the category in question (e.g., what impacts humans are having), of *other forces* (e.g., plate tectonics), as well as the comparison class of other forces (e.g., what are the alternative causes?). 
This is a promising area for future research.

Categories are inherently unobservable. 
You cannot see the category \textsc{dog}, only some number of instances of it.
Yet we easily talk about these abstractions, conveying hard-won generalizations to each other and down through generations.
<!-- The theory presented here gives one explanation of how we do so, providing a computational perspective on how we communicate generalizations and how beliefs play a central role in the meaning of words. -->
The theory presented here provides the first computational perspective on how we communicate generalizations and how beliefs play a central role in the meaning of words.


# References 

<!-- \setlength{\parindent}{-0.1in}  -->
<!-- \setlength{\leftskip}{0.125in} -->
<!-- \noindent -->

<div id = 'refs'></div>

```{r child = 'appendix-cueValidity.Rmd'}
```

```{r child = 'appendix-bda.Rmd'}
```

```{r child = 'appendix-alternativeRSAmodels.Rmd'}
```
