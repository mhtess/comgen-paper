# Case Study 1: Generic Language

Generalizations about categories (i.e., *generic language*; e.g., "Dogs bark.") have received substantial interest in psychology for their role in concept and theory formation [e.g., @Gelman2004], stereotype propogation [@Rhodes2012], and motivation [@Cimpian2007], as well as many other facets of everyday reasoning.
In addition, generics have been the case study of choice for the language of generalization because of their vexing similarity to quantified statements (e.g., "Most dogs bark").
Time after time, psychologists have shown that generics simply do not reduce to quantified statements [e.g., @Khemlani2012, @Prasada2013, @Cimpian2010].

Here, we take our first empirical look at how the *underspecified threshold* endorsement model predicts actual human endorsements of generic statements.
We measure endorsement for thirty generic sentences that cover a range of conceptual distinctions [@Prasada2013]: *majority characteristic* (e.g., "Ducks have wings."),  *minority characteristic* (e.g., "Robins lay eggs."), *striking* (e.g. "Mosquitos carry malaria."),  *false generalization* (e.g., "Robins are female."), and *absent features* (e.g. "Lions lay eggs.").
We further craft sentences to elicit the full range of acceptability judgments (intuitively: "true", "false", and "indeterminate") for generics with properties of low, medium, and high feature-probabilities (Expt. 1a).
We examine generics about animal categories in order to reliably measure the prior belief distribution over the feature-probabilities $P(h)$ (Expt. 1b).
This elicitation procedure includes measurements of the referent-category feature-probabilties $h'$ (e.g.,$P(x \text{ lays eggs} \mid x \text{ is a robin})$), allowing us to both generate predictions for the endorsement model (Eq. \ref{eq:S1}) as well as for simpler, alternative models. 


## Experiment 1a: Generic endorsements

In this experiment, we elicit human endorsements for generalizations about categories (*generics*) taken from the linguistic and psychological literature on generics [@Prasada2013].
The goal of this study is to observe a range of endorsements for generic statements about animal categories. 

### Method
```{r eval = F}
d1 <- read.csv(paste(project.path, "data/generics/endorsement/",
                    "truth-judgments-n100.csv", sep = ""))
d2 <- read.csv(paste(project.path, "data/generics/endorsement/",
                    "naturalGenerics-trials-formatted.csv", sep = ""))

# write.csv(
#   left_join(
#   d1 %>%
#     mutate(sentence = as.character(sentence),
#            sentence = gsub('&quotechar', '', sentence),
#          sentence = gsub('lyme', 'Lyme', sentence)),
#   unique(d2 %>%
#     mutate(sentence = paste(Category, " ", Property, ".", sep = "")) %>%
#     select(Category, Property, sentence))
# ), file = paste(project.path, "data/generics/endorsement/",
#                     "truth-judgments-n100.csv", sep = ""), row.names = F)
```

```{r generic-endorsement}
d.gen.endorse.catch <- read.csv(paste(project.path, "data/generics/endorsement/",
                    "truth-judgments_catch-trials.csv", sep = ""))

d.gen.endorse <- read.csv(paste(project.path, "data/generics/endorsement/",
                    "truth-judgments-n100.csv", sep = ""))



d.gen.endorse.summary <- left_join(
  d.gen.endorse, 
  d.gen.endorse.catch %>% select(workerid, pass)
  ) %>%
  filter(pass == 1) %>%
  rowwise() %>%
  mutate(response = ifelse(response == "agree-key", 1, 0)) %>%
  group_by(sentence) %>%
  multi_boot_standard(col = "response") %>%
  ungroup() %>%
  mutate(sentence = factor(sentence, levels = sentence[order(mean)]))

d.gen.endorse.bayes <- left_join(
  d.gen.endorse, 
  d.gen.endorse.catch %>% select(workerid, pass)
  ) %>%
  filter(pass == 1) %>%
  rowwise() %>%
  mutate(response = ifelse(response == "agree-key", 1, 0),
         sentence = gsub('&quotechar', '', sentence),
         sentence = gsub('lyme', 'Lyme', sentence)) %>%
  group_by(sentence) %>%
  summarize(k = sum(response), n = n()) %>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         low  = qbeta(.025, a, b),
         high = qbeta(.975, a, b),
         MAP_h = (a-1)/(a+b-2),
         mean = a / (a + b)) %>% 
  ungroup()


d.prev <- read.csv(paste(project.path, "data/generics/endorsement/",
                       "naturalGenerics-prior-trials-n57.csv", sep = ""))

d.prev.summary <- d.prev %>%
  mutate(item = paste(Category, " ", Property, ".", sep = "")) %>%
  filter(item %in% d.gen.endorse.bayes$sentence) %>%
  mutate(prevalence = prevalence / 100) %>%
  group_by(Category, Property, item) %>%
  multi_boot_standard(col = "prevalence")

# d.gen.endorse.summary %>%
#   ggplot(., aes(x = sentence, y = mean, ymin = ci_lower, ymax = ci_upper))+
generics.endorsement.spectrum <- left_join(
  d.gen.endorse.bayes, d.prev.summary %>% rename(sentence = item, prev = mean, prev_low = ci_lower, prev_high = ci_upper)
) %>%
  mutate(sentence = factor(sentence, levels = sentence[order(MAP_h)]))  %>%
  #ggplot(., aes(x = sentence, y = MAP_h, ymin = low, ymax = high))+
  ggplot(., aes(x = sentence, y = MAP_h - 0.5, ymin = low - 0.5, ymax = high - 0.5, fill = prev))+
  geom_bar(stat = 'identity', position = position_dodge(), color= 'black', alpha = 0.9)+
  geom_linerange(position = position_dodge())+
  #geom_errorbar(position = position_dodge())+
  coord_flip()+
  ylab("Human generic endorsement")+
  xlab("")+
  #scale_y_continuous(limits = c(0,1), breaks = c(0, 0.5, 1))+
  scale_y_continuous(limits = c(-0.5,0.5), breaks = c(-0.5, 0, 0.5), labels = c(0, 0.5, 1))+
  scale_fill_continuous(breaks = c(0, 1), limits = c(0, 1))+
  guides(fill = guide_colorbar(title = 'feature probability',
                               ticks = F))+
  theme(legend.position = 'bottom')
  #geom_hline(yintercept = -0.25, lty = 3)+
  #geom_hline(yintercept = 0.25, lty = 3)
```

#### Participants

We recruited 100 participants over Amazon's crowd-sourcing platform Mechanical Turk (MTurk).
Participants were restricted to those with US IP addresses and with at least a 95\% MTurk work approval rating (the same criteria apply to all experiments reported). 
4 participants were excluded for failing to recall the button corresponding to *agreement* in the forced-choice task.
5 participants self-reported a native language other than English; removing their data has no effect on the results reported. 
The experiment took about 3 minutes and participants were compensated \$0.35.

#### Procedure and materials

Participants were shown thirty generic sentences in a randomized order.
They were asked to press one of two buttons (randomized between-participants) to signify whether they agreed or disagreed with the sentence (see Table 2 in Appendix for complete list). 
The thirty sentences were presented in a random order between participants and covered a range of conceptual categories described above.
Approximately 10 true, 10 false, and 10 uncertain truth-value generics were selected.
As an attention check, participants were asked at the end of the trials which button corresponded to "Agree".
4 participants were excluded for failing this trial.

### Results

```{r generic-endorsement-manipulationcheck}
# MHT's truth judgments of sentences
# t --> true; f --> false; i --> indeterminate/uncertain
all.sentences <- list("Cardinals are red." = "t",
 "Ducks have wings." = "t",
 "Kangaroos have pouches." = "t",
 "Kangaroos have spots." = "f",
 "Leopards are juvenile." = "i",
 "Leopards have spots." = "t",
 "Leopards have wings." = "f",
 "Lions are male." = "i",
 "Lions have manes." = "t",
 "Lions lay eggs." = "f",
 "Mosquitos attack swimmers." = "i",
 "Mosquitos carry malaria." = "t",
 "Mosquitos dont carry malaria." = "f",
 "Peacocks dont have beautiful feathers." = "f",
 "Peacocks have beautiful feathers." = "t",
 "Robins are female." = "i",
 "Robins carry malaria." = "f",
 "Robins lay eggs." = "t",
 "Sharks are white." = "i",
 "Sharks attack swimmers." = "t",
 "Sharks dont attack swimmers." = "f",
 "Sharks have manes." = "f",
 "Sharks lay eggs." = "i",
 "Swans are full-grown." = "i",
 "Swans are white." = "t",
 "Ticks carry Lyme disease." = "t",
 "Ticks dont carry Lyme disease."= "f",
 "Tigers dont eat people."= "f",
 "Tigers eat people." = "t",
 "Tigers have pouches."= "f")

mht.truth.judgments <- data.frame(sentence = names(all.sentences),
           truth_judgment = as.vector(unlist(all.sentences)))

d.gen.endorse.manipulation.check.bayes <- left_join(
  left_join(
    d.gen.endorse, 
    d.gen.endorse.catch %>% select(workerid, pass)
    ),
  mht.truth.judgments ) %>%
  filter(pass == 1) %>%
  rowwise() %>%
  mutate(response = ifelse(response == "agree-key", 1, 0),
         sentence = gsub('&quotechar', '', sentence),
         sentence = gsub('lyme', 'Lyme', sentence)) %>%
  group_by(truth_judgment) %>%
  summarize(k = sum(response), n = n()) %>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         low  = qbeta(.025, a, b),
         high = qbeta(.975, a, b),
         MAP_h = (a-1)/(a+b-2),
         mean = a / (a + b)) %>% 
  ungroup() %>%
  mutate(truth_judgment = factor(truth_judgment, levels = truth_judgment[order(MAP_h)]))

true_MAP <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "t")[[1,"MAP_h"]]
true_low <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "t")[[1,"low"]]
true_high <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "t")[[1,"high"]]

false_MAP <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "f")[[1,"MAP_h"]]
false_low <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "f")[[1,"low"]]
false_high <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "f")[[1,"high"]]

uncertain_MAP <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "i")[[1,"MAP_h"]]
uncertain_low <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "i")[[1,"low"]]
uncertain_high <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "i")[[1,"high"]]

```

As a manipulation check, the first author assigned an \emph{a priori} truth-judgment (true/false/indeterminate) to each stimulus item.
As one would expect, there were substantial differences in empirical endorsements: true generics were almost universally endorsed (Maximum A-Posteriori estimate and 95\% credible interval of endorsement probability: $`r true_MAP`$ $[`r true_low`, `r true_high`]$);
indeterminate generics were agreed with \emph{less} likely than chance ($`r uncertain_MAP`$ $[`r uncertain_low`, `r uncertain_high`])$) but substantially more than false generics  ($`r false_MAP`$ $[`r false_low`, `r false_high`])$).

In addition to these categorical differences between our thirty generic statements, we find a contiuum of endorsement values (Figure \ \@ref(fig:generics-endorsement-figure) top left).
Such a continuum of judgments is already evidence against any theory that only predicts categorically whether a generic statement is true or false.
Ideally, a complete theory of genericity should be able to explain statements that are endorsed completely, unendorsed completely, and all shades in between.
We next measure the prior distribution over feature-probabilities and articulate a set of formal models that try to predict these endorsement data. 

## Experiment 1b: Feature-probability prior elicitation

The feature-probability prior $P(h)$ in Eq. \ref{eq:L0} describes the belief distribution on the probability of a given feature (e.g. \textsc{lays eggs}) across relevant categories. 
To get an intuition for the kind of knowledge encoded in this belief distribution, consider the following thought experiment: You are walking around and you come across an instance of your favorite kind of animal from Earth (e.g., a peregrine falcon). Ask yourself "is it female?" Your answer will probably depend upon the percentage of the category that you believe to be female (e.g., the percentage of female peregrine falcons, i.e., 50\%). Now, do you think it lays eggs? Your answer to this question will likely depend upon on what animal you're thinking of: If you're thinking of a peregrine falcon, the probability is similar to that of being female (50\%) because female peregrine falcons lay eggs; if you're thinking of a mammal, it's likely that it won't lay eggs (0\%) because most mammals give birth to live young. That is, the answer is either 50\% or 0\% depending on the kind of creature you are thinking of^[
The subjective probability may, in fact, be non-zero and small for what we're calling 0\%. 
Non-zero probabilities allow for the intuitive possibility of a dog that lays eggs potentially due to some strange generic mutation or the interaction of dog genetics with a strange environment. 
].

This thought experiment decomposes the feature-probability prior $P(h)$ into a prior on kinds $P(k)$ and then a conditional probability of the feature-probability given the kind $P(h \mid k)$.
We will use this decomposition to measure the feature-probability prior $P(h) = \int_{k} P(h \mid k) \diff k$, and perform a structured Bayesian data-analysis to model the prior data.
We measured prior distributions empirically for the set of properties (e.g., \textsc{lays eggs, carries malaria}; 21 in total) used in our generic sentences in Expt. 1a.
To create a larger set of properties, we reverse-code responses for five properties to create their corresponding negative properties (e.g., we create a property "doesn't have beautiful feathers" by subtracting from 100\% the responses for "has beautiful feathers").

### Method

#### Participants

We recruited 60 participants over Amazon MTurk.  
3 participants were accidentally allowed to complete the experiment for a second time, so we excluded their second responses (resulting in $n=57$).
2 participants self-reported a native language other than English; removing their data ($n=55$) has no effect on the results reported. 
The experiment took about 10 minutes and participants were compensated \$1.00.

#### Procedure and materials

On each trial of the experiment, participants filled out a table where each row was an animal category and each column was a property. 
Participants first were shown six animal categories randomly sampled from a set corresponding to referent-categories of the generic sentences used in Expt. 1a (e.g., \textsc{robins, mosquitos}) and were asked to generate five of their own (Figure \ \@ref(fig:generics-prior-task) left).
A column then appeared to the right of the animal names with a property label in the column header (e.g., *lays eggs*).
Participants were asked to fill in each row with the percentage of members of each of the species that had the property by giving a number (e.g., "50\%"; Figure \ \@ref(fig:generics-prior-task) right).
Eight property--columns appeared in the table.

This whole procedure was repeated two times (two trials).
In total, each participant generated ten animal names and reported on the feature-probability of sixteen properties for twenty-two animals (their own ten and the experimentally-supplied twelve). 
<!-- For a full list of the properties, and generic sentences used in Expt. 1b, see Table 2 (Appendix). -->


```{r generics-prior-task, fig.cap="Prior elicitation task. A: Participants first generated animal names after seeing six example categories. B: One feature at a time, participants estimated the percentage of the category with the feature, for each category.", fig.height = 3}
fig.priors.expt.stage1 <- ggdraw() + draw_image(paste(project.path, "paper/figs/generics-prior-stage1.jpeg", sep=""), scale = 1)
fig.priors.expt.stage2 <- ggdraw() + draw_image(paste(project.path, "paper/figs/generics-prior-stage2.jpeg", sep=""), scale = 1)

plot_grid(fig.priors.expt.stage1, fig.priors.expt.stage2, labels = "AUTO")
``` 

<!-- \begin{figure} -->
<!-- \centering -->
<!--     \includegraphics[width=\columnwidth]{figs/generics-prior-bothStages.pdf} -->
<!--     \caption{Design of Experiment 1b for eliciting prior knowledge about feature-probabilities. Left: Participants first generated animal names after seeing six example categories. Right: One feature at a time, participants estimated the percentage of the category with the feature, for each category.} -->
<!--     \label{fig:generics-prior-task} -->
<!-- \end{figure} -->


### Results

In order to integrate the results of our prior measurement into richer models of endorsement, we first try to accurately model the prior data by itself.
We do this using a Bayesian mixture model, which assumes that the data generated by participants for each property (e.g., \textsc{lay eggs}) comes from one of two underlying distributions. 
One of these distributions is fixed to represent kinds of animals who *do not have* an stable causal mechanism that could give rise to the property (e.g., \textsc{lions} and \textsc{lay eggs}).
The second distribution is flexible and represents kinds of animals who *do have* such a mechanism; this distribution is flexible and can represent different shapes for different properties (e.g., the differences in feature-probability judgments for \textsc{lay eggs} vs. \text{has wings}).
For model implementation and inference details, see Appendix. 

```{r generic-endorsement-priors}
# this data set already excludes the 2nd attempt of 3 participants who completed the experiment a second time

d.gen.endorse.priors <- read.csv(paste(project.path, 
                                       "data/generics/endorsement/",
                                       "naturalGenerics-prior-trials-n57.csv", sep = ""))
gen.endorse.properties <- levels(d.gen.endorse.priors$Property)

genericEndorsementPriorModelHelpers <- '
var eps = 0.01;//Number.EPSILON;
var log = function(x){ return Math.log(x) }
var exp = function(x){ return Math.exp(x) }

var avoidEndPoints = function(x){
  x == 0 ? eps : 
  x == 1 ? 1 - eps : 
  x
}

var betaShape = function(params){
  return {
      a: params.g * params.d,
      b : (1-params.g) * params.d
  }
}

var preprocessedResponses = map(function(d){
  return avoidEndPoints(d / 100)
}, data)
'


structuredPriorModel <- '
var model = function(){
  var phi = uniformDrift({a:0, b: 1, width:0.2});
  var g = uniformDrift({a:0, b: 1, width:0.2});
  var d = uniformDrift({a:0, b: 100, width:5});
  var stableParams = betaShape({g, d})
  var stableComponent = Beta(stableParams)
  var transientComponent = Beta({a:1, b:100})
  mapData({data: preprocessedResponses}, function(d){
    factor( log(
      phi * exp(stableComponent.score(d)) +
      (1 - phi) * exp(transientComponent.score(d))
    ))
  })
  return {g, d, phi}
}
'

unstructuredPriorModel <- '
var model = function(){
  var g = uniformDrift({a:0, b: 1, width:0.2});
  var d = uniformDrift({a:0, b: 100, width:5});
  var stableParams = betaShape({g, d})
  var stableComponent = Beta(stableParams)
  mapData({data: preprocessedResponses}, function(d){
    factor(stableComponent.score(d))
  })
  return {g, d}
}
'
genericEndorsementPriorModel <- paste(genericEndorsementPriorModelHelpers, structuredPriorModel, sep = "\n")

```

```{r generic-endorsement-priors-unbounded, eval = F}
# this data set already excludes the 2nd attempt of 3 participants who completed the experiment a second time

d.gen.endorse.priors <- read.csv(paste(project.path, 
                                       "data/generics/endorsement/",
                                       "naturalGenerics-prior-trials-n57.csv", sep = ""))
gen.endorse.properties <- levels(d.gen.endorse.priors$Property)


unboundedModel <- '
var eps = 0.01;//Number.EPSILON;
var log = function(x){ return Math.log(x) }
var exp = function(x){ return Math.exp(x) }

var avoidEndPoints = function(x){
  x == 0 ? eps : 
  x == 1 ? 1 - eps : 
  x
}

var preprocessedResponses = map(function(d){
  return avoidEndPoints(d / 100)
}, data)

var shapeParams = function(params){
  return {
    a: params.g * params.d,
    b: (1 - params.g) * params.d
  }
}

var model = function(){

  // var numComponents = (1 + poisson(1));
  var numComponents = 5//discrete([1, 1, 1, 1, 1]) + 1
  var mixtureWeights = sort(normalize(repeat(numComponents, function(){
    return uniformDrift({a: 0, b:1, width: 0.2})
  }))).reverse()

  // var mixtureWeights = dirichlet(ones([numComponents, 1]));
  var componentParams = map(function(i){
    var g = uniformDrift({a: 0, b:1, width: 0.2}), d = uniformDrift({a: 0, b:100, width: 20})
    var betaParams = shapeParams({g, d})
    return betaParams
  }, _.range(numComponents))

  mapData({data: preprocessedResponses}, function(d){
    
    var scr = log(sum(map(function(cParams){
      cParams[1] * exp(Beta(cParams[0]).score(d))
    }, _.zip(componentParams, mixtureWeights))))
//    }, _.zip(componentParams, T.toScalars(mixtureWeights)))))
    factor(scr)
  })

  var componentPredictives = map(function(cParams){
      return beta(cParams)
  }, componentParams)

  return _.fromPairs(
    _.zip(
        map(function(i){ i + "_weight" }, _.range(5)), 
        mixtureWeights
      ).concat(
      _.zip(
        map(function(i){ i + "_predictive" }, _.range(5)), 
        componentPredictives
      )
    )
  )//sort(T.toScalars(mixtureWeights))
};
'

example.generics.prior.properties <- c(
  "dont eat people", "have beautiful feathers",
  "have wings", "are red",
  "carry malaria", "lay eggs", 
  "are female", "have spots"
  )
priorData <- filter(d.gen.endorse.priors, Property == "lay eggs")$prevalence
  n_samples <- 10000
lg = 0
  m.gen.endorse.priors.unbounded <- webppl(unboundedModel,
                                 data = priorData, data_var = "data",
                                 model_var = "model",
                                 chains = 1,
                                 cores = 1,
                                 inference_opts =
                                   list(method = "MCMC",
                                        #kernel = list(HMC = list(steps = 5, stepSize = 0.001)),
                                        samples = n_samples,
                                        burn = n_samples / 2,
                                        lag = lg,
                                        verbose = T))
```
```{r eval = F}
m.gen.endorse.priors.unbounded %>%
      separate(Parameter, into = c("c", "var")) %>% 
      ggplot(., aes( x=  value, fill = c))+
 geom_histogram(position = position_dodge(), binwidth = 0.05)+
 facet_wrap(~var)+
  scale_fill_solarized()
```

```{r generic-endorsement-priors-bayesFactors, eval = F}
genericEndorsementPriorModelPriorLikelihoodHelpers <- '
var properties = _.uniqBy(_.map(data, "Property"));

var eps = 0.01;//Number.EPSILON;
var log = function(x){ return Math.log(x) }
var exp = function(x){ return Math.exp(x) }

var avoidEndPoints = function(x){
  x == 0 ? eps : 
  x == 1 ? 1 - eps : 
  x
}

var betaShape = function(params){
  return {
      a: params.g * params.d,
      b : (1-params.g) * params.d
  }
}
'

genericEndorsementPriorModelsPriorLikelihood <- '
var transientComponent = Beta({a:1, b:100})

var model = function(){

  var propertyLogLikes = map(function(property){

    var propertyData = _.map(_.filter(data, {Property: property}), "prevalence");
    var preprocessedResponses = map(function(d){
        return avoidEndPoints(d / 100)
    }, propertyData);

    var phi = uniformDrift({a:0, b: 1, width:0.2});
    var g = uniformDrift({a:0, b: 1, width:0.2});
    var d = uniformDrift({a:0, b: 100, width:5});
    var stableParams = betaShape({g, d})
    var stableComponent = Beta(stableParams)

    var logLikes = map(function(d){
      var structuredLL = log(
        phi * exp(stableComponent.score(d)) +
        (1 - phi) * exp(transientComponent.score(d))
      )

      var unstructuredLL = stableComponent.score(d);

      return {structuredLL, unstructuredLL}
    }, preprocessedResponses)

    return {
      structuredLL: sum(_.map(logLikes, "structuredLL")), 
      unstructuredLL: sum(_.map(logLikes, "unstructuredLL"))
    }

  }, properties)

  return {
    structuredLL: sum(_.map(propertyLogLikes, "structuredLL")), 
    unstructuredLL: sum(_.map(propertyLogLikes, "unstructuredLL"))
  }
}
'

```

```{r generic-endorsement-priors-bayesFactorsRun, eval = F}
marginal.prior.likelihoods <- data.frame()

# 100 samples takes 30s
m.gen.endorse.priors.logLike <- webppl(
    paste(genericEndorsementPriorModelPriorLikelihoodHelpers,
          genericEndorsementPriorModelsPriorLikelihood,
          sep = '\n'), 
    data = d.gen.endorse.priors, data_var = "data",
    model_var = "model", 
    inference_opts = list(method = "forward", samples = 1000),
    chains = 3, cores = 3
) %>% 
  mutate(value = as.numeric(value))

  
min.ll.1 <- min(
  m.gen.endorse.priors.logLike$value[
    is.finite(
      m.gen.endorse.priors.logLike$value
      )
    ]
  )

m.gen.endorse.priors.logLike <- m.gen.endorse.priors.logLike %>% 
  rowwise() %>%
  mutate(value = ifelse(value == -Inf, min.ll.1, value))
  
save(m.gen.endorse.priors.logLike, file = '../analysis/generics-priors-loglike.Rdata')
```

```{r generic-endorsement-priors-bayesFactorsLoad}
load(file = '../analysis/generics-priors-loglike.Rdata')
marginal.prior.likelihoods <- m.gen.endorse.priors.logLike %>%
  group_by(Chain, Parameter) %>%
  summarize(mll  = logmeanexp(value)) 

marginal.prior.likelihoods <- marginal.prior.likelihoods %>% 
  spread(Parameter, mll) %>%
  mutate(logBF = structuredLL - unstructuredLL, BF = exp(logBF))

roundedBF1 = round(mean(marginal.prior.likelihoods$logBF)/ 10000)*10000
```


Eight example prior distributions reconstructed from the mixture model's posterior parameter distribution, together with a comparison model that lacks one of the components of the mixture model (the "transient" distribution $D_{transient}$), are shown with the raw empirical distributions of feature-probability judgments in Figure \  \@ref(fig:generic-endorsement-priors-figure).
There is a lot of diversity in the shapes of the distributions, which the mixture model accounts for quite well, while a model that only models a single distribution fails to.
One feature-probability prior that the mixture model does not perfectly capture is the prior distribution over the feature \textsc{lay eggs}.
The empirical distribution is tri-modal, with reliable modes at 0\%, 50\%, and 100\%^[
Similar phenomena have been observed in other feature-probability elicitation tasks when the referent-category is fixed [e.g., estimating only the percentage of *robins* that *lay eggs*; @Prasada2013]
]; our simple two-component mixture model has no way to account for such a tri-modal distribution. 
A more complex model (i.e., one with three mixture components) is necessary to perfectly account for this item.

<!-- The Bayes Factor indicates the extent to which the data support one model over another. -->
<!-- The overall Bayes Factor in terms of structured model (S) over the unstructured model (U) for the full feature-probability prior data set is approximately $\log BF_{SU} = `r roundedBF1`$, meaning that the structured hypothesis is approximately `r roundedBF1` orders of magnitude more likely than the unstructured hypothesis to account for the feature-probability prior data. -->
<!-- Indeed, the data strongly suggests the feature-probability prior is structured. -->


```{r generic-endorsement-priors-savageDickeyModel, eval = F}
savageDickeyModel <-'
var model = function(){
  var phi = uniformDrift({a:0, b: 1, width:0.2});
  var g = uniformDrift({a:0, b: 1, width:0.2});
  var d = uniformDrift({a:0, b: 100, width:5});
  var stableParams = betaShape({g, d})
  var stableComponent = Beta(stableParams)
  var transientComponent = Beta({a:1, b:100})
  mapData({data: preprocessedResponses}, function(d){
    factor( log(
      phi * exp(stableComponent.score(d)) +
      (1 - phi) * exp(transientComponent.score(d))
    ))
  })
  return {phi}
}

//var samples = data.samples[0], burn = data.burn[0], lag = data.lag[0];
// var samples = 2000, burn = burn / 2, lag = 0;
var modelPosterior = Infer({method: "MCMC", samples:10000, burn:5000, lag:0, 
verbose: true}, model)
var modelPrior = Infer({method: "forward", samples:10000}, model)

var pointNull = 1;

var logBFs = map(function(diff){
  var savageDickeyDenomenator = expectation(modelPrior, function(x){return Math.abs(x.phi-pointNull)<diff})
  var savageDickeyNumerator = expectation(modelPosterior, function(x){return Math.abs(x.phi-pointNull)<diff})
  var savageDickeyRatio = savageDickeyNumerator / savageDickeyDenomenator
  return [diff, Math.log(savageDickeyRatio)]
}, [0.01, 0.02, 0.03, 0.04, 0.05])

var returnVals = {
  prior: modelPrior, posterior: modelPosterior, logBFs: logBFs
}
returnVals
'
```

```{r generic-endorsement-priors-bayesFactors-supermodel, eval= F}
genericEndorsementPriorSupermodel <- '
var model = function(){
  var alpha = uniformDrift({a:0, b: 1, width:0.2});
  //var structured = flip();
  var phi = uniformDrift({a:0, b: 1, width:0.2});
  var g1 = uniformDrift({a:0, b: 1, width:0.2});
  var d1 = uniformDrift({a:0, b: 100, width:5});
  // var g0 = uniformDrift({a:0, b: 1, width:0.2});
  // var d0 = uniformDrift({a:0, b: 100, width:5});
  var stableParams1 = betaShape({g: g1, d: d1})//, 
      //stableParams0 = betaShape({g: g0, d: d0});
  var stableComponent1 = Beta(stableParams1)//,
      //stableComponent0 = Beta(stableParams0);

  var transientComponent = Beta({a:1, b:100})
  mapData({data:preprocessedResponses},
    function(d){
    var scr1 = log(phi * exp(stableComponent1.score(d)) +
                    (1 - phi) * exp(transientComponent.score(d)))
    var scr0 = stableComponent1.score(d);
    // display(exp(scr1) + " " + exp(scr0));
    var combined_ll = log(alpha * exp(scr1) + (1 - alpha) * exp(scr0));
    // factor(structured ? scr1 : scr0)
    factor(combined_ll)
  })
  return alpha
}
'
```

```{r generic-endorsement-priors-bayesFactor-supermodelRun, cache = T, eval = F}
for (p in gen.endorse.properties){
  priorData <- filter(d.gen.endorse.priors, Property == p)$prevalence
  
  m.gen.endorse.priors.supermodel.posterior <- webppl(
    paste(genericEndorsementPriorModelHelpers, genericEndorsementPriorSupermodel, sep = '\n'), 
    data = priorData, data_var = "data",
    model_var = "model",  chains = 2,
                                 cores = 2,
                                 inference_opts = 
                                   list(method = "MCMC",
                                        samples = 2000, 
                                        burn = 2000, 
                                        lag = 2, 
                                        verbose = T)
    )
  
  qplot(m.gen.endorse.priors.supermodel.posterior$value)
}
```

```{r generic-endorsement-priors-bayesFactors-enumerate, eval = F}
genericEndorsementPriorModelPriorLikelihoodEnumerate <- '
var eps = 0.00000001;

var model = function(){
  var phi = uniformDraw(_.range(eps, 1-eps, 0.1))
  var g = uniformDraw(_.range(eps, 1-eps, 0.05))
  var d = uniformDraw(_.range(eps, 100, 2))
  var stableParams = betaShape({g, d})
  var stableComponent = Beta(stableParams)
  var transientComponent = Beta({a:1, b:100})
  var logLike = sum(map(function(d){
    var scr = log(
      phi * exp(stableComponent.score(d)) +
      (1 - phi) * exp(transientComponent.score(d))
    )
    return scr
  }, preprocessedResponses))
  return logLike
}
'
genericEndorsementUnstructuredModelPriorLikelihoodEnumerate <- '
var eps = 0.00000001;

var model = function(){
  var g = uniformDraw(_.range(eps, 1-eps, 0.2))
  var d = uniformDraw(_.range(eps, 100, 2))
  var stableParams = betaShape({g, d});
  var stableComponent = Beta(stableParams);
  var logLike = sum(map(function(d){
    return stableComponent.score(d)
  }, preprocessedResponses))
  return logLike
}
'
```

```{r generic-endorsement-priors-bayesFactorsRunEnumerate, cache = T, eval =F}
marginal.prior.likelihoods <- data.frame()

for (p in gen.endorse.properties){
  priorData <- filter(d.gen.endorse.priors, Property == p)$prevalence
  
  m.gen.endorse.priors.logLike.enum1 <- webppl(
    paste(genericEndorsementPriorModelHelpers,
          genericEndorsementPriorModelPriorLikelihoodEnumerate, sep = '\n'), 
    data = priorData, data_var = "data",
    model_var = "model", 
    inference_opts = list(method = "enumerate")
    )
  
  m.gen.endorse.priors.logLike.enum0 <- webppl(
    paste(genericEndorsementPriorModelHelpers,
          genericEndorsementUnstructuredModelPriorLikelihoodEnumerate, sep = '\n'), 
    data = priorData, data_var = "data",
    model_var = "model", 
    inference_opts = list(method = "enumerate")
    )

  marginal.prior.likelihood1 <- logmeanexp(m.gen.endorse.priors.logLike.enum1$support)
  marginal.prior.likelihood0 <- logmeanexp(m.gen.endorse.priors.logLike.enum0$support)

  marginal.prior.likelihoods <- bind_rows(
    marginal.prior.likelihoods, 
    data.frame(Property = p, structuredLL = marginal.prior.likelihood1, unstructuredLL = marginal.prior.likelihood0)
  )
}

marginal.prior.likelihoods <- marginal.prior.likelihoods %>% mutate(logBF = structuredLL - unstructuredLL, BF = exp(logBF))

```

```{r generic-endorsement-priors-model-run, eval = F}
m.gen.endorse.priors.posteriorPredictive <- data.frame()
n_samples <- 1000
lg = 0
example.generics.prior.properties <- c(
  "dont eat people", "have beautiful feathers",
  "have wings", "are red",
  "carry malaria", "lay eggs", 
  "are female", "have spots"
  )


for (p in example.generics.prior.properties ) {
  priorData <- filter(d.gen.endorse.priors, Property == p)$prevalence
  
    m.gen.endorse.priors.structured <- webppl(
      paste(genericEndorsementPriorModelHelpers, structuredPriorModel, sep = ""),
                                 data = priorData, data_var = "data",
                                 model_var = "model",
                                 chains = 2,
                                 cores = 2,
                                 inference_opts =
                                   list(method = "MCMC",
                                        samples = n_samples,
                                        burn = n_samples / 2,
                                        lag = lg,
                                        verbose = T))
    
    m.gen.endorse.priors.structured.predictive <- m.gen.endorse.priors.structured %>%
      group_by(Iteration, Chain) %>%
      spread(Parameter, value) %>%
      rowwise() %>%
      mutate(
        a = g*d,
        b = (1-g)*d,
        isPresent = rbinom(1, 1, prob = phi),
        prevalence = ifelse(isPresent == 1, 
                           rbeta(n = 1, 
                                 shape1 = a,
                                 shape2 = b),
                           rbeta(n = 1,
                                 shape1 = 1,
                                 shape2 = 100)),
        Property = p, src = "Structured"
      ) %>% ungroup()
    
    m.gen.endorse.priors.unstructured <- webppl(
      paste(genericEndorsementPriorModelHelpers, unstructuredPriorModel, sep = ""),
                                 data = priorData, data_var = "data",
                                 model_var = "model",
                                 chains = 2,
                                 cores = 2,
                                 inference_opts =
                                   list(method = "MCMC",
                                        samples = n_samples,
                                        burn = n_samples / 2,
                                        lag = lg,
                                        verbose = T))
    
    m.gen.endorse.priors.unstructured.predictive <- m.gen.endorse.priors.unstructured %>%
      group_by(Iteration, Chain) %>%
      spread(Parameter, value) %>%
      rowwise() %>%
      mutate(
        a = g*d,
        b = (1-g)*d,
        prevalence = rbeta(n = 1, 
                                 shape1 = a,
                                 shape2 = b),
        Property = p, src = "Unstructured"
      ) %>% ungroup()
    
    
    m.gen.endorse.priors.posteriorPredictive <- bind_rows(
      m.gen.endorse.priors.posteriorPredictive,
      bind_rows(
       m.gen.endorse.priors.structured.predictive %>%
          select( src, Property, prevalence),
      m.gen.endorse.priors.unstructured.predictive %>%
          select( src, Property, prevalence),
      data.frame(Property = p,
                 src = "Data",
                 prevalence = priorData/100)
      )
    )
    
    
  # for (c in 1:2){
  #     m.gen.endorse.priors.savage <- webppl(
  #   paste(genericEndorsementPriorModelHelpers, 
  #         savageDickeyModel, sep = '\n'), 
  #   data = priorData, 
  #   data_var = "data"
  #   )
  
  # m.inferred.prior <- get_samples(
  #   data.frame(m.gen.endorse.priors.savage$prior) %>%
  #     rename(prob = probs), n_samples)
  # 
  # m.inferred.posterior <- get_samples(
  #   data.frame(m.gen.endorse.priors.savage$posterior) %>%
  #     rename(prob = probs), n_samples)
  # 
  # posterior.density <- density(m.inferred.posterior$phi,
  #         from = 0, to = 1)
  # 
  # prior.density <- density(m.inferred.prior$phi,
  #         from = 0, to = 1)
  # 
  # logBF01 = log(approxfun(posterior.density)(1)) - 
  #   log(approxfun(prior.density)(1))



  
  # m.gen.endorse.priors.savage.summary <- bind_rows(
  #   m.gen.endorse.priors.savage.summary,
  #     bind_rows(
  #     data.frame(Property = p, 
  #                Chain = c, 
  #                diff = NA,
  #                logBF01 = logBF01,
  #                BF01= exp(logBF01)),
  #     data.frame(m.gen.endorse.priors.savage$logBFs) %>%
  #       rename(diff = X1, logBF01 = X2) %>%
  #       mutate(Property = p, Chain = c, BF01 = exp(logBF01))
  #     )
  # )
  # 
  # }



    
    #for (c in 1:3){

      # posterior.logspline <-  polspline::logspline(filter(
      #   m.gen.endorse.priors, Parameter == 'phi', Chain == c)$value,
      #   lbound = 0, ubound = 1)
      # 
      #   posterior <- dlogspline(1, posterior.logspline)
      # prior <- dunif(1, min = 0, max = 1)
      # BF01 <- posterior/prior
      # 
      # print(log(BF01))

  #}



  #print(p)
  #print(priorData)
}
save(m.gen.endorse.priors.posteriorPredictive, file = '../analysis/generics-priors-posteriorPredictives.Rdata')

```

```{r generic-endorsement-priors-figure, fig.width = 7, fig.height = 6, fig.cap="Cumulative density plots for prior distributions representing the feature-probability of eight different features. Distributions are the posterior predictive distributions for the Structured and Unstructured prior models, as well as the raw empirical distribution. A completely uniform distribution would be represented as the y = x line."}
load('../analysis/generics-priors-posteriorPredictives.Rdata')

gen.priors.densities <- ggplot(m.gen.endorse.priors.posteriorPredictive %>%
         filter(src == 'Data'), 
           aes( x = prevalence))+
           #aes( x = prevalence, color = src, lty = src))+
    geom_density(aes(y = ..scaled..), adjust = 1.3, size = 1)+
    #scale_fill_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    #stat_ecdf(size = 1)+
    #scale_color_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    #scale_alpha_manual(values = c(0.6, 0.4, 0))+
    facet_wrap(~Property, nrow = 2)+
    #scale_linetype_manual(values = c(1, 4, 3))+
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    #coord_fixed()+
    xlab("") +
    ylab("Normalized probability density")+
    theme(legend.position = "bottom", legend.title = element_blank())


gen.priors.cdfs <-m.gen.endorse.priors.posteriorPredictive %>%
  mutate(src = factor(src, levels = c("Data", "Structured", "Unstructured"),
                      labels = c("Empirical distribution", 
                                 "Mixture of Betas",
                                 "Single Beta"))) %>%
ggplot(.,
           aes( x = prevalence, color = src, lty = src))+
    #geom_density(aes(y = ..scaled..), adjust = 1.3, size = 1)+
    #scale_fill_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    stat_ecdf(size = 1)+
    scale_color_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    scale_alpha_manual(values = c(0.6, 0.4, 0))+
    facet_wrap(~Property, nrow = 2)+
    scale_linetype_manual(values = c(1, 4, 3))+
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    #coord_fixed()+
    xlab("Feature-probability") +
    ylab("Cumulative probability density")+
    theme(legend.title = element_blank(), legend.position = 'bottom')



gen.prior.fig1 <- gen.priors.densities +
  theme(plot.margin = unit(c(10,0,0,0), "pt")) 
gen.prior.fig2 <- gen.priors.cdfs  +
  theme(plot.margin = unit(c(10,0,6,0), "pt")) 

plot_grid( 
 gen.prior.fig1, gen.prior.fig2,
           align = 'vh',
           labels = c("A", "B"),
           hjust = -1,
           nrow = 2,
           rel_heights = c(1, 1.2)
           #rel_widths = c(1.2, 3, 2)
           #rel_widths = c(1, 1.3)
           )
```


```{r generic-endorsement-priors-figure2, eval = F}
ggplot(m.gen.endorse.priors.summary %>% filter(Parameter == 'phi') %>%
         mutate(Property = factor(Property, levels = Property[order(MAP)]) ),
       aes( x = Property, y = MAP, ymin = cred_lower, ymax = cred_upper))+
  geom_bar(stat = 'identity', position = position_dodge())+
  geom_errorbar(position = position_dodge())+
  coord_flip()+
  ylab("MAP estimate and 95% HDI of Mixture parameter")
```


## Endorsement model comparison

To try to understand the generic endorsement data, we articulate a set of formal models that have been previously-proposed in the literature, a reduced-form of our proposed endorsement model, and our full endorsement model.
As the simplest baseline hypothesis, we first explore whether referent-category feature-probability itself predicts generic endorsement (e.g., does the fraction of \textsc{robins} that \textsc{lay eggs} predict the felicity of *Robins lay eggs*?).
As a secondary alternative hypothesis, we explore whether referent feature-probability and *cue validity*---the probability of a kind given the feature (described in more detail below)---predicts the endorsement data.
We fit these models using standard maximum-likelihood techniques; we model uncertainty in the input measurements (i.e., referent feature-probability, cue validity) by bootstrapping those data.

Our underspecified-threshold endorsement model has access to the full prior distribution on feature-probabilities $P(h)$ (measured in Expt. 1b), from which the interpreter model infers likely thresholds for the generic. 
The endorsement model is a model of a speaker who is deciding whether or not to say the generic to the interpreter model.
We identify three, theoretically-substantial components of this model: (a) the prior distribution $P(h)$, (b) the endorsement as a speaker decision of whether or not to produce the generic, and (c) the uncertain threshold.
As a control model, we use the same infrastructure but assign a fixed semantics to the utterance (i.e., analgous to the quantifier "some", which only rules out the lowest possible degree). 
This control model has both features (a) and (b) but lacks the uncertainty over the threshold (c).
This provides a strict test of the uncertain-threshold hypothesis.

```{r generic-regression-models}
d.cv <- read.csv(paste(project.path, "data/generics/endorsement/", "cue-validity-2-freeProduction-trials.csv", sep = ""))

d.target.items <- fromJSON(paste(project.path, "data/generics/endorsement/", "originalStims_wrongDeployment.json", sep = "")) %>%
  mutate(sentence = paste(category, property))


### Preprocessing
# - Force all responses to lower case. 
# - Remove spaces.
# - Fix mosquito mispellings.
# - Count "deertick" as "tick".
# - Remove plural "s".
# 
# Mark if the produced animal matches the generic of interest.

mosquito.mispellings <- c("mosqu", "mesqu", "misqu", "mosiq")

d.target.items <- fromJSON(paste(project.path, "data/generics/endorsement/", "originalStims.json", sep = "")) %>%
  mutate(
    property = gsub("'", "", property),
    Property = gsub("'", "", Property),
    sentence = paste(Category, Property),
    item = paste(category, property))


d.cv.bootstrapped <- data.frame()
resample_n <- length(levels(factor(d.cv$workerid)))
for (i in 1:10){
  
  d.cv.bsample <- d.cv %>% 
    select(workerid, property, response) %>% 
    spread(property, response) %>% 
    sample_n(resample_n, replace = TRUE) %>%
    gather(property, category, -workerid) %>% 
        group_by(property) %>%
    mutate(n = n(),
           item = paste(category, property)) %>%
    filter(item %in% d.target.items$item) %>%
    group_by(category, property) %>%
    summarize(mentions = n(),
              trials = mean(n), # mean(n) == n, because it's just the number of subjects
              prop = mentions / trials,
              prop = ifelse(is.na(prop), 0.01, prop),
              iteration = i)
  
  d.cv.bootstrapped <- bind_rows(d.cv.bootstrapped, d.cv.bsample)
}

empiricalLower = function(dist){
  xi <- quantile(dist, 0.025)
  return (xi[["2.5%"]])
}
empiricalUpper = function(dist){
  xi <- quantile(dist, 0.975)
  return (xi[["97.5%"]])
}
empiricalMean = function(dist){
  xi <- quantile(dist, 0.5)
  return (xi[["50%"]])
}


d.gen.cv.summary <- left_join(
    d.target.items %>% select(-sentence),
    d.cv.bootstrapped %>%
      group_by(property) %>%
      summarize(cv_mean = empiricalMean(prop),
                cv_ci_lower = empiricalLower(prop),
                cv_ci_upper = empiricalUpper(prop))
) %>%
  mutate(
    cv_ci_lower = ifelse(is.na(cv_mean), 0.01, cv_ci_lower),
    cv_ci_upper = ifelse(is.na(cv_mean), 0.01, cv_ci_upper),
    cv_mean = ifelse(is.na(cv_mean), 0.01, cv_mean)
  )
  
# quantile(filter(d.cv.bootstrapped, property == "is red")$prop, c(0.025, 0.5, 0.975))
# 
# d.cv.bootstrapped %>%
#   group_b

# need to filter by items specified with endorsement
# d.gen.cv.summary <- left_join(
#   d.target.items %>% select(-sentence),
#   d.cv %>%
#     rename(category = response) %>%
#     group_by(property) %>%
#     mutate(n = n(),
#            item = paste(category, property)) %>%
#     filter(item %in% d.target.items$item) %>%
#     group_by(category, property) %>%
#     summarize(mentions = n(),
#               trials = mean(n), # mean(n) == n, because it's just the number of subjects
#               prop = mentions / trials)
#   ) %>%
#   mutate(prop = ifelse(is.na(prop), 0.01, prop))

d.gen.endorse.prev.cue <- left_join(
  left_join(
    left_join(
      d.gen.endorse, 
      d.gen.endorse.catch %>% select(workerid, pass)
      ) %>%
      filter(pass == 1) %>%
    rowwise() %>%
    mutate(response = ifelse(response == "agree-key", 1, 0),
         sentence = gsub('&quotechar', '', sentence),
         sentence = gsub('lyme', 'Lyme', sentence),
          sentence = gsub('[.]', '', sentence)
    ),
    d.prev.summary %>% 
      rename(prev_mean = mean,
             sentence = item,
             prev_ci_lower = ci_lower,
             prev_ci_upper = ci_upper) %>%
      ungroup() %>%
      mutate(sentence = gsub('[.]', '', sentence))
    ),
  d.gen.cv.summary %>% select(-item))

glm.gen.endorse.prev <- glm(response ~ prev_mean, 
              data = d.gen.endorse.prev.cue, family = 'binomial')

glm.gen.endorse.prev.cv <- glm(response ~ prev_mean + cv_mean, 
              data = d.gen.endorse.prev.cue, family = 'binomial')

d.gen.endorse.prev.cue.uniq <- unique(select(
    d.gen.endorse.prev.cue, Property, Category, sentence, 
    prev_mean, prev_ci_lower, prev_ci_upper, cv_mean, cv_ci_lower, cv_ci_upper
  ))

d.gen.endorse.prev.cue.uniq.lower <- d.gen.endorse.prev.cue.uniq %>% 
  select(-prev_mean, -cv_mean) %>% 
  rename(prev_mean = prev_ci_lower,
         cv_mean = cv_ci_lower)
d.gen.endorse.prev.cue.uniq.upper <- d.gen.endorse.prev.cue.uniq %>% 
  select(-prev_mean, -cv_mean) %>% 
  rename(prev_mean = prev_ci_upper,
         cv_mean = cv_ci_upper)

d.gen.endorse.regression.prevalence <- left_join(
  d.gen.endorse.bayes,
  d.gen.endorse.prev.cue.uniq %>%
    mutate(
      prediction_mean = predict(glm.gen.endorse.prev, ., type = "response"),
      prediction_ci_lower = predict(glm.gen.endorse.prev, d.gen.endorse.prev.cue.uniq.lower, type = "response"),
      prediction_ci_upper = predict(glm.gen.endorse.prev, d.gen.endorse.prev.cue.uniq.upper, type = "response")
    ) %>%
    mutate(sentence = paste(Category, " ", Property, ".", sep = ""))
) %>% mutate(
  sqErr = (MAP_h - prediction_mean) ^ 2
)

d.gen.endorse.regression.prevalence.cuevalidity <- left_join(
  d.gen.endorse.bayes,
  d.gen.endorse.prev.cue.uniq %>%
    mutate(
      prediction_mean = predict(glm.gen.endorse.prev.cv, ., type = "response"),
      prediction_ci_lower = predict(glm.gen.endorse.prev.cv, d.gen.endorse.prev.cue.uniq.lower, type = "response"),
      prediction_ci_upper = predict(glm.gen.endorse.prev.cv, d.gen.endorse.prev.cue.uniq.upper, type = "response"),
      sentence = paste(Category, " ", Property, ".", sep = "")
      )
) %>% mutate(sqErr = (MAP_h - prediction_mean) ^ 2)

d.gen.endorse.regression <- bind_rows(
  d.gen.endorse.regression.prevalence.cuevalidity %>%
    mutate(src = "regression_Prev_Cuevalidity"),
  d.gen.endorse.regression.prevalence %>% 
    mutate(src = "regression_Prev")
  )
# ggplot(d.gen.endorse.regression, 
#        aes (x = prediction, y = MAP_h, ymin = low, ymax = high, color = cuevalidity))+
#   geom_errorbar(alpha = 0.3)+
#   geom_abline(intercept = 0, slope = 1, lty = 3)+
#   # geom_text_repel(data = d.gen.endorse.regression.prevalence.cuevalidity %>% 
#   #                   rename(cuevalidity = prop) %>% 
#   #                   filter(sqErr > 0.05), 
#   #                 aes(label = sentence, color = cuevalidity), force = 1, size = 3)+
#   geom_point()+
#   xlim(0,1)+
#   ylim(0,1)+
#   coord_fixed()+
#   xlab("Logistic model prediction")+
#   ylab("Human generic endorsement")+
#   facet_wrap(~src)

r2.gen.n <- length(d.gen.endorse.bayes$sentence)
r2.gen.regression.prev <- round(with(d.gen.endorse.regression.prevalence, cor(MAP_h, prediction_mean))^2,2)
mse.gen.regression.prev <-  round(mean(d.gen.endorse.regression.prevalence$sqErr), 3)

r2.gen.regression.prev.cv <- round(with(d.gen.endorse.regression.prevalence.cuevalidity, cor(MAP_h, prediction_mean))^2, 3)
mse.gen.regression.prev.cv <- round(mean(d.gen.endorse.regression.prevalence.cuevalidity$sqErr), 3)

intermediate.prev.quantiles <-  quantile(d.gen.endorse.regression.prevalence$prev_mean, c(0.25, 0.76))


d.gen.endorse.regression.prevalence.intermediateprev <- d.gen.endorse.regression.prevalence %>%
  filter(
    (prev_mean > intermediate.prev.quantiles[["25%"]]) &
    (prev_mean < intermediate.prev.quantiles[["76%"]])
)
r2.gen.n.intermedprev <- length(d.gen.endorse.regression.prevalence.intermediateprev$sentence)

r2.gen.regression.prev.intermedprev <- round(with(d.gen.endorse.regression.prevalence.intermediateprev, cor(MAP_h, prediction_mean))^2,2)
mse.gen.regression.prev.intermedprev <-  round(mean(d.gen.endorse.regression.prevalence.intermediateprev$sqErr), 3)
```

### Baseline models

#### Referent feature-probability

From the feature-probability prior data (Expt. 1b), we estimate participants' beliefs about the probability of a feature for the referent categories (e.g., the percentage of \textsc{robins} that \textsc{lay eggs}).
We find a little over half of the variance in the endorsement data is explained this way ($r^2(`r r2.gen.n`) = `r r2.gen.regression.prev`$; MSE=$`r mse.gen.regression.prev`$; Figure \  \@ref(fig:generic-endorsement-priors-figure), upper-right: lower-left facet). 
The referent feature-probability baseline is a decent model of these data because our stimulus set includes generics that are true with high feature-probability properties (e.g., *Leopards have spots.*) and generics that are false with low feature-probability properties (e.g., *Leopards have wings.*). 

However, large deviations from an account based purely on referent feature-probabilities remain: Generics in which the referent-category has intermediate feature-probabilities (feature-probability quartiles 2 and 3: $`r round(100*intermediate.prev.quantiles[["25%"]])`\% < \text{feature-probability} < `r  round(100*intermediate.prev.quantiles[["76%"]])`\%$), are not at all explained by referent feature-probability ($r_{Q2,3}^2(`r r2.gen.n.intermedprev`) = `r r2.gen.regression.prev.intermedprev`$; MSE = $`r mse.gen.regression.prev.intermedprev`$).
This includes generics that are judged true with relatively low referent feature-probabilities (e.g., "Mosquitos carry malaria") and false with relatively high referent feature-probabilities (e.g., "Sharks don't eat people").

#### Cue validity and referent feature-probability

##### Model specification

As a secondary baseline hypothesis, we explore whether prevalence and *cue validity* together predict generic endorsementse.
If feature-probability is the "forward probability" (e.g., one's predictions about whether or not an entity will lay eggs after learning that the entity is a robin), *cue validity* is the inverse probability: $P(x \in k \mid x \in f)$ (e.g., one's predictions about whether or not the entity is a robin, upon learning that it lays eggs).
*Cue validity* thus captures the distinctiveness of the feature for the category. 
For example, if one learns that an entity carries malaria, that entity is probably a mosquito because only mosquitos carry malaria. 
Previous studies have found cue validity to be correlated with generic endorsement [@Khemlani2012], though its role in generic meaning is controversial [@Leslie2007; @Leslie2008].
Empirically, cue validity is not a *necessary* criteron for generic endorsement (e.g., in the sentence: "Dogs have four legs").
Theoretically, if generics have to do with generalizations, which are formally described by the "forward probability", it is not obvious why the "inverse probability" (cue validity) would be part of the semantic content of generics.


We described in Expt 1b. that $P(h)$ can be decomposed into a prior on kinds $P(k)$ and the likelihood of a feature-probability given the kind $P(h \mid k)$.
If there is a finite number of kinds in the feature-probability prior for a feature, and one knows the probability of the feature for each kind (i.e., $P(h \mid k)= P(f \mid k)$), then the cue validity of that feature for a particular kind is encoded in the feature-probability prior and it can be derived via Bayes' Rule:
$$ P(k \mid f) = \frac{P(f \mid k) \cdot P(k)}{\sum_{k' \in K} P(f \mid k') \cdot P(k')} $$
Previous empirical studies of generic language, however, have not measured the feature-probability prior and instead try to estimate cue-validity by asking about the distinctiveness of features for given animals [e.g., @Khemlani2012]
In keeping with this tradition, we measured cue validity using a free production paradigm, following @Cree2006.
Participants ($n = 50$) were supplied with a feature (e.g., "X lays eggs") and asked to generate a kind (e.g., "what do you think X is ?").
For a detailed discussion of the results of this experiment and comparison to other measurements of cue validity as well as the *feature-probability prior derived cue validity*, see Appendix.

##### Results

A linear model that uses predictors for both referent feature-probability and cue validity does a better job at explaning the endorsement data than just feature-probability alone ($r^2(`r r2.gen.n`) = `r r2.gen.regression.prev.cv`$; MSE=$`r mse.gen.regression.prev.cv`$).
This model is able to account for the endorsements of examples like "Mosquitos carry malaria" and "Lions have manes", as these features are very diagnostic of the kind.
Deviations, however, still remain.
For example, "Robins lay eggs" still receives only intermediate endorsement by this model, and "Mosquitos don't carry malaria" is judged to be a good statement.

These highlight a shortcoming of a simple model based on cue validity.
"Lays eggs" is a somewhat diagnostic feature for *birds*, but there are many kinds of birds, and the feature is not itself diagnostic for "Robins".
A simple metric like cue validity is too blunt to capture this subtlety.
Additionally, negative properties (like, "not carrying malaria") are completely undiagnostic for almost every category, and the fitted regression model doesn't know how to penalize "Mosquitos don't carry malaria" because the feature-probability is high.

### Communicative endorsement models

The model we propose for endorsing generalizations is a model of an agent that decides whether or not to say the generalization to a naive interpretor.
The endorsement model $Endorse$ (Eq. \ref{eq:S1}) decides whether or not to assert the generalization to a naive interpretor $Interpret$ (Eq. \ref{eq:L0}) conditioned on a particular referent-category feature-probability $h'$ (e.g.,  the percentage of robins that lay eggs).
We use the empirically estimated referent-category feature-probability (the same as used in the baseline regression model) as the $h'$ in the endorsement model.
The empirically measured priors from Expt. 1b are used as the literal interpreter's feature-probability prior $P(h)$.

To incorporate the measurements of these model components into our endorsement model, we build a single, Bayesian joint-inference model of the referent-category feature-probabilities $h'$, feature-probability priors $P(h)$ (both from Expt. 1b data), and the endorsement data (Expt. 1a). 
Modeling the data from both Expt. 1a and 1b by a single, joint-inference model simultaneously makes explicit all of our assumptions about how these data were generated, and is the proper Bayesian way to represent the uncertainty in our measurement of the prior elicitation data (see Appendix and Figure \  \@ref(fig:genericsModelDiagram) for further model specification details).
The $Endorse$ model is then fully specified after setting the single optimality parameter $\lambda_1$ (in Eq. \ref{eq:S1}).
To learn about the credible values of the parameters of the model and resulting model predictions, we ran an incrementalized version of MCMC [@Ritchie] for 3 chains of 100,000 iterations, discarding the first 50,000 for burnin.

The number of parameters of this full data-analytic model is relatively large, and thus there is a concern that the parameters themselves (as opposed to the exact structure of our model) are doing the work. 
To address this concern, we construct a strong, alternative model that uses the same data analytic strategy with the same number of the parameters.
This control model is a lesioned version of the uncertain threshold model, which we construct by removing the uncertainty about the semantic threshold $\theta$. 
All other architectural features (the prior distribution, the information-theoretic decision of whether or not to produce the generalizations) as well as their respective parameters are the same as for the uncertain threshold model.
We examine the predictions of the control model first.

#### Fixed-threshold model

The fixed-threshold model is a lesioned version of the unceratin-threshold model: It models endorsement as an information-theoretic decision of producing the generic vs. an alternative, and has access to the full prior distribution on feature-probabilities $P(h)$. 
The only difference is that this model uses a fixed-threshold as the core meaning of a generic statement, rather than an uncertain threshold.
For this model, we use $\theta = 0$ (corresponding intutively to the semantics of the quantified "Some" statements).

^[
  Other fixed-threshold models are possible (e.g., generic means "Most" $\theta = 0.5$) but require the further   specification of a noise-model for participants' responses in the cases where participants endorse the statement when it is literally false (e.g., "Mosquitos carry malaria" would be literally false under a $\theta = 0.5$ semantics).
]
This is a strong alternative model to our uncertain-threshold model; the only difference concerns the threshold.

```{r generic-RSAmodels, fig.width = 8}
n_chains <- 3
n_samples <- 100000
burn <- n_samples / 2
lg <- 20

model_prefix <- "results-generics-jointModel-S1-smntcs_"
#fixed_model_prefix <- "results-generics-jointModel-wNoise-inferFixedThreshold-20bins-S1-smntcs_some-"
# m.samp <- data.frame()
# m.samp.fixed <- data.frame()
#for (i in seq(1, n_chains)){
  #mi.gen <- fread(paste(project.path,  "models/generics/results/", model_prefix, "generic-",
  #                  n_samples, "_burn", burn, "_lag", lg, "_chain", i, ".csv", sep = ""))
  #m.samp <- bind_rows(m.samp, mi.gen %>% mutate(chain = i))

#   mi.fixed <- fread(paste(project.path,  "models/generics/results/", fixed_model_prefix,
#                     n_samples, "_burn", burn, "_lag", lg ,"_chain", i, ".csv", sep = ""))
#   m.samp.fixed <- bind_rows(m.samp.fixed, mi.fixed %>% mutate(chain = i))
# }
# 
# save(m.samp,
#      file = paste(project.path,  "models/generics/results/", model_prefix, "generic-",
#                     n_samples, "_burn", burn, "_lag", lg, "_",n_chains , "chains.RData", sep = ""))
# 
# save(m.samp.fixed,
#      file = paste(project.path,  "models/generics/results/", model_prefix, "some-",
#                     n_samples, "_burn", burn, "_lag", lg, "_",n_chains , "chains.RData", sep = ""))


load(file = paste(project.path,  "models/generics/results/", model_prefix, "generic-",
                    n_samples, "_burn", burn, "_lag", lg, "_",n_chains , "chains.RData", sep = ""))

# load(file = paste(project.path,  "models/generics/results/", model_prefix, "some-",
#                     n_samples, "_burn", burn, "_lag", lg, "_",n_chains , "chains.RData", sep = ""))
load(file = paste(project.path, "models/generics/results/results-generics-jointModel-S1-some-inferFixedThreshold-noise-100000_burn50000_lag20_2chains.RData", sep = ""))

# m.samp.fixed <- fread(paste(project.path,  "models/generics/results/", "results-generics-jointModel-wNoise-20bins-S1-smntcs_some-100000_burn50000_lag20_chain1.csv", sep = ""))
# 
# m.samp.fixed <- fread(paste(project.path,  "models/generics/results/", "results-generics-jointModel-wNoise-inferFixedThreshold-20bins-S1-smntcs_some-100000_burn50000_lag20_chain1.csv", sep = ""))
# 
# tail(m.samp.fixed)

fixed.params.posterior <-  m.samp.fixed %>% 
  filter(param %in% c("noise", "fixedThreshold")) %>%
  group_by(param) %>% 
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))

m.gen.somemodel.endorsement <- m.samp.fixed %>%
  filter(type == 'endorsement', param == "some") %>%
  group_by(type, param, property, category) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))

# m.gen.fullmodel.endorsement <- m.samp.samp %>%
#   rename(type = Parameter, property = Property, category = Category, val = Value, param = Extra) %>%
#   group_by(type, property, category, param) %>%
#   summarize(MAP = estimate_mode(val),
#             cred_upper = hdi_upper(val),
#             cred_lower = hdi_lower(val))

m.gen.fullmodel.endorsement <- m.samp %>%
 filter(type == 'endorsement', param == 'generic') %>%
  # filter(type == 'predictive', param == 'generic') %>%
 # filter(type == 'endorsement') %>%
  group_by(type, param, property, category) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))
```

```{r generic-model-scatters}
m.gen.endorse.rsa <- bind_rows(
  left_join(
    d.gen.endorse.bayes,
    m.gen.fullmodel.endorsement %>%
      mutate(sentence = paste(category, " ", property, ".", sep = "")) %>%
      ungroup() %>%
      select(-type, -param)
  ) %>%
    mutate(src = "generics_model"),
  left_join(
    d.gen.endorse.bayes,
    m.gen.somemodel.endorsement %>%
      mutate(sentence = paste(category, " ", property, ".", sep = "")) %>%
      ungroup() %>%
      select(-type, -param)
  ) %>%
    mutate(src = "some_model")
)


r2.gen.rsa.generics <- compute_r2(
  m.gen.endorse.rsa %>% filter(src == "generics_model"), 
  "MAP", "MAP_h", sigfigs = 4
)
  
r2.gen.rsa.fixed <- compute_r2(
  m.gen.endorse.rsa %>% filter(src == "some_model"),  
  "MAP", "MAP_h", sigfigs = 4
)

mse.gen.rsa.generics <- compute_mse(
  m.gen.endorse.rsa %>% filter(src == "generics_model"), 
  "MAP", "MAP_h", sigfigs = 5
)

mse.gen.rsa.fixed <- compute_mse(
  m.gen.endorse.rsa %>% filter(src == "some_model"),  
  "MAP", "MAP_h", sigfigs = 5
)

generics.endorsement.models <- bind_rows(
  d.gen.endorse.regression,
  left_join(
    m.gen.endorse.rsa %>% 
      rename(prediction_mean = MAP, prediction_ci_lower = cred_lower, prediction_ci_upper = cred_upper),
    d.gen.endorse.regression %>% 
      select(sentence, Property, Category, prev_mean, cv_mean)
  )
) %>%
  mutate(src = factor(src, levels = c( "some_model",
                                      "generics_model",
                                      
                                      "regression_Prev", 
                                      "regression_Prev_Cuevalidity"
                                     ),
                      labels = c("Fixed semantics model",
                                 "Uncertain semantics model",
                                 
                                 "Feature-probability alone",
                                 "Cue validity + Feature-probability"
                                 ))) %>%
  ggplot(., aes ( x = prediction_mean, xmin = prediction_ci_lower, xmax = prediction_ci_upper,
                  y = MAP_h, ymin = low, ymax = high, fill = prev_mean ))+
  geom_abline(intercept = 0, slope = 1, lty = 3)+
  geom_linerange(alpha = 0.4)+
  geom_errorbarh(alpha = 0.4)+
  geom_point(shape = 21, size = 3)+
  scale_x_continuous(limits = c(-0.01, 1.01), breaks = c(0,  1))+
  scale_y_continuous(limits = c(-0.01, 1.01), breaks = c(0, 1))+
  #scale_fill_continuous(low = "#2b83ba", high = "#d7191c")+
  coord_fixed()+
  xlab("Model prediction")+
  ylab("Human generic endorsement")+
  facet_wrap(~src, nrow = 2)+
  #theme(legend.position = "bottom")+
  guides(fill = F)
```


```{r generic-model-insets, fig.width = 4.75, fig.height = 1.5}
example.generics.properties <- c("dont eat people", 
                      "carry malaria", 
                      "lay eggs", 
                      "are female",
                      "have spots")

example.generics <- c("Tigers dont eat people", 
                      "Mosquitos carry malaria", 
                      "Robins lay eggs", 
                      "Robins are female",
                      "Leopards have spots")

m.gen.fullmodel.prior.parameters <- m.samp %>%
  filter(type == "prior", property %in% example.generics.properties) %>%
  group_by(param, property, category) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))

# use MAP estimates to generate L(h | generic) & L(h | silence) predictions

m.gen.fullmodel.prior.parameters.tidy <- m.gen.fullmodel.prior.parameters %>%
  ungroup() %>%
  select(param, property, category, MAP) %>%
  mutate(param = paste(param, category, sep = "_")) %>%
  select(-category) %>%
  spread(param, MAP) %>%
  rename(mix = mixture_NA, 
         stable_mean = stableFreq_mean, 
         stable_concentration = stableFreq_sampleSize) %>%
  mutate( a = stable_mean * stable_concentration, 
          b = (1 - stable_mean) * stable_concentration)

gen.listener.predictions <- data.frame()
  
# run interpreter model to generate "hypothetical intepretation" plots
for (p in example.generics.properties){
 priorParams <- m.gen.fullmodel.prior.parameters.tidy %>% filter(property == p) 
 inputData = list(prior = list(params = data.frame(a = priorParams[["a"]],
                                                   b = priorParams[["b"]]),
                               mix = priorParams[["mix"]]), 
                  utt = "generic")
 l0.rs.posterior <- webppl(paste(rsaHelpers, uncertain.threshold.model, uncertain.threshold.call, sep = '\n'), data = inputData, data_var = "data")
 l0.rs.prior <- webppl(paste(rsaHelpers, no.utterance.model, no.utterance.call, sep = '\n'), data = inputData, data_var = "data")
 gen.listener.predictions <- bind_rows(
   gen.listener.predictions, 
   l0.rs.prior %>% select(value) %>% mutate(property = p, Parameter = "state_Prior"),
    l0.rs.posterior %>% select(value) %>% mutate(property = p, Parameter = "state_Posterior")
   )
}

m.gen.fullmodel.target.prevalence <- m.samp %>%
  filter(type == "targetPrevalence") %>%
  group_by(param, property, category) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))

m.gen.speakerBeliefs <- m.samp %>%
  filter(type == "targetPrevalence") %>%
  mutate(Sentence = paste(category, property)) %>%
  filter(Sentence %in% example.generics) %>%
  mutate(property = factor(property))

# m.samp.prev.params <- m.samp %>%
#     filter(type == "targetPrevalence") %>%
#     mutate(Sentence = paste(category, property)) %>%
#     filter(Sentence %in% example.generics) %>%
#     mutate(parameter = paste(param, property, category, sep = "_")) %>%
#     select(-param, -property, -category, -chain, -type) %>%
#     group_by(parameter) %>%
#     mutate(iteration = ave(parameter==parameter, parameter, FUN=cumsum)) %>%
#     ungroup() %>%
#     separate(parameter, into = c("parameter", "property", "category"), sep= "_") %>%
#     group_by(category, property, iteration) %>%
#     spread(parameter, val) %>%
#     rowwise() %>%
#     mutate(
#       a = mean*sampleSize,
#       b = (1-mean)*sampleSize,
#       val = rbeta(n = 1, shape1 = a, shape2 = b)
#       ) %>%
#     ungroup()

gen.inset.distributions <- bind_rows(
  gen.listener.predictions %>%
    mutate(category = NA),
  m.gen.speakerBeliefs %>%
    select(property, val) %>%
    rename(value = val) %>%
    mutate(Parameter = "speakerBeliefs")
)


category.text.labels <- data.frame(property = c("dont eat people", 
                      "carry malaria", "lay eggs",  "are female", "have spots"),
             category = c("Tigers", "Mosquitos", "Robins", "Robins", "Leopards"),
             x = c(0.3, 0.3, 0.47, 0.05, 0.6),
            y = c(0.45, 0.5, 0.45, 0.5, 0.26))

gen.inset.distributions.refactored <- gen.inset.distributions %>%
  mutate(Parameter = factor(Parameter, levels = c("state_Prior",
                                                  "state_Posterior",
                                                  "speakerBeliefs"),
                            labels = c("Feature-probability Prior", 
                                       "Interpretation given generalization", 
                                       "Referent feature-probability")),
         property = fct_relevel(property,
                                "have spots", "lay eggs","carry malaria", 
                                "are female", "dont eat people") )

generics.endorsement.insets <- ggplot(gen.inset.distributions.refactored, 
                                      aes( x = value, fill = Parameter, 
                 color = Parameter, lty = Parameter, alpha = Parameter ))+
  geom_density(aes(y = ..scaled..), adjust = 4, size = 1)+
  facet_wrap(~property, nrow = 1)+
  geom_label_repel(data = category.text.labels,
                  aes(label = category, x = x , y = y),
                  inherit.aes = F, color = "#2b83ba")+
    #scale_fill_manual(values = c("#636363", "#abdda4", "#2b83ba", "#d7191c"))+
    #scale_color_manual(values = c("#636363", "#abdda4", "#2b83ba", "#d7191c"))+
    scale_fill_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    scale_color_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    scale_alpha_manual(values = c(0.6, 0.4, 0))+
    #scale_linetype_manual(values = c(3, 4, 2, 1))+
    scale_linetype_manual(values = c(3, 4, 1))+
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    xlab("Feature-probability") +
    ylab("Scaled probability density")+
    theme(legend.position = "bottom", legend.title = element_blank())
```



```{r generics-endorsement-figure, fig.width = 11, fig.height=8.5, fig.cap="Endorsing generalizations about categories. Top left: Human elicited endorsements for thirty generic sentences reveal a continuuum of endorsements. Top right: Model fits for the uncertain semantics speaker model (upper right), a fixed semantics speaker model (upper left), and regression models based on referent feature-probability alone (lower left) and feature-probability + cue validity (lower right). Bottom: Five example feature-probability priors, listener posteriors upon hearing the generalization, and speaker belief distributions about the referent feature-probability. These distributions are inferred using all three data sources from Expt. 1 (see Figure \\ref{fig:genericsModelDiagram} for an overview of the Bayesian data analytic approach).", cache = F}

fig.gen.endorse.data <- generics.endorsement.spectrum +
  theme(plot.margin = unit(c(6,2,0,0), "pt")) #+ ggtitle("Feature-probability priors")
fig.gen.endorse.models <- generics.endorsement.models +
  theme(plot.margin = unit(c(6,2,0,0), "pt")) #+ ylab("")
fig.gen.endorse.priors <- generics.endorsement.insets +
  theme(plot.margin = unit(c(26,10,0,10), "pt"))

#+ 
    #theme(strip.text.y = element_blank())#+ ggtitle("Interpreter model posterior")
#p4 <- s1.simulations.scatter + theme(plot.margin = unit(c(18,0,6,6), "pt"))
  
fig.gen.endorse.toprow <- plot_grid(fig.gen.endorse.data,
                                     fig.gen.endorse.models, 
                                    # align = 'v',
                                    #  axis = 'l',
                                    labels = c("A", "B"),
                                    rel_widths = c(0.9, 1))



plot_grid(fig.gen.endorse.toprow,
          fig.gen.endorse.priors,
          labels = c("", "C"), nrow = 2, rel_heights = c(1.1, 0.6)
          )
# prow <- plot_grid( p1 + theme(legend.position="none"),
#            p2 + theme(legend.position="none"),
#            p3 + theme(legend.position="none"),
#            p4,
#            align = 'vh',
#            labels = c("A", "B", "C"),
#            hjust = -1,
#            #nrow = 1,
#            #rel_widths = c(1.2, 3, 2)
#            rel_widths = c(1.2, 3, 1.05, 1.7)
#            )

# grid.arrange(generics.endorsement.spectrum, 
#              generics.endorsement.models,
#              generics.endorsement.insets, ncol = 2,
#              layout_matrix = cbind(c(1,1,3), c(2,2,3)))

```

To evaluate this model, we examine the posterior distribution which represents what we should believe about the latent parameters of the model (referent feature-probabilities, feature-probability priors, and the single speaker optimality parameter) given the observed data, as well as what predictions of the model given these parameter values.
Importantly, the model recapitulates the same feature-probability prior data (e.g., the probability of laying eggs among various species) and the referent-category feature-probability data (e.g., the probability of laying eggs among robins; see Appendix for full posteriors) as it did when the Bayesian data analysis model only included these data and their associated model (results section of Expt. 1b).
This means the theoretically-interesting predictions of this model --- predictions of generic endorsement --- are based on intuitively meaningful model components (i.e., the shapes of the feature-probability distributions). 

We compare the fixed-threshold model's posterior predictive distribution of generic endorsement to the empirical truth judgments.
Figure \@ref(fig:generics-endorsement-figure) (top right; top left subplot) shows the fixed-threshold model's ability to predict the generic endorsement data ($r^2(`r r2.gen.n`) = `r r2.gen.rsa.fixed`$; MSE = $`r mse.gen.rsa.fixed`$).
The model fails to distinguish any of the generic sentences because they are all somewhat true. 
All items get endorsed by at least some participants; the model then infers that the referent feature-probability $h'$ must be greater than 0\%; but, if the referent feature-probability is greater than 0\%, the statement is 100\% true given the fixed-threshold semantics.

The endorsement model decides if it better to assert the 0\%-threshold statement or stay silent. 
For some items, a fixed-threshold meaning carries some information-gain over silence (i.e., some model predictions \> 0.5). 
For many items, however, the 0\%-threshold is almost as meaning-less as silence; the model, thus, has a hard time strongly endorsing statements (most model predictions around 0.5\%).
Consistent with *a priori* intuition, assigning a fixed-semantics is too lenient a formalization for generic sentences. 
This demonstrates that the Bayesian data analytic approach used to fit the parameters of our model is not itself causally sufficient for any model of endorsements to explain the human endorsement data.
The fixed-threshold model, even with access to the full prior distribution $P(h)$, falls short in explaining the data.

\mht{Outfit fixed-threshold model with noise parameter?}

#### Uncertain threshold model

Our underspecified threshold model is the same as the fixed-threshold RSA model described above, except that rather than having a fixed threshold at $\theta = 0$, it has uncertainty over the threshold specified with a uniform prior distribution over $\theta$.
We construct the same joint-inference Bayesian data analytic model as we did for the fixed-threshold analysis.
As we see in Figure \ \@ref(fig:generics-endorsement-figure) (top right, top right subplot), the endorsement model that relies upon an uncertain-threshold semantics explains nearly all of the variance in human endorsements ($r^2(`r r2.gen.n`) = `r r2.gen.rsa.generics`$; MSE = $`r mse.gen.rsa.generics`$).

To gain some intuition for why that is the case, examine the bottom row of Figure \ref{fig:generics-endorsement-figure}.
The blue, solid lined distribution corresponds to the speaker's beliefs about the probability of the feature for the referent-category (i.e., *referent feature-probability*).
The grey distribution shows the listener's prior distribution over probabilities of the feature (these are the corresponding Probability Density Functions of the Cumulative Density Functions shown in Figure \ \@ref(fig:generic-endorsement-priors-figure).
It is also the listener $L_0$'s posterior on referent feature-probability upon hearing the silent utterance.
The red distribution shows the $L_0$ posterior upon hearing the generic utterance that corresponding to an uncertain threshold-semantics. 
The speaker model decides when the generic utterance conveys information that would bring the listener's distribution more in line with the speaker's own distrubtion. 
Thus, it finds that "Leopards have spots", "Robins lay eggs", and "Mosquitos carry malaria" are also useful things to say.
"Robins are female" is neither good nor bad, as the information-content of the utterance is very small relative to silence.
"Tigers don't eat people" is rather misleading, however; it doesn't bring the listener closer to the speaker's distribution, despite the speaker believing that the feature-probability is more than half. 

## Discussion

Generic language is the premier case study for generalizations in language. 
Generics have been studied extensively in the cognitive and developmental psychological literatures and have been shown to have deep implications for wide ranging phenomena from stereotype propogataion [@Rhodes2012] to motivation [@Cimpian2007].
Heretofore, no formal model has made precise, quantitative predictions about the flexible endorsement patterns of generics (*Birds lay eggs* v. *are female*; *Mosquitos carry malaria*).

By measuring listeners' prior beliefs about the probability of the property across kinds and the feature-probability of the referent kind (e.g., the percentage of birds that lay eggs), we were able to show how a semantics based on probability is tenable in spite of alleged counterexamples.
The key theoretical insight is that the threshold must be *underspecified* in the semantics and determined in context.
The decision of whether or not to endorse the generic comes down to whether or not the generic would make the interpretation belief distribution more in line with the feature-probability that the endorser had in mind.

In explaining the variable endorsements of generics, we measured the prior distribution over the probabilities of features across categories in addition to the probability of the feature in the referent-category (e.g., the percentage of robins that lay eggs) to model the endorsement data.
Thus, we have shown how these measurements are related to generic endorsement via our model. 
In our second case study, we extend the theory to the domain of events and provide the first evidence that the factors identified by our theory are causally related to endorsement. 
<!-- This evidence is correlational in nature, however. -->
<!-- We do not yet know if the variables that we are measuring are causally related to endorsement. -->

