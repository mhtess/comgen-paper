# Case Study 1: Generic Language

Learning from generalizations about categories (i.e., *generic language*; e.g., "Dogs bark.") is believed to play a central role in concept and theory formation [e.g., @Gelman2004], stereotype propagation [@Rhodes2012], motivation [@Cimpian2007], and many other facets of everyday reasoning.
In addition, generics have been the case study of choice for the semantics of the language of generalization because of their tantalizing similarity to quantified statements (e.g., "Most dogs bark").
However, intuitions and empirical data argue that generics simply do not reduce to quantified statements in a simple way [e.g., @Khemlani2012; @Prasada2013; @Cimpian2010].

We first investigate how the *underspecified threshold* endorsement model predicts actual human endorsements of generic statements.
We measure endorsement for thirty generic sentences that cover a range of conceptual distinctions previously discussed in the empirical literature on generics [@Prasada2013]: characteristic features displayed by a majority (e.g., "Ducks have wings."),  characteristic features displayed by a minority (e.g., "Robins lay eggs."), features that are *striking* or *dangerous* (e.g. "Mosquitos carry malaria."),  noncharacteristic features displayed by a minority (e.g., "Robins are female."), and features that are totally absent (e.g. "Lions lay eggs.").
We further craft sentences with the goal of eliciting the full range of acceptability judgments (intuitively: "true", "false", and "indeterminate") for generics with properties of low, medium, and high referent-prevalence (Expt. 1a).
We examine generics about animal categories in order to reliably measure the prior belief distribution over the prevalence of features $P(p)$.
The prevalence elicitation procedure (Expt. 1b) includes measurements of the referent-prevalence $p$ for different categories  (e.g., $P(x \text{ lays eggs} \mid x \text{ is a robin})$), allowing us to generate predictions for the endorsement model (Eq. \ref{eq:S1}) as well as for simpler, alternative models. 

## Experiment 1a: Generic endorsements

In this experiment, we elicit human endorsements for generalizations about categories (*generics*) taken from the linguistic and psychological literature on generics [@Prasada2013].
The goal of this study is to observe a range of endorsements for generic statements about animal categories. 

### Method

```{r generic-endorsement}
load("cached_results/case1_endorseSummary_byItem.RData") # d.gen.endorse.bayes.prev, d.gen.endorse.manipulation.check.bayes
generics.endorsement.spectrum <- ggplot(
    d.gen.endorse.bayes.prev, 
    aes(x = sentence, y = MAP_h - 0.5, ymin = low - 0.5, ymax = high - 0.5, fill = prev)
  )+
  geom_bar(stat = 'identity', position = position_dodge(), color='black',  alpha = 1)+
  geom_linerange(position = position_dodge(), size = 1)+
  coord_flip()+
  ylab("Human generic endorsement")+
  xlab("")+
  scale_fill_viridis(breaks = c(0, 1), limits = c(0, 1))+
  scale_y_continuous(limits = c(-0.5,0.5), breaks = c(-0.5, 0, 0.5), labels = c(0, 0.5, 1))+
  guides(fill = guide_colorbar(title = 'Referent prevalence',
                               ticks = F))+
  theme(legend.position = 'bottom')
```

#### Participants

We recruited 100 participants over Amazon's crowd-sourcing platform Mechanical Turk (MTurk).
Participants were restricted to those with US IP addresses and with at least a 95\% MTurk work approval rating (the same criteria apply to all experiments reported). 
4 participants were excluded for failing to recall the button corresponding to *agreement* in the forced-choice task.
5 participants self-reported a native language other than English; removing their data has no effect on the results reported. 
The experiment took about 3 minutes and participants were compensated \$0.35.

#### Procedure and materials

Participants were shown thirty generic sentences in a randomized order.
They were asked to press one of two buttons (*P* or *Q*; randomized between-participants) to indicate whether they agreed or disagreed with the sentence (see Figure\ \@ref(fig:generics-endorsement-figure)A for the full list). 
The thirty sentences covered a range of conceptual categories described above.
Approximately ten true, ten false, and ten uncertain truth-value generics were selected.
As an attention check, participants were asked at the end of the trials which button corresponded to "Agree".
Four participants were excluded for failing this trial.

### Results

```{r generic-endorsement-manipulationcheck}
# MHT's truth judgments of sentences
# t --> true; f --> false; i --> indeterminate/uncertain
all.sentences <- list("Cardinals are red." = "t",
 "Ducks have wings." = "t",
 "Kangaroos have pouches." = "t",
 "Kangaroos have spots." = "f",
 "Leopards are juvenile." = "i",
 "Leopards have spots." = "t",
 "Leopards have wings." = "f",
 "Lions are male." = "i",
 "Lions have manes." = "t",
 "Lions lay eggs." = "f",
 "Mosquitos attack swimmers." = "i",
 "Mosquitos carry malaria." = "t",
 "Mosquitos dont carry malaria." = "f",
 "Peacocks dont have beautiful feathers." = "f",
 "Peacocks have beautiful feathers." = "t",
 "Robins are female." = "i",
 "Robins carry malaria." = "f",
 "Robins lay eggs." = "t",
 "Sharks are white." = "i",
 "Sharks attack swimmers." = "t",
 "Sharks dont attack swimmers." = "f",
 "Sharks have manes." = "f",
 "Sharks lay eggs." = "i",
 "Swans are full-grown." = "i",
 "Swans are white." = "t",
 "Ticks carry Lyme disease." = "t",
 "Ticks dont carry Lyme disease."= "f",
 "Tigers dont eat people."= "f",
 "Tigers eat people." = "t",
 "Tigers have pouches."= "f")

true_MAP <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "t")[[1,"MAP_h"]]
true_low <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "t")[[1,"low"]]
true_high <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "t")[[1,"high"]]

false_MAP <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "f")[[1,"MAP_h"]]
false_low <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "f")[[1,"low"]]
false_high <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "f")[[1,"high"]]

uncertain_MAP <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "i")[[1,"MAP_h"]]
uncertain_low <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "i")[[1,"low"]]
uncertain_high <- filter(d.gen.endorse.manipulation.check.bayes, truth_judgment == "i")[[1,"high"]]
```

As a manipulation check, the first author assigned an \emph{a priori} truth-judgment (true/false/indeterminate) to each stimulus item.
As one would expect, there were substantial differences in empirical endorsements: true generics were almost universally endorsed (Maximum A-Posteriori estimate and 95\% credible interval of endorsement probability: $`r true_MAP`$ $[`r true_low`, `r true_high`]$);
indeterminate generics were endorsed at a rate \emph{less} likely than chance ($`r uncertain_MAP`$ $[`r uncertain_low`, `r uncertain_high`])$) but substantially more than false generics  ($`r false_MAP`$ $[`r false_low`, `r false_high`])$).

In addition to these categorical differences between our thirty generic statements, we find a continuum of endorsement values (Figure\ \@ref(fig:generics-endorsement-figure)A).
Such a continuum of judgments is already evidence against any theory that only predicts categorically whether a generic statement is true or false.
Ideally, a complete theory of genericity should be able to explain statements that are endorsed completely, rejected completely, and all shades in between.
We next measure the prevalence prior distribution and use them to articulate a set of quantitative models that try to predict this quantitative variability in endorsements. 

## Experiment 1b: Prevalence prior elicitation

The prevalence prior $P(p)$ in Eq. \ref{eq:L0} describes the belief distribution on the probability of a given feature (e.g., \textsc{lays eggs}) across relevant categories. 
To get an intuition for the kind of knowledge encoded in this belief distribution, consider the following thought experiment: You are walking around and you come across an instance of your favorite kind of animal from Earth (e.g., a peregrine falcon). 
Ask yourself "is it female?" 
Your answer will probably depend upon the percentage of the category that you believe to be female (e.g., the percentage of female peregrine falcons, i.e., 50\%). 
Now, do you think it lays eggs? 
Your answer to this question will likely depend upon on what animal you're thinking of: If you're thinking of a peregrine falcon, the probability is similar to that of being female (50\%) because female peregrine falcons lay eggs; if you're thinking of a mammal, it's likely that it won't lay eggs (0\%) because most mammals give birth to live young. 
That is, the answer is roughly either 50\% or 0\% depending on the kind of creature you are thinking of.^[
  The subjective probability may, in fact, be non-zero and small as opposed to 0. 
  Non-zero probabilities allow for the intuitive possibility of a mammal animal that generally reproduces by giving live birth which in fact lays eggs, potentially explained by some strange generic mutation.
]

This thought experiment decomposes the prevalence prior $P(p)$ into a prior distribution on kinds $P(k)$ and then a conditional probability of the prevalence given the kind $P(p \mid k)$.
We will use this decomposition to measure the prevalence prior $P(p) = \int_{k} P(p \mid k) \diff k$ and build a structured statistical model to accurately reconstruct the prevalence prior.
We measured prior distributions empirically for the set of properties (e.g., *lays eggs, carries malaria*; 21 in total) used in our generic sentences in Expt. 1a.
To create a larger set of properties, we reverse-code responses for five properties to create their corresponding negative properties (e.g., we create a property "doesn't have beautiful feathers" by subtracting from 100\% the responses for "has beautiful feathers").

### Method

#### Participants

We recruited 60 participants over Amazon MTurk.  
3 participants were accidentally allowed to complete the experiment for a second time, so we excluded their second responses (resulting in $n=57$).
2 participants self-reported a native language other than English; removing their data ($n=55$) has no effect on the results reported. 
The experiment took about 10 minutes and participants were compensated \$1.00.

#### Procedure and materials

On each trial of the experiment, participants filled out a table where each row was an animal category and each column was a property. 
Participants first were shown six animal categories randomly sampled from a set corresponding to referent-categories of the generic sentences used in Expt. 1a (e.g., \textsc{robins, mosquitos}) and were asked to generate five of their own (Figure\ \@ref(fig:generics-prior-task)A).
A column then appeared to the right of the animal names with a property label in the column header (e.g., *lays eggs*).
Participants were asked to fill in each row with the percentage of members of each of the species that had the property by giving a number (e.g., "50\%"; Figure\ \@ref(fig:generics-prior-task)B).
Eight property--columns in total appeared in the table.

This whole procedure was repeated two times (two trials).
In total, each participant generated ten animal names and reported on the prevalence of sixteen properties for twenty-two animals (their own ten and the experimentally-supplied twelve). 

```{r generics-prior-task, fig.cap="Prior elicitation task. A: Participants first generated animal names after seeing six example categories. B: One feature at a time, participants estimated the percentage of the category with the feature, for each category.", fig.height = 3, cache = F}
fig.priors.expt.stage1 <- ggdraw() + draw_image(paste(project.path, "paper/figs/generics-prior-stage1.jpeg", sep=""), scale = 1)
fig.priors.expt.stage2 <- ggdraw() + draw_image(paste(project.path, "paper/figs/generics-prior-stage2.jpeg", sep=""), scale = 1)

plot_grid(fig.priors.expt.stage1, fig.priors.expt.stage2, labels = "AUTO")
``` 


### Qualitative results

The elicited prior distributions have a diversity of shapes (8 examples shown in Figure\ \@ref(fig:generic-endorsement-priors-figure)A) that are roughly consistent with the schematic prior distributions used in the *Model Simulations* section (Figure\ \@ref(fig:simulations)A).
The property *being female* is present in almost all categories in almost exactly the same proportion, whereas properties such as *laying eggs* or *having spots* exhibit more structure represented by the multimodality of these distributions. 
*Being red* exists mostly at extremely low prevalence levels (i.e., 0 prevalence) but also at high prevalence levels (e.g., 50\% or 100\%), whereas *carrying malaria* is really only present at low prevalence levels.
This diversity is important because our endorsement model makes different predictions depending on the shape of the distrubution.

### Quantitative modeling

<!-- Accurately modeling the prior data is important to generate precise quantitative predictions about endorsement. -->
In order to integrate the results of this prior measurement task into our model of endorsement, we must build a statistical model of the prior elicitation data.
We approximate the prevalence distribution for each property (e.g., \textsc{lay eggs}) with a Mixture of Betas model, which assumes that the data generated for each kind comes from one of two underlying Beta distributions.^[
  The Beta distribution is chosen because the support of this distribution is numerical values between 0 - 1, exactly the form of the response data in the prior elicitation task.
] 
We specify one of these distributions *a priori* to represent kinds of animals who *do not have* a stable causal mechanism that could give rise to the property (e.g., \textsc{lions} and \textsc{lay eggs}), which results in prevalence or prevalence values close to or equal to 0.^[
  This assumption is similar in spirit to that employed by *Hurdle Models* of epidemiological data, where the observed count of zeros is often substantially greater than one would expect from standard models, such as the Poisson [e.g., when modeling adverse reactions to vaccines; @hurdleModels]
]
This "null distribution" is potentially present for all features in exactly the same way (i.e., the lack of producing the feature).
The second distribution represents kinds of animals who *do have* such a mechanism, and the two parameters of this distribution are not specified *a priori* and are not the same for all properties, but are inferred on a property-wise basis from participants' responses.
The *Mixture of Betas* distribution has a third free parameter (for each property), the relative contribution of the "null distribution" (for example: we expect the "null distribution" to not contribute at all to properties like *being female*, for which almost all categories have at least some members with the property).
To ensure the *Mixture of Betas* model of the prior is not overly complex, we fit an additional model that represents only a single underlying distribution (*Single Beta*) for a comparison.
<!-- is flexible and can represent different shapes for different properties (e.g., the differences in prevalence judgments for \textsc{lay eggs} vs. \text{has wings}). -->
For more details about model implementation and inference, see Appendix C. 

The prior distributions over prevalence are well modeled as a mixture of two Beta distributions and not as a single Beta distribution (Figure\  \@ref(fig:generic-endorsement-priors-figure)B; red vs. blue lines). 
The *Single Beta* model provides a good fit for the *being female* distribution, but overly smooths the other distributions, washing out the latent structure in participants' responses.
One property that the *mixture of Betas* model does not perfectly capture is the prior distribution over the feature *lays eggs*.
The empirical distribution is tri-modal, with reliable modes at 0\%, 50\%, and 100\%; a simple two-component mixture model has no way to account for such a tri-modal distribution.^[
  The third mode at 100\% is not attributable to categories for which all members could be female (e.g., chickens). Instead, it appears that some participants are responding that 100\% of several different kinds of birds (e.g., robins) lay eggs. This may result from participants implicitly only considering female members of the category as relevant to answer a question about a reproductive capactiy like *lays eggs*; this restriction of what enters into the prevalence computation is known as *domain restriction*, is posited in several theories of generics [e.g., @Cohen1999], and has been observed in other prevalence elicitation tasks [e.g., @Prasada2013].
]
A more complex model (i.e., one with three mixture components) would be necessary to perfectly account for this item. 
Using a three-component model for this distribution does not change the resulting model predictions and we maintain the simpler two-component mixture for uniformity.

```{r generic-endorsement-priors-figure, fig.width = 7, fig.asp = 0.9, out.width = "\\textwidth", fig.pos = "!h", fig.cap="A: Empirically elicited prior distributions over prevalence for eight properties. B: Cumulative density plots reveal that a model of a mixture of two Beta distributions does substantially better at capturing the structure of the priors than a single Beta distribution. Distributions are the posterior predictive distributions for the models of the prior and the raw empirical distribution. A completely uniform distribution would be represented as the y = x line.", cache = F}
load('../analysis/generics-priors-posteriorPredictives.Rdata')

gen.priors.densities <- ggplot(m.gen.endorse.priors.posteriorPredictive %>%
         filter(src == 'Data'), 
           aes( x = prevalence))+
           #aes( x = prevalence, color = src, lty = src))+
    geom_density(aes(y = ..scaled..), adjust = 0.5, size = 1)+
    #scale_fill_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    #stat_ecdf(size = 1)+
    #scale_color_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    #scale_alpha_manual(values = c(0.6, 0.4, 0))+
    facet_wrap(~Property, nrow = 2)+
    #scale_linetype_manual(values = c(1, 4, 3))+
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    #coord_fixed()+
    xlab("") +
    ylab("Normalized probability density")+
    theme(legend.position = "bottom", legend.title = element_blank())


gen.priors.cdfs <- m.gen.endorse.priors.posteriorPredictive %>%
  mutate(src = factor(src, levels = c("Data", "Structured", "Unstructured"),
                      labels = c("Empirical distribution", 
                                 "Mixture of Betas",
                                 "Single Beta"))) %>%
ggplot(.,
           aes( x = prevalence, color = src, lty = src))+
    #geom_density(aes(y = ..scaled..), adjust = 1.3, size = 1)+
    #scale_fill_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    stat_ecdf(size = 1.3)+
    scale_color_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    scale_alpha_manual(values = c(0.6, 0.4, 0))+
    facet_wrap(~Property, nrow = 2)+
    scale_linetype_manual(values = c(1, 2, 3))+
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    #coord_fixed()+
    xlab("prevalence") +
    ylab("Cumulative probability density")+
    theme(legend.title = element_blank(), legend.position = 'bottom')



gen.prior.fig1 <- gen.priors.densities +
  theme(plot.margin = unit(c(10,0,0,0), "pt")) 
gen.prior.fig2 <- gen.priors.cdfs  +
  theme(plot.margin = unit(c(10,0,6,0), "pt")) 

plot_grid( 
 gen.prior.fig1, gen.prior.fig2,
           align = 'vh',
           labels = c("A", "B"),
           hjust = -1,
           nrow = 2,
           rel_heights = c(1, 1.2)
           #rel_widths = c(1.2, 3, 2)
           #rel_widths = c(1, 1.3)
           )
```



## Endorsement model comparison

We can now compute the predictions of our endorsement model; we first articulate a set of alternatives: models that have been previously proposed in the literature, an alternative form of our proposed endorsement model, and our full endorsement model.

```{r}
load("cached_results/case1_regressionResults.RData") # d.gen.endorse.regression.prevalence, d.gen.endorse.regression.prevalence.cuevalidity, d.gen.endorse.regression,
r2.gen.n <- length(d.gen.endorse.bayes.prev$sentence)
r2.gen.regression.prev <- round(with(d.gen.endorse.regression.prevalence, cor(MAP_h, prediction_mean))^2,2)
mse.gen.regression.prev <-  round(mean(d.gen.endorse.regression.prevalence$sqErr), 3)

r2.gen.regression.prev.cv <- round(with(d.gen.endorse.regression.prevalence.cuevalidity, cor(MAP_h, prediction_mean))^2, 3)
mse.gen.regression.prev.cv <- round(mean(d.gen.endorse.regression.prevalence.cuevalidity$sqErr), 3)

intermediate.prev.quantiles <-  quantile(d.gen.endorse.regression.prevalence$prev_mean, c(0.25, 0.76))


d.gen.endorse.regression.prevalence.intermediateprev <- d.gen.endorse.regression.prevalence %>%
  filter(
    (prev_mean > intermediate.prev.quantiles[["25%"]]) &
    (prev_mean < intermediate.prev.quantiles[["76%"]])
)
r2.gen.n.intermedprev <- length(d.gen.endorse.regression.prevalence.intermediateprev$sentence)

r2.gen.regression.prev.intermedprev <- round(with(d.gen.endorse.regression.prevalence.intermediateprev, cor(MAP_h, prediction_mean))^2,2)
mse.gen.regression.prev.intermedprev <-  round(mean(d.gen.endorse.regression.prevalence.intermediateprev$sqErr), 3)
```

### Baseline models

We present two baseline, quantitative models that have previously been used in the empirical literature on generic language.
In addition to serving as alternatives, these regressions model help us understand the statistical properties of our thirty items. 
First, we estimate how well referent prevalence itself predicts generic endorsement (e.g., does the fraction of robins that lay eggs predict the felicity of *Robins lay eggs*?).
Second, we include *cue validity*---the probability of a kind given the feature---as a second predictor in a linear model.
We fit these models using standard maximum-likelihood techniques and model uncertainty in the input measurements (i.e., referent prevalence, cue validity) by bootstrapping those data.

#### Referent prevalence

From the prevalence prior data (Expt. 1b), we estimate participants' beliefs about the referent prevalence (e.g., the percentage of \textsc{robins} that \textsc{lay eggs}) and use it to predict endorsement.
We find a little over half of the variance in the endorsement data is explained this way ($r^2(`r r2.gen.n`) = `r r2.gen.regression.prev`$; MSE=$`r mse.gen.regression.prev`$; Figure \ \@ref(fig:generic-endorsement-priors-figure), upper-right: lower-left facet). 
Referent prevalence alone does okay at predicting these data because our stimulus set includes generics that are true with high prevalence properties (e.g., "Leopards have spots.") and false with low prevalence properties (e.g., "Leopards have wings."). 

Large deviations from an account based purely on referent prevalence remain: Generics in which the referent-category has intermediate prevalence (prevalence quartiles 2 and 3: $`r round(100*intermediate.prev.quantiles[["25%"]])`\% < \text{prevalence} < `r  round(100*intermediate.prev.quantiles[["76%"]])`\%$), are not at all explained by referent prevalence ($r_{Q2,3}^2(`r r2.gen.n.intermedprev`) = `r r2.gen.regression.prev.intermedprev`$; MSE = $`r mse.gen.regression.prev.intermedprev`$).
This includes generics that are judged true with relatively low referent prevalence (e.g., "Mosquitos carry malaria") and false with relatively high referent prevalence (e.g., "Sharks don't eat people").

#### Cue validity and referent prevalence

<!-- ##### Model specification -->

Cue validity indexes the diagnosticity of the feature for a kind, given formally by the probability of a kind given the feature $P(k \mid f)$.
As discussed in the Computational Model section above, cue validity is linearly related to expected value of the prevalence prior distribution (see Appendix A for derivation).
Cue validity thus is similar to a *point estimate* of the prevalence prior distribution, a single metric that collapses across the full distribution. 
<!-- Comparison to a model based on cue validity thus  -->

<!-- To show that the full distribution over prevalence is important, we explore a secondary baseline model based on prevalence and cue validity. -->
Though the cue validity of a property for a category can be derived from the prevalence prior distribution, previous empirical studies have estimated cue validity from different empirical sources [@Cree2006; @Khemlani2012].
In the empirical literature on generics, researchers often ask directly about the cue validity probability [e.g., "There is an animal that lays eggs. What is the probability that it is a robin?"; @Khemlani2012], though in the broader literature on semantic memory a *free production* paradigm is often employed [e.g., "X lays eggs. What do you think X is?"; @Cree2006]. 
We found that these two ways of estimating cue validity diverge for a number of key cases (Appendix B).
Most notably, undiagnostic features (e.g., *is female*), which in theory have a cue validity close to zero, were rated as having intermediate cue validity in the *direct question* paradigm. 
In response to a direct question such as "There is an animal that is female. What is probability that it is a robin?", participants could be responding "I don't know" rather than reporting their intuitive base rate that a random animal would be a robin. 
The free production paradigm did not produce artifacts such as this one, and we chose to use it as the more veridical estimate of cue validity. 
For a detailed analysis of the different cue validity measurements and comparison to cue validity derived from the prevalence prior, see Appendix B.

<!-- It thus implicitly captures information about other categories, though cannot capture the structure that is present in the prevalence priors. -->

<!-- Whereas prevalence encodes a prediction about whether or not an entity $x$ of a certain kind $k$ will have the property $f$, $P(x \in f \mid x \in k)$, *cue validity* represents the inverse prediction: that an entity with a certain property $f$ is a member of kind $k$: $P(x \in k \mid x \in f)$ (e.g., one's predictions about whether or not the entity is a robin, upon learning that it lays eggs). -->
<!-- Previous studies have found cue validity to be correlated with generic endorsement [@Khemlani2012]. -->
<!-- For example, the truth of "Mosquitos carry malaria" can be explained by appealing to the fact that *only mosquitos* carry malaria, and htus if one learns that an entity carries malaria, that entity is probably a mosquito (high cue validity). -->
<!-- Of course, cue validity is not a necessary feature of a true generic statament: "Dogs have four legs" is true despite the feature of having four legs being utterly undiagnostic for dogs. -->

<!-- Cue validity is not unrelated to the prevalence prior.  -->
<!-- The prevalence prior $P(p_{fk})$ is a probability distribution, which can decomposed into a distribution on kinds $P(k)$ and the likelihood of a prevalence given the kind $P(h \mid k)$. -->
<!-- For simplicity, assume we have perfect knowledge of the prevalence for each kind: $P(h \mid k) = P(f \mid k)$. -->
<!-- Then if the prevalence prior is composed of a finite number of kinds, the cue validity of that feature for a particular kind is given by Bayes' Rule: -->
<!-- $$ P(k \mid f) = \frac{P(f \mid k) \cdot P(k)}{\sum_{k' \in K} P(f \mid k') \cdot P(k')}  = \frac{P(f \mid k) \cdot P(k)}{\mathbb{E}_{k' \sim P(k')}P(h \mid k')} $$ -->
<!-- The normalizing constant in this equation is the expected value of the prevalence prior distribution.  -->
<!-- though its role in generic meaning is controversial [@Leslie2007; @Leslie2008]. -->

<!-- If prevalence is the "forward probability" (e.g., one's predictions about whether or not an entity will lay eggs after learning that the entity is a robin), *cue validity* is the inverse probability:  -->
<!-- *Cue validity* thus captures the distinctiveness of the feature for the category.  -->
<!-- Empirically, cue validity is not a *necessary* criteron for generic endorsement (e.g., in the sentence:  -->
<!-- In addition, if generics have to do with generalizations, which are formally described by the "forward probability", it is not obvious why the "inverse probability" (cue validity) would be part of the semantic content of generics. -->



<!-- ##### Results -->

```{r cue.validity.examples}
generics.cuevalidity.examples <- c(
  "Mosquitos carry malaria.", "Lions have manes.",
  "Robins lay eggs.", 
  "Mosquitos dont carry malaria."
  )

reg.mosquitos.carry <- d.gen.endorse.regression %>% filter(src == "regression_Prev_Cuevalidity", sentence == "Mosquitos carry malaria.")
reg.robins.lay.eggs <- d.gen.endorse.regression %>% filter(src == "regression_Prev_Cuevalidity", sentence =="Robins lay eggs.")
reg.lions.manes <- d.gen.endorse.regression %>% filter(src == "regression_Prev_Cuevalidity", sentence == "Lions have manes.")
reg.mosquitos.dont.carry <- d.gen.endorse.regression %>% filter(src == "regression_Prev_Cuevalidity", sentence == "Mosquitos dont carry malaria.")
```

A linear model that uses predictors for both referent-prevalence and cue validity does a better job at explaining the endorsement data than just prevalence alone ($r^2(`r r2.gen.n`) = `r r2.gen.regression.prev.cv`$; MSE=$`r mse.gen.regression.prev.cv`$).
This model is able to account for the endorsements of examples like "Mosquitos carry malaria" (model endorsement and bootstrapped-95\% confidence interval = `r round(reg.mosquitos.carry[[1,"prediction_mean"]], 2)` [`r round(reg.mosquitos.carry[[1,"prediction_ci_lower"]], 2)`,
`r round(reg.mosquitos.carry[[1,"prediction_ci_upper"]], 2)`]) and "Lions have manes" (`r round(reg.lions.manes[[1,"prediction_mean"]], 2)` [`r round(reg.lions.manes[[1,"prediction_ci_lower"]], 2)`,
`r round(reg.lions.manes[[1,"prediction_ci_upper"]], 2)`])), as these features are very diagnostic of the kind (human endorsement both $> 0.9$).
Deviations, however, still remain.
For example, "Robins lay eggs" still receives only intermediate endorsement by this model (`r round(reg.robins.lay.eggs[[1,"prediction_mean"]], 2)` [`r round(reg.robins.lay.eggs[[1,"prediction_ci_lower"]], 2)`,
`r round(reg.robins.lay.eggs[[1,"prediction_ci_upper"]], 2)`]; human endorsement = `r round(reg.robins.lay.eggs[[1,"MAP_h"]], 2)` [`r round(reg.robins.lay.eggs[[1,"low"]], 2)`,
`r round(reg.robins.lay.eggs[[1,"high"]], 2)`]), and "Mosquitos don't carry malaria" is misjudged to be a pretty good statement (`r round(reg.mosquitos.dont.carry[[1,"prediction_mean"]], 2)` [`r round(reg.mosquitos.dont.carry[[1,"prediction_ci_lower"]], 2)`,
`r round(reg.mosquitos.dont.carry[[1,"prediction_ci_upper"]], 2)`]; human endorsement = `r round(reg.mosquitos.dont.carry[[1,"MAP_h"]], 2)` [`r round(reg.mosquitos.dont.carry[[1,"low"]], 2)`,
`r round(reg.mosquitos.dont.carry[[1,"high"]], 2)`]).

"Robins lay eggs" and "Mosquitos don't carry malaria" highlight a shortcoming of reducing structured prevalence prior distributions to single point-estimates of cue validity.
*Lays eggs* is a somewhat diagnostic feature for *birds*, but there are many kinds of birds, and the feature is not itself diagnostic for a particular kind of bird like *robins*.
Thus, the cue validity is low even though robins are in the distinctive part of the "lays eggs" prevalence prior distribution (Figure\ \@ref(fig:simulations)A bottom).
Furthermore, cue validity glosses over the difference between "undiagnostic" features (features present in almost every category; e.g., "not carrying malaria") and "false" features (features that are absent a particular category; e.g., "lions" and "lay eggs"; see Appendix B for more discussion of this distinction).
Such a model makes the wrong prediction for non-distinctive properties with high referent prevalence (e.g., "Mosquitos don't carry malaria").
A simple metric like cue validity is too blunt to capture these subtleties.

### Communicative endorsement models

Our underspecified-threshold model considers how well the generalization would bring a naive interpreter's prior distribution on prevalence $P(p)$ (Eq. \ref{eq:L0}; e.g., the prevalence of laying eggs among other animals) in line with the referent prevalence ($p$ in Eq. \ref{eq:S1}; e.g, the prevalence of laying eggs among robins).
There are several substantive components to this hypothesis: (a) context in the form of a prior distribution over prevalence $P(p)$, (b) endorsement as a decision-theoretic process of uttering the generalization vs. not uttering it, and (c) vagueness in the semantics of a generalization.
We construct an alternative, lesioned model by removing the vagueness in meaning, assigning a fixed semantics to the generalization (i.e., analogous to the quantified statements) but keeping the prior and the decision-theoretic architecture in place. 
There are no correspondingly simple ways to lesion the other two components (context or speaker decision) while still producing a model that makes quantitative, context-sensitive predictions. 


<!-- The model we propose for endorsing generalizations is a model of an agent that decides whether or not to say the generalization to a naive interpreter. -->
For each of these communicative endorsement models, we build a joint Bayesian data analysis model of the referent prevalence $p$, prevalence priors $P(p)$ (both from Expt. 1b data), and the endorsement data (Expt. 1a). 
Empirically elicited referent prevalence and prevalence prior data (Expt. 1b) directly constrain the parameters that generate those quantities in the model ($p$ in Eq. \ref{eq:S1} and $P(p)$ in Eq. \ref{eq:L0}, respectively). 
The prevalence prior $P(p)$ is modeled as a mixture of Betas while referent prevalence is modeled by a single Beta distribution.
The endorsement data (Expt. 1a) is modeled by our endorsement model (Eq. \ref{eq:S1}), which has one free parameter $\lambda$.
Predicting the data from both Expt. 1a and 1b by a single, joint-inference model simultaneously makes explicit our assumptions about how these data were generated and is the proper way to represent the uncertainty in our measurement of the prior elicitation data (see Appendix C and Figure\ \@ref(fig:genericsModelDiagram) for further model specification details).
To learn about the credible values of the parameters of the joint-inference model and resulting model predictions, we ran an incrementalized version of MCMC [@Ritchie2016] for 3 chains of 150,000 iterations, discarding the first 50,000 for burn-in.

<!-- The number of parameters of this full data-analytic model is relatively large, and thus there is a concern that the parameters themselves (as opposed to the exact structure of our model) are doing the work.  -->
<!-- To address this concern, we construct a strong, alternative model that uses the same data analytic strategy and only differs in the uncertain semantic threshold $\theta$ is replaced with a fixed-threshold. -->

#### Lesioned model (no vagueness)

For a strong alternative model, we lesion our uncertain threshold model so that it has a fixed threshold $\theta$.
We make this the strongest, possible fixed-threshold model by searching for the best possible single, threshold that fits the data.
Finally, a fixed-threshold model will have to accommodate responses that are literally inconsistent with the threshold (i.e., a participant endorsing a generic when the referent prevalence is less than $\theta$); we thus outfit this model with an additional extrinsic noise parameter, to allow for random guessing. 
Thus, our fixed-threshold alternative model says that participants make an information-theoretic decision, taking into account the interpreter's prior distribution $P(p)$, using a fixed-threshold semantics, where deviations from a pure information-theoretic decision are accounted for by noise.
This alternative model has 2 additional parameters to our uncertain threshold model (the fixed-threshold and the proportion of noise responses).
<!-- We examine the predictions of this alternative model first. -->

```{r generic-model-scatters, cache = F}
load("cached_results/case1_modelPosteriors.RData")
#d.gen.endorse.rsa.regression, gen.inset.distributions.refactored, generics.fixed.noise.posterior, fixed.threshold.posterior,fixed.threshold.s1opt,uncertain.threshold.s1opt 

r2.gen.rsa.generics <- compute_r2(
  d.gen.endorse.rsa.regression %>% filter(src == "Uncertain semantics model"), 
  "prediction_mean", "MAP_h", sigfigs = 4
)
  
r2.gen.rsa.fixed <- compute_r2(
  d.gen.endorse.rsa.regression %>% filter(src == "Fixed semantics model"),  
  "prediction_mean", "MAP_h", sigfigs = 4
)

mse.gen.rsa.generics <- compute_mse(
  d.gen.endorse.rsa.regression %>% filter(src == "Uncertain semantics model"), 
  "prediction_mean", "MAP_h", sigfigs = 5
)

mse.gen.rsa.fixed <- compute_mse(
  d.gen.endorse.rsa.regression %>% filter(src == "Fixed semantics model"),  
  "prediction_mean", "MAP_h", sigfigs = 5
)

generics.endorsement.models <- d.gen.endorse.rsa.regression %>%
  mutate(src = factor(src,
                       levels = c("Prevalence",
                                 "Uncertain semantics model",
                                 "Cue validity + Prevalence",
                                 "Fixed semantics model"
                                 ),
                       labels = c("Prevalence",
                                 "Generic Endorsement model",
                                 "Cue validity + Prevalence",
                                  "Lesioned endorsement model\n(no vagueness)"
                                 )
                      )) %>%
  ggplot(., 
         aes ( x = prediction_mean, xmin = prediction_ci_lower, xmax = prediction_ci_upper,
                  y = MAP_h, ymin = low, ymax = high, fill = prev_mean ))+
  geom_abline(intercept = 0, slope = 1, lty = 3)+
  geom_linerange(alpha = 0.4)+
  geom_errorbarh(alpha = 0.4)+
  geom_point(shape = 21, size = 3)+
  scale_x_continuous(limits = c(-0.01, 1.01), breaks = c(0,  1))+
  scale_y_continuous(limits = c(-0.01, 1.01), breaks = c(0, 1))+
  scale_fill_viridis()+
  coord_fixed()+
  xlab("Model prediction")+
  ylab("Human generic endorsement")+
  facet_wrap(~src, nrow = 2)+
  guides(fill = F)
```

```{r generic-model-insets, fig.width = 4.75, fig.height = 1.5}


category.text.labels <- data.frame(property = c("dont eat people", 
                      "carry malaria", "lay eggs",  "are female", "have spots"),
             category = c("Tigers", "Mosquitos", "Robins", "Robins", "Leopards"),
             x = c(0.3, 0.3, 0.47, 0.05, 0.6),
            y = c(0.45, 0.5, 0.45, 0.5, 0.26))


generics.endorsement.insets <- ggplot(gen.inset.distributions.refactored, 
                                      aes( x = value, fill = Parameter, 
                 color = Parameter, lty = Parameter, alpha = Parameter ))+
  geom_density(aes(y = ..scaled..), adjust = 4, size = 1)+
  facet_wrap(~property, nrow = 1)+
  geom_label_repel(data = category.text.labels,
                  aes(label = category, x = x , y = y),
                  inherit.aes = F, color = "#2b83ba")+
    #scale_fill_manual(values = c("#636363", "#abdda4", "#2b83ba", "#d7191c"))+
    #scale_color_manual(values = c("#636363", "#abdda4", "#2b83ba", "#d7191c"))+
    scale_fill_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    scale_color_manual(values = c("#636363", "#d7191c", "#2b83ba"))+
    scale_alpha_manual(values = c(0.6, 0.4, 0))+
    #scale_linetype_manual(values = c(3, 4, 2, 1))+
    scale_linetype_manual(values = c(3, 4, 1))+
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    xlab("Prevalence") +
    ylab("Scaled probability density")+
    theme(legend.position = "bottom", legend.title = element_blank())
```

```{r generics-endorsement-figure, fig.width = 11, fig.asp = 0.8, out.width = "\\textwidth", fig.pos = "!h", fig.cap="Endorsing generalizations about categories. A: Human elicited endorsements for thirty generic sentences reveal a continuum of endorsements. B: Model fits for the uncertain semantics speaker model (upper right), a fixed semantics speaker model (upper left), and regression models based on referent prevalence alone (lower left) and prevalence + cue validity (lower right). C: Five example prevalence priors, listener posteriors upon hearing the generalization, and speaker belief distributions about the referent prevalence. These distributions are inferred using all three data sources from Expt. 1 (see Figure \\ref{fig:genericsModelDiagram} for an overview of the Bayesian data analytic approach).", cache = F}

fig.gen.endorse.data <- generics.endorsement.spectrum +
  theme(plot.margin = unit(c(6,2,0,0), "pt")) #+ ggtitle("prevalence priors")
fig.gen.endorse.models <- generics.endorsement.models +
  theme(plot.margin = unit(c(6,2,0,0), "pt")) #+ ylab("")
fig.gen.endorse.priors <- generics.endorsement.insets +
  theme(plot.margin = unit(c(26,10,0,10), "pt"))

  
fig.gen.endorse.toprow <- plot_grid(fig.gen.endorse.data,
                                     fig.gen.endorse.models, 
                                    # align = 'v',
                                    #  axis = 'l',
                                    labels = c("A", "B"),
                                    rel_widths = c(0.9, 1))



plot_grid(fig.gen.endorse.toprow,
          fig.gen.endorse.priors,
          labels = c("", "C"), nrow = 2, rel_heights = c(1.1, 0.6)
          )
```

To evaluate the fixed-threshold model, we examine model predictions as well as the posterior distribution over latent parameters of the model (referent prevalence, prevalence priors, the optimality, fixed-threshold, and noise parameters) given the observed data.
The Maximum A-Posteriori value and 95\% highest probability density interval for the inferred (fixed) threshold and noise parameters are `r round(fixed.threshold.posterior[[1,"MAP"]], 2)` [`r round(fixed.threshold.posterior[[1,"cred_lower"]], 2)`,
`r round(fixed.threshold.posterior[[1,"cred_upper"]], 2)`] and `r round(generics.fixed.noise.posterior[[1,"MAP"]], 2)` [`r round(generics.fixed.noise.posterior[[1,"cred_lower"]], 2)`,
`r round(generics.fixed.noise.posterior[[1,"cred_upper"]], 2)`], respectively.
The inferred optimality parameter in Eq. \ref{eq:S1} is `r round(fixed.threshold.s1opt[[1,"MAP"]], 2)` [`r round(fixed.threshold.s1opt[[1,"cred_lower"]], 2)`, `r round(fixed.threshold.s1opt[[1,"cred_upper"]], 2)`].

```{r fixed-threshold-examples}
fixed.threshold.generics.examples <- c("Tigers eat people.", "Peacocks have beautiful feathers.", "Lions lay eggs.", "Mosquitos attack swimmers.")
fixed.threshold.model.examples <- d.gen.endorse.rsa.regression %>% 
  filter(src == "Fixed semantics model", sentence %in% fixed.threshold.generics.examples)

tigers.eat.people <- filter(fixed.threshold.model.examples, sentence == "Tigers eat people.")
peacocks.feathers <- filter(fixed.threshold.model.examples, sentence == "Peacocks have beautiful feathers.")
lions.lay.eggs <- filter(fixed.threshold.model.examples, sentence == "Lions lay eggs.")
mosquitos.attack.swimmers <- filter(fixed.threshold.model.examples, sentence == "Mosquitos attack swimmers.")
```

We compare the fixed-threshold model's posterior predictive distribution of generic endorsement to the empirical truth judgments.
Figure\ \@ref(fig:generics-endorsement-figure)B (top left subplot) shows the fixed-threshold model's ability to predict the generic endorsement data ($r^2(`r r2.gen.n`) = `r r2.gen.rsa.fixed`$; MSE = $`r mse.gen.rsa.fixed`$).
Though the model is able to capture a lot of the variance, it only makes three kinds of judgments: true, false, or neither. 
It treats "Tigers eat people" (`r round(tigers.eat.people[[1,"prediction_mean"]],2)` [`r round(tigers.eat.people[[1,"prediction_ci_lower"]], 2)`,
`r round(tigers.eat.people[[1,"prediction_ci_upper"]], 2)`]) as good a statement as "Peacocks have beautiful feathers" (`r round(peacocks.feathers[[1,"prediction_mean"]],2)` [`r round(peacocks.feathers[[1,"prediction_ci_lower"]], 2)`,
`r round(peacocks.feathers[[1,"prediction_ci_upper"]], 2)`]), though participants seem to give a substantially weaker endorsement of the former ("Tigers eat people" = `r round(tigers.eat.people[[1,"MAP_h"]],2)` [`r round(tigers.eat.people[[1,"low"]], 2)`,
`r round(tigers.eat.people[[1,"high"]], 2)`]; "Peacocks have beautiful feathers" = `r round(peacocks.feathers[[1,"MAP_h"]],2)` [`r round(peacocks.feathers[[1,"low"]], 2)`,
`r round(peacocks.feathers[[1,"high"]], 2)`]).
Similarly, "Lions lay eggs" is judged to be just as bad as "Mosquitos attack swimmers"(`r round(lions.lay.eggs[[1,"prediction_mean"]],2)` [`r round(lions.lay.eggs[[1,"prediction_ci_lower"]], 2)`,
`r round(lions.lay.eggs[[1,"prediction_ci_upper"]], 2)`, `r round(mosquitos.attack.swimmers[[1,"prediction_mean"]],2)` [`r round(mosquitos.attack.swimmers[[1,"prediction_ci_lower"]], 2)`,
`r round(mosquitos.attack.swimmers[[1,"prediction_ci_upper"]], 2)`], respectively), though participants rate the former as completely false (`r round(lions.lay.eggs[[1,"MAP_h"]],2)` [`r round(lions.lay.eggs[[1,"low"]], 2)`,
`r round(lions.lay.eggs[[1,"high"]], 2)`]) while the latter is kind of true (`r round(mosquitos.attack.swimmers[[1,"MAP_h"]],2)` [`r round(mosquitos.attack.swimmers[[1,"low"]], 2)`,
`r round(mosquitos.attack.swimmers[[1,"high"]], 2)`]).
This alternative model is unable to make these fine-grained distinctions because it must use the same semantic threshold in all contexts. 

<!-- The model fails to distinguish any of the generic sentences because they are all somewhat true.  -->
<!-- All items get endorsed by at least some participants; the model then infers that the referent prevalence $h'$ must be greater than 0\%; but, if the referent prevalence is greater than 0\%, the statement is 100\% true given the fixed-threshold semantics. -->

<!-- The endorsement model decides if it better to assert the 0\%-threshold statement or stay silent.  -->
<!-- For some items, a fixed-threshold meaning carries some information-gain over silence (i.e., some model predictions \> 0.5).  -->
<!-- For many items, however, the 0\%-threshold is almost as meaning-less as silence; the model, thus, has a hard time strongly endorsing statements (most model predictions around 0.5\%). -->
<!-- Consistent with *a priori* intuition, assigning a fixed-semantics is too lenient a formalization for generic sentences.  -->
<!-- This demonstrates that the Bayesian data analytic approach used to fit the parameters of our model is not itself causally sufficient for any model of endorsements to explain the human endorsement data. -->
<!-- The fixed-threshold model, even with access to the full prior distribution $P(p_{fk})$, falls short in explaining the data. -->

#### Uncertain threshold model

Our underspecified threshold model is the same as the fixed-threshold model, except that rather than having a fixed $\theta$ for all contexts, the model infers context-specific $\theta$'s.
We use the same Bayesian data analysis approach; however, we are able to drop the additional parameters required for the fixed-threshold model (the fixed-threshold and noise parameters).
Thus, this model has two fewer parameters than the fixed-threshold model above.


```{r distortionEffect}
load("cached_results/case1_prevParamDistortion.RData") #m.gen.endorse.priors.parameters, m.gen.endorse.refprev.parameters

r2.prevprior.distortion <- compute_r2(
  m.gen.endorse.priors.parameters, 
  "MAP", "MAP_joint", sigfigs = 4
)

r2.refprev.distortion <- compute_r2(
  m.gen.endorse.refprev.parameters, 
  "MAP", "MAP_joint", sigfigs = 4
)
```


We first examine the posterior predictive distribution on the prevalence prior and referent prevalence data to ensure that the joint-inference model does not distort these parameters at the service of predicting the endorsement data (e.g., such a distortion could manifest by the joint-inference model inferring that 100% of mosquitos carry malaria in order to predict that "Mosquitos carry malaria" is a good utterance).
This is an important step in model validation because it tells us that our model's predictions are derived from reasonable values of the parameters (e.g., that not all mosquitos carry malaria).
Importantly, the model captures the prior elicitation data (e.g., the probability of carry malaria among various species) and the referent prevalence data (e.g., the prevalence of carrying malaria among mosquitos) as well as it did when the Bayesian data analysis model only included these data and their associated model ($r_{\text{prevalence prior parameters}}^2(`r length(m.gen.endorse.refprev.parameters[,"Parameter"])`) = `r r2.refprev.distortion`$; $r_{\text{referent prevalence parameters}}^2(`r length(m.gen.endorse.priors.parameters[,"Parameter"])`) = `r r2.prevprior.distortion`$; see Appendix D, Figure 14).
This result confirms that the theoretically-interesting predictions of this model --- predictions of generic endorsement --- are based on intuitively meaningful model components (i.e., the shapes of the prevalence distributions). 
Finally, the inferred optimality parameter in the endorsement model (Eq. \ref{eq:S1}) is `r round(uncertain.threshold.s1opt[[1,"MAP"]], 2)` [`r round(uncertain.threshold.s1opt[[1,"cred_lower"]], 2)`, `r round(uncertain.threshold.s1opt[[1,"cred_upper"]], 2)`], a range consistent with the literature on similar models.

As we see in Figure\ \@ref(fig:generics-endorsement-figure)B (top right subplot), the endorsement model that uses an uncertain-threshold semantics explains nearly all of the variance in human endorsements ($r^2(`r r2.gen.n`) = `r r2.gen.rsa.generics`$; MSE = $`r mse.gen.rsa.generics`$).
To gain some intuition for why the model makes the predictions that it does, we can examine the relevant model components that give rise to these predictions (Figure \ref{fig:generics-endorsement-figure}C).
Recall that the prevalence prior (grey distribution) is also the interpreter model's posterior upon hearing the null, or silent, utterance, and that the endorsement model decides whether to produce the generic utterance or remain silent given some referent prevalence (blue distribution). 
The interpreter model's prevalence posterior upon hearing the generic utterance (red distribution) differs depending on the property being predicated (facets). 

The endorsement model decides if the generic utterance conveys information that would bring the interpreter's distribution more in line with the endorsement model's referent prevalence. 
The model rates "Robins lay eggs" as a good utterance because the prevalence posterior implied by the generalization is similar to the referent prevalence.^[
  We again note that modeling the prevalence prior for *lays eggs* as a two-component mixture model, as we have assumed for all the prevalence priors, results in a distribution with a peak at 0\% and around 70\%. 
  This 70\% peak is a result of smoothing the underlying bimodal distribution at 50\% and 100\%. 
  The same smoothing occurs for the referent-prevalence, as participants report as often that either 50\% or 100\% of robins lay eggs.
  If instead, the prevalence prior is modeled as a tri-modal distribution and the referent-prevalence modeled as bi-modal (mainintaing the structure at 50\% and 100\%), the resulting interpretation (i.e., prevalance posterior given generalization) is that either 50\% or 100\% lay eggs. 
  This is precisely the bimodal referent-prevalence distribution for *robins lay eggs*, and so the endorsement model still predicts high endorsement. 
  The fact that domain restriction enters in both the prevalence prior and the referent prevalence measurements, in effect, cancels one another out and has no effect on the resulting endorsement model predictions.
]
The prevalence posterior for "Robins are female" is almost indistinguishable from the prevalence prior, because the prevalence prior has such low variance (almost all animals have female members in exactly the same proportion); the endorsement model then has no basis to prefer silence or the generic statement and the prediction is that the utterance should be neither good nor bad, endorsement around 0.5.
"Mosquitos carry malaria" is an interesting case because the prevalence prior has high variance (i.e., participants are highly uncertain about the prevalence of carrying malaria among categories).
As a result, the prevalence posterior also has high variance; still, the prevalence posterior is more consistent with the referent prevalence than the prevalence prior, and the endorsement model predicts "Mosquitos carry malaria" is a good utterance.
Finally, an utterance with high referent prevalence, such as "Tigers don't eat people", is predicted to have low endorsement because the generic would be misleading; even if most tigers don't eat people, saying "Tigers don't eat people" implies that all don't eat people, which is too strong.

<!-- The grey distribution shows the interpreter model's prevalence prior (these are the corresponding Probability Density Functions of the Cumulative Density Functions shown in Figure\ \@ref(fig:generic-endorsement-priors-figure)). -->
<!-- The prevalence prior is also the interpreter's prevalence posterior upon hearing the silent utterance $Int(p \mid u = null)$. -->



## Discussion

Generic language is the premier case study for generalizations in language. 
Generics have been studied extensively in the cognitive and developmental psychological literatures and have deep implications for wide ranging phenomena from stereotype propagation [@Rhodes2012] to motivation [@Cimpian2007].
Heretofore, no formal model has been able to pass the most basic empirical test for generics: knowing when they are true and false.
<!-- Determining the truth conditions is the first step towards articulating a formal theory of how the language updates beliefs. -->
This empirical case study demonstrates that a semantics based on the prevalence of the feature is tenable despite of alleged counterexamples (e.g., "Robins lay eggs" vs. "Robins are female").
The key theoretical insight is that the truth-functional semantics is underspecified, or vague, and resolved in context by a process of probabilistic inference.
Our model provides a clear delineation of world knowledge (formalized as a prevalence prior) from the semantics of generics. 
We return to this point, and its implications for theory-building, in the general discussion. 

<!-- The decision of whether or not to endorse the generic comes down to whether or not the generic would make the interpretation belief distribution more in line with the prevalence that the endorser had in mind. -->

In explaining the variable endorsements of generics, we related the referent prevalence (e.g., the percentage of robins that lay eggs) and the prevalence prior (e.g., the prevalence of laying eggs for different kinds of animals)  to the endorsement of the generalization (e.g., "Robins lay eggs") via an information-theoretic communicative model where the meaning of a generic is simple but underspecified.
In this case study, we used generic statements about familiar animal categories, which has long been the cleanest domain for testing semantic theories of generics by providing minimal comparison like "Robins lay eggs" vs. "Robins are female". 
However, modeling familiar category generics is a correlational analysis: The relevant quantities in the model were measured rather than manipulated.
We now seek to demonstrate how these quantities are causally related to endorsement, by manipulating referent prevalence (Case Study 2) and prevalence priors (Case Study 3).
In addition, we take this opportunity to highlight the generality of the theory, by performing these additional empirical tests in different domains for generalization: events and causes. 

<!-- With a tenable quantitative theory in hand,  -->

<!-- In our second case study, we provide a stricter test of the quantitative precision of our theory and  -->


<!-- the probabilities of features across categories in addition to the probability of the feature in the referent-category (e.g., the percentage of robins that lay eggs) to model the endorsement data. -->
<!-- Thus, we have shown how these measurements are related to generic endorsement via our model.  -->
<!-- In our second case study, we extend the theory to the domain of events and provide the first evidence that the factors identified by our theory are causally related to endorsement.  -->
<!-- This evidence is correlational in nature, however. -->
<!-- We do not yet know if the variables that we are measuring are causally related to endorsement. -->

