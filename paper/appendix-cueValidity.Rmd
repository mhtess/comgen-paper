```{r echo = F}
library(langcog)
library(tidyverse)
library(ggthemes)
library(jsonlite)
theme_set(theme_few())
project.path <- "../"
data.path <- "data/generics/endorsement/"
substrRight <- function(x, n){
  substr(x, nchar(x)-n+1, nchar(x))
}
```

# Appendix B: Measuring Cue Validity

In considering generalizations about categories, or *generic language*, theories try to formalize the truth conditions by analogy to quantified statements (e.g., *some*, *most*, ...).
Here, the prevalence of the feature, or $P(f \mid k)$, is the critical quantity.
Quantifiers can be described as conditions on prevalence: 
$$
\denote{some} = \{P(f \mid k) > 0\}, \denote{most} = \{P(f \mid k) > 0.5\}, \denote{all} = \{P(f \mid k) = 1\}
$$
The question for formal semantic theories has been: is there some threshold $\theta$ beyond which generic statements become true?
Hard-constraints on $\theta$ are difficult to defend because of cases like *Mosquitos carry malaria*, where only the weakest threshold would suffice (i.e., *Some mosquitos carry malaria*).


@Leslie2007 notes that mosquitos are the only species to carry malaria, and perhaps this explains why the generic is true.
More formally, the inverse probability --- the probability of the kind $k$ given the feature $f$: $P(k \mid f)$ --- might be able to save statistical-based approaches [@Leslie2007; @Khemlani2012].
This inverse probability, referred to as *cue validity*, encodes the diagnosticity of the feature for the kind.
For example, after learning that an entity *carries malaria*, the probability that the entity is a *mosquito* is very high. 

As noted in the main text, cue validity can be derived from a prevalence prior composed of a finite number of kinds.
This relationship follows from Bayes' Rule:

$$
P(x \in k | x \in f) = \frac{P(x \in f \mid x \in k) \cdot P(x \in k)}{\sum_{k'}P(x \in f \mid x \in k') \cdot P(x \in k')}
$$

where $x$ is an arbitrary entity, $k$ is a kind (represented as a collection of entities in the kind), and $f$ is a feature (represented as a collection of entities with the feature).
However, in a generative model composed of a potentially infinite number of kinds, the cue validity of a feature for a particular kind is 0. 
In our mixture model formulation of the prevalence prior, we explicitly represent the prevalence prior as a generative model with a potentially infinite number of kinds.
In this formulation, the mixture parameter $\phi$ can be seen as a generalization of cue validity, to the case of potentially infinite numbers of kinds.


Cue validity is not itself sufficient to explain endorsements because of cases like *Dogs have four legs*, where many kinds have the feature. 
However, one could argue for some integration of the two probabilities --- prevalence and cue validity.
This is often used as the "statistical-based approach" and operationalized by measuring prevalence and cue validity, and using both measurements as predictors of generic endorsement in a logistic-regression of generic endorsement [@Khemlani2012; @Prasada2013].

In Expt. 1, we articulated this alternative model by measuring cue validity (and prevalence) and predicting generic endorsement from a regression model.
In a small review of the literature, we discovered different methods for measuring cue validity, which lead to different results. 
Therefore, here we propose three *a priori* desiderata that a measurement of cue validity should satisfy.
We describe two experiments that represent the primary methods for measuring cue validity and compare them with these desiderata in mind.
Finally, we compare the cue validity measured using these different methods to cue validity derived from our prevalence prior elicitation task (Expt. 1a, main text).

## Measuring cue validity

Measuring cue validity involves collecting participants' judgments that relate to the probability of the kind given the feature.
There are at least two distinct, prominent ways of measuring cue validity.
One way is ask participants directly about the probability (or likelihood, odds, etc...) of the kind given the feature, using a likert scale or slider [@Khemlani2012; @Prasada2013].
We'll call this approach the "direct question" approach.
Another way is to supply participants with the feature alone and have them generate the kind or kinds [@Cree2006].
We'll call this approach the "free production" approach. 

What criteria can we use to validate one measurement over the other?
We propose three boundary cases *a priori* that the measurement should be able to satisfy.
For each case, we provide four examples which will be used to evaluate each measure.

\begin{enumerate}
\item{Completely diagnostic features: We contend there are a number of features that only one (or very small) of categories contain. Examples include: \emph{carrying malaria} (mosquitos), \emph{carrying Lyme disease} (ticks), \emph{having manes} (lions), \emph{having pouches} (kangaroos). The cue validity of these features for the corresponding categories should be very high (close to 1).}
\item{Completely absent features: For features that are completely absent in a kind, the cue validity should be extremely low or 0. There are many examples of these kinds of pairings; for example, \emph{has wings} (leopard), \emph{has a mane} (shark), "has spots" (kangaroo), "has a pouch" (tiger). }
\item{Completely undiagnostic features: We contend there are a number of features that almost every category contains. Examples include: \emph{is female} (robin), \emph{is male} (lion), \emph{is juvenile} (kangaroo), \emph{is full-grown} (leopard). The cue validity of these features for particular categories should be extremely low or 0. Learning that an entity is female tells you almost nothing about what kind of animal it is.}
\end{enumerate}

We collected cue validity ratings by running both a direct question and a free production experiment.
For the free production experiment, the cue validity is the proportion of responses of the target category (e.g., "mosquitos") for the property (e.g., "carries malaria").
Of primary interest is the measurement for the boundary conditions described above.
The experiments can be seen on \url{https://mhtess.github.io}.

## Experimental materials

Materials were the same for both experiments.
They were a collection of familiar properties and animal categories used in Expt. 1b (endorsement of generic statements) described in the main text.
There were twenty-one properties in total.

## Direct question experiment

### Method

#### Participants

We recruited 40 participants from Amazon's Mechanical Turk.
Participants were restricted to those with U.S. IP addresses and who had at least a 95\% work approval rating.
The experiment took on average 5 minutes and participants were compensated \$0.75 for their work.

#### Procedure

Following the procedure in @Khemlani2012 and @Prasada2013, participants were presented with prompts of the following form:

\begin{quotation}
Imagine you come across a thing that \textsc{f}.
What are the odds that it is a \textsc{k}?
\end{quotation}

Participants responded using a slider bar with endpoints labeled "unlikely" and "likely". 
The slider appeared with no handle present; participants had to click on the slider for the slider handle to appear.

Participants completed the thirty target trials (corresponding to the thirty generic statements used in Expt. X) in addition to ten filler trials (total number of trials = 40).
The filler trials were made up of random category -- property pairings.
All trials were presented in a randomized order.

## Free production experiment

### Method

#### Participants

We recruited 50 participants from Amazon's Mechanical Turk.
Participants were restricted to those with U.S. IP addresses and who had at least a 95\% work approval rating.
The experiment took on average 3 minutes and participants were compensated \$0.40 for their work.

#### Procedure

On each trial, participants were presented with prompts of the following form:

\begin{quotation}
Imagine you come across a thing (animal or insect) that \textsc{f}.
What do you think it is?
\end{quotation}

Participants responded by filling in a text box with their response.
There were twenty-one trials in total, one for each property.
Trials were presented in a randomized order.

## Results and Evaluation

To process the free production, we forced all characters to lower case, removed spaces, and made all terms into singular terms (e.g., "lions" --> "lion").
As well, "mosquito" was a commonly mispelled label; we counted anything that started with "mosqu", "mesqu", "misqu", "mosiq" as "mosquito". 

To calculate confidence intervals for the free production data, we resampled participants (with replacement) and computed the proportion of responses that were of the target category (e.g., the proportion of "mosquito" responses for the cue "carries malaria"). We did this one-thousand times to took the empirical 95\% quantile of those proportions.

We are interested in the results of each measure (direct question and free production) for the three conditions corresponding to the desiderata outlined above. 
To evaluate each measure, we selected four examples property--category pairs that we believe are unambiguously instances of the boundary conditions described above (these items are described above with the desiderata).

Figure \ref{fig:cv-bothquestions-barplots} shows the results for the twelve items of interest for both measurements.
We see that for the "false" features, both measures behave as desired (shown by the dotted line). 
The cue validity of a feature that is false is zero or near-zero.
For "diagnostic" features, both measures behave reasonably well.
Learning that an entity has malaria strongly implies that it is a mosquito. 
However, there are some subtle differences among our examples that only the free production measure picks up on.
"Having a mane" is strongly diagnostic for a "lion" but also for a "horse" (and so the overall cue validity for lion is around 0.5).
"Carrying Lyme disease" is mostly diagnostic for a "tick" but also "deer" (and thus, the cue validity for tick is not maximal).

The measures deviate the most in their treatment of the undiagnostic features. 
Learning that an entity is female does not strongly imply that it is a robin, which is accurately reflected in the free production measure but not in the direct question measure.
A similar phenomena can be observed in direct question measure for the other undiagnostic features.

```{r directQuestion, echo = F}
d.cv.dq <- read.csv(paste(project.path, data.path, "cue-validity-1-trials.csv", sep = ""))

diagnostic.props <- data.frame(
  item = c("kangaroo has a pouch", "lion has a mane", 
           "mosquito carries malaria", "tick carries Lyme disease"),
  type = "diagnostic"
)

undiagnostic.props <- data.frame(
  item = c("kangaroo is juvenile", "lion is male", "robin is female", "leopard is full-grown"),
  type = "undiagnostic"
)

false.props <- data.frame(
  item = c("leopard has wings", "kangaroo has spots", "tiger has a pouch", "shark has a mane"),
  type = "false"
)

target.items <- bind_rows(diagnostic.props, undiagnostic.props, false.props)

d.cv.targets <- left_join(
  d.cv.dq %>%
    mutate(property = gsub("&quotechar", "'", property), 
         item = paste(category, property)),
  target.items) %>%
  filter(!is.na(type)) %>%
  mutate(type = factor(type, levels = c("diagnostic", "undiagnostic", "false")))

d.cv.summary <- d.cv.targets %>%
  group_by(item, type) %>%
  multi_boot_standard(column = "response")


# ggplot(d.cv.summary, aes ( x = item, y = mean, ymin = ci_lower, ymax = ci_upper))+
#   geom_bar(stat = 'identity', position = position_dodge())+
#   geom_errorbar(position = position_dodge())+
#   facet_wrap(~type, scales = 'free')+
#   scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
#   theme(axis.text.x = element_text(angle = 90))+
#   xlab("")+
#   ylab("Probability rating of cue validity")+
#   ggtitle("Direct question paradigm")

```

```{r freeProduction, echo = F}
d.cv.fp <- read.csv(paste(project.path, 
                          data.path, "cue-validity-2-freeProduction-trials.csv", sep = ""))

mosquito.mispellings <- c("mosqu", "mesqu", "misqu", "mosiq")

d.cv.fp <- left_join(
  d.cv.fp %>%
 # rename(property = category) %>%
  mutate(response = tolower(response),
         response = gsub(" ", "", response),
         response = ifelse(substr(response, 1, 5) %in% mosquito.mispellings, "mosquito", response),
         response = ifelse(response == "deertick", "tick", response),
         response = ifelse(substrRight(response, 1) == "s",
                           substr(response, 1, nchar(response)-1), response),
         property = gsub("&quotechar", "'", property),
         item = paste(response, property)),
    target.items)

d.cv.fp.summary <- left_join(
  target.items,
  d.cv.fp %>%
    rename(category = response) %>%
    group_by(property) %>%
    mutate(n = n()) %>%
    filter(!(is.na(type))) %>%
    group_by(category, property, item) %>%
    summarize(mentions = n(),
              trials = mean(n), # mean(n) == n, because it's just the number of subjects
              prop = mentions / trials)
  ) %>%
  mutate(prop = ifelse(is.na(prop), 0.01, prop)) %>%
  mutate(type = factor(type, levels = c("diagnostic", "undiagnostic", "false")))



d.cv.bootstrapped <- data.frame()
resample_n <- length(levels(factor(d.cv.fp$workerid)))
for (i in 1:100){
  
  d.cv.bsample <-left_join(
    target.items,
    d.cv.fp %>% 
    select(workerid, property, response) %>% 
    spread(property, response) %>% 
    sample_n(resample_n, replace = TRUE) %>%
    gather(property, category, -workerid) %>% 
    group_by(property) %>%
    mutate(n = n(),
           item = paste(category, property))
    ) %>%
    group_by(category, property, item, type) %>%
    summarize(mentions = n(),
              trials = mean(n), # mean(n) == n, because it's just the number of subjects
              prop = mentions / trials,
              prop = ifelse(is.na(prop), 0.01, prop),
              iteration = i)
  
  d.cv.bootstrapped <- bind_rows(d.cv.bootstrapped, d.cv.bsample)
}

empiricalLower = function(dist){
  xi <- quantile(dist, 0.025)
  return (xi[["2.5%"]])
}
empiricalUpper = function(dist){
  xi <- quantile(dist, 0.975)
  return (xi[["97.5%"]])
}
empiricalMean = function(dist){
  xi <- quantile(dist, 0.5)
  return (xi[["50%"]])
}


d.cv.fp.summary <- d.cv.bootstrapped %>%
      group_by(item, type) %>%
      summarize(mean = empiricalMean(prop),
                ci_lower = empiricalLower(prop),
                ci_upper = empiricalUpper(prop)) %>%
  mutate(
    ci_lower = ifelse(is.na(mean), 0.01, ci_lower),
    ci_upper = ifelse(is.na(mean), 0.01, ci_upper),
    mean = ifelse(is.na(mean), 0.01, mean)
  )

d.cv.bothtasks <- bind_rows(
  d.cv.summary %>% mutate(task = 'direct question'),
  d.cv.fp.summary %>% mutate(task = 'free production')
)

theoretical.cv <- data.frame(
  type = c( "false", "undiagnostic","diagnostic"),
  cv = c(0.05, 0.05, 0.95)
)

```


```{r, cache = F}
cv.bothquestions.bars <- d.cv.bothtasks %>%
  mutate(type = factor(type, levels = c( "false", "undiagnostic","diagnostic"))) %>%
  ggplot(., aes ( x = item, y = mean, ymin = ci_lower, ymax = ci_upper))+
  geom_bar(stat = 'identity', position = position_dodge(0.5),
           width = 0.5, color = 'black', fill = 'white')+
  geom_errorbar(position = position_dodge(0.5), width = 0.3)+
  facet_grid(task~type, scales = 'free')+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  geom_hline(data = theoretical.cv, aes(yintercept = cv), lty = 2)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
        strip.text.y = element_text(angle = 0))+
  xlab("")+
  ylab("Cue validity")+
  guides(color = F)
```


```{r dq.histogram, fig.width = 3}
dq.histograms <- ggplot(d.cv.targets %>%
         filter(item %in% undiagnostic.props$item), aes( x = response ))+
  geom_histogram() + 
  facet_wrap(~item, nrow = 1) + 
  scale_x_continuous(breaks = c(0, 1))+
  scale_y_continuous(breaks = c(0, 10))
```


```{r cv-bothquestions-barplots, fig.height = 6, fig.width = 8, fig.cap="Empirically measured cue validity for two different tasks. Items are grouped by whether the property is never present in the category (false), the property is always present in the category and every other category (undiagnostic), or present in the category and absent from most other categories (diagnostic). Dotted lines denote theoretical cue validity representing the desiderata (see text). Error bars denote bootstrapped 95% confidence intervals."}
grid.arrange(cv.bothquestions.bars, dq.histograms, heights = c(2.5, 1))
```


## Comparison with prevalence prior derived cue validity

Here, we treat each entry (participant free production or experimentally supplied category like mosquitos) as contributing to the prevalence prior. This will have the prevalence prior favor kinds that are easy to produce (like dogs and cats) as well as the experimentally supplied kinds (like mosquitos and robins).



```{r}
diagnostic.props <- data.frame(
  item = c("kangaroo has a pouch", "lion has a mane", 
           "mosquito carries malaria", "tick carries Lyme disease"),
  type = "diagnostic"
)

undiagnostic.props <- data.frame(
  item = c("leopard is juvenile", "lion is male", "robin is female", "swan is full-grown"),
  type = "undiagnostic"
)

false.props <- data.frame(
  item = c("leopard has wings", "kangaroo has spots", "tiger has a pouch", "shark has a mane"),
  type = "false"
)

target.items2 <- bind_rows(diagnostic.props, undiagnostic.props, false.props)
```

```{r cv-fp-data-analyzeAll, cache = T}
d.target.items <- fromJSON(paste(project.path, "data/generics/endorsement/", "originalStims.json", sep = "")) %>%
  mutate(
    property = gsub("'", "", property),
    Property = gsub("'", "", Property),
    sentence = paste(Category, Property),
    item = paste(category, property))

d.cv.bootstrapped.all <- data.frame()
resample_n <- length(levels(factor(d.cv.fp$workerid)))
for (i in 1:1000){
  
  d.cv.bsample <- d.cv.fp %>% 
    select(workerid, property, response) %>% 
    spread(property, response) %>% 
    sample_n(resample_n, replace = TRUE) %>%
    gather(property, category, -workerid) %>% 
        group_by(property) %>%
    mutate(n = n(),
           item = paste(category, property)) %>%
    filter(item %in% d.target.items$item) %>%
    group_by(category, property) %>%
    summarize(mentions = n(),
              trials = mean(n), # mean(n) == n, because it's just the number of subjects
              prop = mentions / trials,
              prop = ifelse(is.na(prop), 0.01, prop),
              iteration = i)
  
  d.cv.bootstrapped.all <- bind_rows(d.cv.bootstrapped.all, d.cv.bsample)
}

d.gen.cv.all.summary <- left_join(
    d.target.items %>% select(-sentence),
    d.cv.bootstrapped.all %>%
      group_by(category, property) %>%
      summarize(cv_mean = empiricalMean(prop),
                cv_ci_lower = empiricalLower(prop),
                cv_ci_upper = empiricalUpper(prop))
) %>%
  mutate(
    cv_ci_lower = ifelse(is.na(cv_mean), 0.01, cv_ci_lower),
    cv_ci_upper = ifelse(is.na(cv_mean), 0.01, cv_ci_upper),
    cv_mean = ifelse(is.na(cv_mean), 0.01, cv_mean)
  )

```


```{r cuevalidity-model-pk}
wp.pkg.path <- paste(project.path, "models/generics/node_modules/utils/", sep = "")


cueValidityModel1 <- '
var cueValidity = function(){
  var kind = uniformDraw(data.prior);
  observe(Bernoulli({p: kind.prevalence/100}), true)
  return kind.Category == data.kind[0]
}

expectation(Infer({model: cueValidity}))
'


d.prev <- read.csv(paste(project.path, "data/generics/endorsement/",
                       "naturalGenerics-prior-trials-n57.csv", sep = ""))


d.gen.endorse.priors <- read.csv(paste(project.path, 
                                       "data/generics/endorsement/",
                                       "naturalGenerics-prior-trials-n57.csv", sep = ""))
gen.endorse.properties <- levels(d.gen.endorse.priors$Property)


wp.cueValidity <- data.frame()
for (p in gen.endorse.properties){
  categories <- levels(factor(filter(d.gen.cv.all.summary, Property == p)$Category))
  prevPriorData = d.gen.endorse.priors %>% filter(Property == p)
  
  for (catg in categories){

    dataToPass <- list(
      prior = prevPriorData,
      kind = catg
    )
    
    wp.rs <- webppl(cueValidityModel1, 
       data_var = "data",
       data = dataToPass)
    
    wp.cueValidity <- bind_rows(wp.cueValidity, 
                                data.frame(Category = catg,
                                           Property = p,
                                           cv = wp.rs))
    
  }
  print(p)
}
# save(wp.cueValidity, file =
#        paste(project.path, model.path,
#              "results/prevPriorDerivedCueValidity.Rdata", sep = ""))
# load(file = 
#        paste(project.path, model.path, 
#              "results/prevPriorDerivedCueValidity.Rdata", sep = ""))
```


```{r}
md.fp <- left_join(left_join(
  d.gen.cv.all.summary,
  wp.cueValidity %>% rename(prev_prior_derived_cv = cv)
  #wp.cueValidity.uniform %>% rename(prev_prior_derived_cv = cv)
), target.items2)

ggplot(md.fp, aes (x = prev_prior_derived_cv, y = cv_mean, ymin = cv_ci_lower, ymax = cv_ci_upper,
                   color = type, alpha = type))+
  geom_point()+
  scale_alpha_manual(values = c(1,1,1,0.5))+
  geom_errorbar()+
  ylim(0, 1) + xlim(0, 1)+
  coord_fixed()

compute_r2(md.fp, "cv_mean", "prev_prior_derived_cv")
compute_mse(md.fp, "cv_mean", "prev_prior_derived_cv")


md.fp.desid <- md.fp %>%
  filter(item %in% target.items2$item)

compute_r2(md.fp.desid, "cv_mean", "prev_prior_derived_cv")
compute_mse(md.fp.desid, "cv_mean", "prev_prior_derived_cv")
```

```{r}
d.cv.dq.all <- d.cv.dq %>%
  mutate(property = gsub("&quotechar", "", property), 
         item = paste(category, property)) %>%
  group_by(category, property) %>%
  multi_boot_standard(column = 'response')

md.dq <-  left_join( left_join(
    left_join(
    d.gen.cv.all.summary %>% select(Category, Property, category, property, item),
    d.cv.dq.all),
    #wp.cueValidity.uniform %>% rename(prev_prior_derived_cv = cv)
    wp.cueValidity %>% rename(prev_prior_derived_cv = cv)
  ), target.items2)

ggplot(md.dq, aes (x = prev_prior_derived_cv, y = mean, ymin = ci_lower, ymax = ci_upper,
                     color = type))+
  geom_point()+
  geom_errorbar()+
  ylim(0, 1) + xlim(0, 1)+
  coord_fixed()
  
compute_r2(md.dq, "mean", "prev_prior_derived_cv")
compute_mse(md.dq, "mean", "prev_prior_derived_cv")

md.dq.desid <- md.dq %>%
  filter(item %in% target.items2$item)

compute_r2(md.dq.desid, "mean", "prev_prior_derived_cv")
compute_mse(md.dq.desid, "mean", "prev_prior_derived_cv")


```

```{r cvModel-uniformPrior}
cueValidityModel2 <- '
var cueValidity = function(){
  var kind = uniformDraw(data.allKinds);
  var responses = _.filter(data.prior, {Category: kind});
  var response = _.isEmpty(responses) ? {prevalence: 0} : uniformDraw(responses);
  observe(Bernoulli({p: response.prevalence/100}), true)
  return kind == data.kind[0]
}

expectation(Infer({model: cueValidity}))
'

allKinds = levels(factor(d.gen.endorse.priors$Category))

wp.cueValidity.uniform <- data.frame()
for (p in gen.endorse.properties){
  categories <- levels(factor(filter(d.gen.cv.all.summary, Property == p)$Category))
  prevPriorData = d.gen.endorse.priors %>% filter(Property == p)
  
  for (catg in categories){

    dataToPass <- list(
      prior = prevPriorData,
      kind = filter(prevPriorData, Category == catg)[1, "Category"],
      allKinds = allKinds
    )
    
    wp.rs <- webppl(cueValidityModel2, data_var = "data", data = dataToPass)
    
    wp.cueValidity.uniform <- bind_rows(wp.cueValidity.uniform, 
                                data.frame(Category = catg,
                                           Property = p,
                                           cv = wp.rs))
    
  }
  print(p)
}
# save(wp.cueValidity, file = 
#        paste(project.path, model.path, 
#              "results/prevPriorDerivedCueValidity.Rdata", sep = ""))
# dataToPass %>% toJSON(.)
# 
# wp.rs

```



