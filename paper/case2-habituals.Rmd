# Case Study 2: Habitual Language

<!-- THINK ABOUT: -->
<!-- Kelley, H. H. (1973). The processes of causal attribution. American psychologist, 28(2), 107-128. -->
<!-- Orvis, B. R., Cunningham, J. D., & Kelley, H. H. (1975). A closer examination of causal inference: The roles of consensus, distinctiveness, and consistency information. Journal of Personality and Social Psychology, 32(4), 605-616. -->

As with categories, particular events like "Mary smoked yesterday" can be generalized into *habitual sentences*: "Mary smokes". 
<!-- Events are a broad class of entities.  -->
In our second case study, we focus on habituals about people's behaviors that take the form: *singular noun phrase* $+$ *present tense simple verb phrase* (e.g., "Mary smokes cigarettes"). 
Learning about the behaviors of others is useful because they tell us about what that person is like more generally [e.g., @Repacholi1997; @Seiver2013].
When children describe their lives to others, a surprisingly large amount of the language produced concerns the actions of people close to them [e.g., "My brother works part-time at the restaurant"; @McGuire1986].
<!-- Habitual language about people may be a more conservative form of *trait language* (e.g., "Mary is a smoker.") and convey that behaviors are relatively enduring [@Gelman1999; @Gelman2004]. \ndg{save that last point for discussion?} -->

To test the generality of our theory, we use the same computational model and follow the same general experimental structure as in the first case study. 
We take the event analogue of prevalence to be the *rate* with which the event occurs (e.g., how often Mary smokes).^[
  Specifically, for the generalization "Mary smokes", the instances being generalized are *instances of Mary*.
]
We test this by first measuring the prevalence (rate) prior distribution for various actions (e.g., how often different people smoke cigarettes; Expt. 2a).
We then measure endorsements of habitual statements while manipulating the referent prevalence  (Expt. 2b), and use our computational model to predict habitual endorsements. 
<!-- We estimate the priors using a novel data-analytic approach that takes advantage of the mixture-model validated in Expt. 1b. -->
By describing novel characters to participants, we are able to directly the manipulate the referent prevalence, which we were unable to do for generalizations about familiar categories.
<!-- In addition to directly manipulating the referent feature-probability, these experiments provide the first test of the generality of our theory: It can apply to generalizations about events as well as categories. -->

Finally, if habituals (and generics) are truly language for conveying generalizations, they should reflect speakers' expectations, not merely their observations. 
Our computational model predicts endorsement rates given a referent prevalence $p$. 
Does prevalence represent the actual, objective frequency in the world (e.g., the rate at which a person has smoked cigarettes in the past) or a subjective, predictive belief in the head (e.g., the rate at which a person is expected to smoke in the future)?
Such a distinction could explain why "Supreme Court Justices have even social security numbers" sounds strange even if nine out of the nine current justices have even social security numbers [@Cohen1999]: Our predictions about the evenness of the *next* justice's social security number are driven by strong prior beliefs that selection for the Supreme Court is uncorrelated from the numerical properties of one's social security number; the current observations are not enough in this case to sway those beliefs.
<!-- not swayed by these observations due to -->
We elucidate this aspect of our theory by measuring participants' predictions about future rates and endorsements of habituals when causal forces intervene on the world (e.g., the person buys a pack of cigarettes; Expt. 2c).
We then use our model to test whether habitual endorsements are based on past frequencies or future expectations.

<!-- (e.g., how often Mary has actually smoked in the past month) or the predictive probability of the event in the near future (e.g., how often we should expect Mary to smoke next week)? -->

<!-- Young children can use actors' behaviors to make inferences about what the actors are like more generally [e.g., @Repacholi1997; @Seiver2013]. -->
<!--  explored 5th to 12th graders' responses to open ended probes such as "Tell us about your school / family".  -->
<!-- A surprisingly large number of utterances (over 85\% in their study) were about people, and roughly two-thirds of those utterances used action verbs (e.g., "My brother works part-time at the restaurant."). -->

<!-- This set of experiments is organized as follows. -->
<!-- In the **Event Prior Elicitation** study (Expt. 1), -->
<!-- In the **Endorsing Habitual Language** experiment (Expt. 2), we introduce a separate participants to different characters, each of whom have done some action with some frequeny (e.g., *In the last month, John ran 3 times*), and ask them to judge the corresponding generalization (e.g., *John runs*). -->
<!-- In the **Interpreting Habitual Language** experiment (Expt. 3), we invert this paradigm, supplying participants with the generalization (e.g., *John runs*) and ask them to juge how often the person does some action (e.g., answer the question: *How often do you think John runs?*). -->

<!-- We evaluate our family of RSA models, in addition to some ad-hoc non-Bayesian models, on these experimental data. -->
<!-- Because the ad-hoc models do not comprise a formalized alternative theory of endorsement and interpretation, but rather statistical descriptions of the data, we compare the ad-hoc models to the simplest RSA model on endorsement and interpretation, separately. -->
<!-- After presenting the results of Expts. 2 \& 3, we compare RSA models on their ability to jointly predict both endorsement and interpretation data simultaneously.  -->
<!-- Finally, having validated a communicative theory of genericity, we probe the theory in more detail in Experiment 4 by asking whether past frequency or predictive probability is what is being communicated. -->


<!-- The internal structure of particular events can differ in substantial ways. -->
<!-- For example, events differ in their expected duration. -->
<!-- For example, the event of *smoking a cigarette* plausibly takes between 5 - 10 minutes, while the event of *wearing socks* probably lasts for several hours. -->
<!-- We take as a starting point the fact that events can be segmented and aggregated; the relevant dimension becomes the frequency with which the event occurs. -->


## Experiment 2a: Measuring the prevalence prior for events

In order to generate model predictions for corresponding habitual statement, we first elicit the prior distributions over rates for different events.
For language about the behaviors of people, $P(p)$ represents a language user's background knowledge about the rates with which people do a behavior; this prior can be constructed as a distribution over *different people*, each of whom do the behavior with a different rate.
We designed our elicitation task to take advantage of the mixture-model representation of the prevalence prior used in Expt. 1.
In particular, we assume, to a first approximation, that the distribution over prevalence can be represented as a mixture of those who tend to do the action with a stable rate and those who do not do the action.^[
  It is likely that more than just these two possibilities are represented in people's intuitive prior, corresponding to individuals with additional traits or demographics.
  We assume here a simple two-component structure so as to not make the specification of the prior overly complex.
]
With the further assumption that, all else being equal, past is predictive of future behavior, we operationalize these two kinds of people as *people who have done the action before* and *people who have not the action before*.
We design this experiment to measure participants' beliefs about the relative proportion of these two kinds of people (as a measure of the mixture parameter in the model) as well as the rate at which people (who have done the action before) do the action.
We will assume for simplicity that people who have never done the action before will probably never do the action. 


### Method

#### Participants

We recruited 40 participants from Amazon's Mechanical Turk.
Participants were restricted to those with U.S. IP addresses and who had at least a 95\% work approval rating.
The experiment took on average 12 minutes and participants were compensated \$1.25 for their work.

#### Materials

To construct our stimulus set, we choose actions from five categories of typical human behaviors: eating/consuming food and drug, work, wearing clothing, entertaining activities, and hobbies. 
For each category, we created pairs or triplets of events that shared a superordinate actions (e.g. *writing* poems vs. novels).
The events were chosen to intuitively cover a range of likely frequencies.
In total, thirty-one events were used.
For a full list of the stimuli used in Expts. 2a-c, see Appendix D.

#### Procedure

For each event, participants were asked two questions, with different dependent measures. 
These questions were designed to measure the two components of the prevalence prior model.
We anticipated there to be different beliefs about the rates and relative proportions of people depending on whether an actor is male or female, so we asked about both genders separately. The two questions were schematically:

\begin{enumerate}
\item ``How many \{men, women\} have *done action* before?'' \\

Participants responded ``N out of every J.'' by entering a number for N and choosing J from a drop-down menu (options: \{1000 - 10 million\}, incremented by 10x; default setting: 1000).

\item ``For a typical \{man, woman\} who has *done action*  before, how frequently does he or she \textsc{do action}?''\\  

Participants responded ``M times in K.'' by entering a number for M and choosing K from a drop-down menu (options: \{week, month, year, 5 years\}; default setting: year).
\end{enumerate}

For example, one set of prompts read: "How many women have smoked cigarettes before?"; "For a typical woman who has smoked cigarettes before, how frequently does she smoke cigarettes?"
Participants answered both questions for both genders on each slide (4 questions total per slide, order of male / female randomized between-subjects), and every participant completed all 31 items in a randomized order.
The difference in meaning of these questions was explained to participants on an instructions page before the experimental trials and tested for recall on a subsequent trial.
Participants responded to this attention check by selecting an option from a drop-down menu consisting of four options (one correct description of the questions and three distractors). 

```{r habituals-priors-subfigs, fig.width = 8}
example.habituals <- c(#"smokes marijuana", 
                       "smokes cigarettes","drinks coffee",
                       "climbs mountains", "hikes", "runs", 
                       #"wears a suit", "wears a watch", "wears socks", 
                       "writes poems", 
                       "writes novels", 
                       "goes to the movies", "plays the banjo"
                       )
annualRates = list("5 years" = 1/5, "2 years" = 1/2,
                   "year" = 1, "6 months" = 2, "2 months" = 6,
                   "month" = 12, "2 weeks" = 26 ,"week" = 52)


load("cached_results/case2_priors_parametersPredictives.RData") #habituals.priors.marginals, rs.habituals.bda.posterior.predictive.measurementSpace, rs.habituals.bda.prior.summary.wide


n.items.habituals.prior.modeling <- length(rs.habituals.bda.prior.summary.wide$item)

rs.habituals.bda.posterior.predictive.measurementSpace.wide <- rs.habituals.bda.posterior.predictive.measurementSpace %>%
  select(-mean, -ci_lower, -ci_upper, -empirical_mean, -n, -cred_upper, -cred_lower) %>%
  spread(measurement, MAP)

figure.habituals.priors.scatter <- ggplot(
  rs.habituals.bda.posterior.predictive.measurementSpace.wide %>%
                    filter(item %in% example.habituals, gender == "female"),
       aes(x = `% people who have done action before`, 
           y = `Frequency of doing action (log scale)`, 
           color = gender, label = item))+
  scale_color_solarized()+
  geom_point(data = rs.habituals.bda.posterior.predictive.measurementSpace.wide %>%
                    filter(!(item %in% example.habituals) | gender == "male"),
             inherit.aes = F, 
             aes(
               x = `% people who have done action before`, 
               y = `Frequency of doing action (log scale)`, color = gender))+
  geom_point()+
  geom_label_repel(segment.alpha = 1,label.size = 0.25,
                   force = 10, size = 2, color = 'black')+

  #geom_errorbar(alpha = 0.3)+geom_errorbarh(alpha = 0.3)+
  coord_fixed(1/7.5)+
  #xlab("Model posterior predictive")+
  #ylab("Empirical means")+
  theme(legend.position = c(0.83, 0.16))+
  scale_y_continuous(
      limits = c(-1.5, 6),
      breaks = c(0, 2.5, 4, 5.9),
      labels = c( "annually", "monthly", "weekly", "daily")
    )+
  scale_x_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))

# figure.habituals.priors.scatter <- ggplot(rs.habituals.bda.posterior.predictive.measurementSpace,
#        aes(x = MAP, xmin = cred_lower, xmax = cred_upper,
#            y = empirical_mean, ymin = ci_lower, ymax = ci_upper, color = gender))+
#   facet_wrap(~measurement, scales = 'free', nrow = 2)+
#   scale_color_solarized()+
#   geom_point()+
#   geom_errorbar(alpha = 0.3)+geom_errorbarh(alpha = 0.3)+
#   geom_label_repel(data = rs.habituals.bda.posterior.predictive.measurementSpace %>%
#                     filter(item %in% example.habituals, gender == "female"), 
#                   inherit.aes =F, segment.alpha = 0.1,label.size = 0.15,
#                   aes(x = MAP,y = empirical_mean, label = item), force = 15, size = 2)+
#   coord_fixed()+
#   xlab("Model posterior predictive")+
#   ylab("Empirical means")+
#   theme(legend.position = c(0.86, 0.09))


figure.habituals.priors.marginals <- ggplot(habituals.priors.marginals %>%
  filter(val > -4.9) %>%
  mutate(gender = factor(gender, levels = c("female", "male"))), 
  aes( x = val, color = gender, lty = src))+
  geom_density(size = 0.4, aes( y = ..scaled.. ))+
  facet_wrap(~item, nrow = 4)+
  geom_rect(data = rs.habituals.bda.posterior.predictive.measurementSpace %>%
               filter(gender == 'male', measurement == "% people who have done action before"), 
            aes(xmin = -5, xmax = -4.5, ymin = 0, ymax = 1 - MAP),
              fill = "#268bd2", 
            color = 'black',
             inherit.aes = F)+
  geom_rect(data = rs.habituals.bda.posterior.predictive.measurementSpace %>%
               filter(gender == 'female', measurement == "% people who have done action before"), 
            aes(xmin = -4.25, xmax = -3.75,
                                            ymin = 0, ymax = 1 - MAP),
            fill = "#cb4b16",
            color = 'black',
             inherit.aes = F)+
  scale_color_solarized()+
  scale_linetype_manual(values = c(2, 1))+
  xlab("Frequency")+
  ylab("Scaled probability density")+
  scale_y_continuous(limits = c(0,1), breaks = c(0, 1)) +
  theme(legend.position = c(0.93, -0.1),
        legend.title = element_blank(),
        legend.box = 'vertical',
        axis.text.x = element_text(angle = 90, hjust = 1, vjust = 1),
        legend.margin=margin(t=0, r=0, b=-0.25, l=0, unit="cm"),
    strip.text = element_text(size = 7))+
  scale_x_continuous(
      limits = c(-5, 8),
      breaks = c(-5, 0, 2.5, 4, 5.9),
      labels = c("almost never", "annually", "monthly", "weekly", "daily")
      )+
  guides(color = F)
```

```{r habituals-prior-figure, fig.width = 15,fig.asp = 0.37, fig.cap="Prevalence priors for events (Expt. 2a). A: Parameters of prevalence priors for the 31 items in Expt. 2. Items cover much of the range of possible parameter values. B: Reconstructed prevalence priors. In order to show frequencies for events that are very rare across people (e.g., \"writes novels\"), extremely low frequencies (\\emph{almost never}) are omitted. Instead, height of the bars on left denote the Maximum A-Posteriori values of the mixture component (in terms of \\emph{proportion of people who have never done action before} scatter-plot), reflecting the (inverse) popularity of the event across people.", cache = F}
cowplot::plot_grid(
  figure.habituals.priors.scatter + theme(plot.margin = unit(c(6,6,6,0), "pt")), 
  figure.habituals.priors.marginals + theme(plot.margin = unit(c(6,0,6,6), "pt")),
  labels = c("A", "B"), #nrow = 1,
  align = 'h',
  rel_widths = c(1, 3)
  )
```

<!-- Every response for a frequency of a person doing an action is a sample from $P_{event}(h)$. -->
<!-- We first transformed all responses onto the same scale of "times per year" (assuming 52 weeks per year; 12 months per year). -->
<!-- An empirical distribution can be found by making a histogram of these responses.  -->
<!-- The empirical distributions follows a kind-of log-normal distribution, with the additional feature of having a substantial probability mass at the frequency of 0 (for those individuals who never do the action).  -->
<!-- We thus log-transformed the empirical data.  -->
<!-- To avoid values of -Infinity, we set the responses of 0 to mean roughly "once every twenty years". -->

<!-- We took the log-transformed (and "-Infinity"-adjusted) raw data and binned each response to the closest bins on a grid with binwidth of 0.5 in the log-"times per year" scale. -->
<!-- This yields a grid of 23 points with frequencies ranging from "once every twenty years" (log-frequency = -3) to "twenty times per day" (log-frequency = 9). -->

### Data analysis and results

All participants responded correctly to both questions in the attention check trial, so all collected data was used in the analysis.
Question 1 elicits the proportion of people who have done an action before. 
We rescale this to be a number between 0 and 1, and model it as generated from a Beta distribution: $d_{1} \sim \text{Beta}(\gamma, \xi)$. 
Question 2 elicits the rate with which a person (who has done the action before) does the action.
We model this as generated by a log-normal distribution: $\ln d_{2} \sim \text{Gaussian}(\mu, \sigma)$.
Each item was modeled independently for each gender.
We learned about the credible values of the parameters by running MCMC for 100,000 iterations, discarding the first 50,000 for burn-in.

The priors elicited cover a range of possible parameter values as intended (Figure\ \@ref(fig:habituals-prior-figure)A):
We observe a correlation in our items between the mean \% of Americans who have \textsc{done action} before (Question 1) and the mean log-frequency of action (Question 2) ($r_{1,2}(`r  n.items.habituals.prior.modeling`) = `r round(with(rs.habituals.bda.prior.summary.wide, cor(mix_mean, sfreq_mean)),2)`$).
Items in our data set that tend to be more popular actions also tend to be more frequent actions (e.g., \emph{wears socks}) and visa-versa (e.g., \emph{steals cars}), though there are notable exceptions (e.g., \emph{plays the banjo} is not popular but done frequently when done at all, as is \emph{smokes cigarettes}; \emph{goes to the movies} is a popular activity though not done particularly often).
This diversity is relevant because the speaker model (Eq. \ref{eq:S1}) will endorse habitual sentences (e.g., \emph{Sam goes to the movies vs. the ballet.}) contingent on these details of the prior distribution. 

To generate prevalence prior distributions, we built a Bayesian mixture-model for this prior elicitation task, analogous to that used in Case Study 1 (Expt. 1b).
The only difference is that we estimate the mixture component $\phi$ directly from responses to Question 1. 
We assume that those who have not done the action before will probably not do the action in the future. 
With these assumptions, the prevalence distribution is given by:

\begin{align}
\phi & \sim \text{Beta}(\gamma_{Q1}, \xi_{Q1}) \nonumber \\ 
\ln p & \sim \begin{cases}
		\text{Gaussian}(\mu_{Q2}, \sigma_{Q2}) &\mbox{if } \text{Bernoulli}(\phi) = \textsc{T} \label{eq:priorModel}  \\
				\text{Delta}(p=0.01) &\mbox{if } \text{Bernoulli}(\phi) = \textsc{F} \\
		\end{cases}
\end{align}

Figure\ \@ref(fig:habituals-prior-figure)B shows example reconstructed priors.
In addition to specifying the correct way to combine our two prior-elicitation questions, using this inferred prior resolves two technical difficulties.
First, it smooths effects that are clearly results of the response format. 
For example, a very common rating for certain events is \emph{1 time per year}.
Presumably participants would be just as happy reporting \emph{approximately} 1 time per year (e.g., on average, 1.2 times per year); the raw data does not reflect this due to demands of the dependent measure.
Second, this methodology better captures the tails of the prior distribution (i.e., very frequent or very infrequent rates) which have relatively little data and need to be regularized by the analysis.


<!-- Analagous to what was done with the prior knowledge for generic language, from the inferred parameters and the assumed functional forms (i.e., the mixture distribution), we can generate $P(h)$ modeled as a mixture of individuals who have done the action before and those that haven't. -->
<!-- That is, $P(h)$ was constructed by sampling $\lambda$ as follows: -->

<!-- We first sample a mixture weight $\phi$ from the posterior distribution over the mixture component hyperparameters $\text{Beta}(\gamma_{mixture}, \xi_{mixture})$, inferred from the first question of the experiment. -->
<!-- We then flip a coin weighted by $\phi$: $\text{Bernoulli}(\phi)$. -->
<!-- If the coin comes up heads (i.e., $\text{Bernoulli}(\phi) = \textsc{true}$), a frequency $h$ is sampled from the distribution of people who have done the action before. This distribution is a log-normal distribution with parameters inferred from the second question of the experiment $\ln h \sim \text{Gaussian}(\mu_{stable}, \sigma_{stable})$. -->
<!-- If the coin comes up tails, a frequency $h$ is sampled from the distribution of people who have not done the action before. -->
<!-- We make the simplifying assumption that people who have not done the action before will probably never do the action (expected frequency: once every hundred years). -->

<!-- Some items show substantial differences between the genders (e.g., \emph{wears a bra}) and some show subtle differences (e.g., \emph{watches professional football}). -->
<!-- Recall that the prior distributions $P(h)$ used in the RSA are with respect to an alternative class of entities (e.g., other individuals).  -->
<!-- It's possible that when evaluating habitual statements under certain contexts, $P(h)$ is with respect to either *all people* or *people of the same gender*.  -->
<!-- For example, are the frequency conditions by which a man would qualify to "watch  football" (habitually) different than those by which a woman would qualify to "watch  football"? -->
<!-- We explore the possibility of different truth conditions for habituals of different gendered characters in Experiment 2b, for select items with priors that differ substantially by gender. -->


<!-- We analyze the data using a Bayesian approach to allow for a consistent integration into the computational model. -->
<!-- The data we would like to explain are: the bins $n_i \in \{-3, -2.5, ..., 8.5, 9\}$ that participants gave in response to items $i \in \{1, ..., 27\}$. -->
<!-- The data is explained as a function of subjective beliefs $P_i$, with $P_{ij}$ being participants' (as a collective) belief about the relative likelihood for bin $j$ for item $i$.  -->
<!-- Each $P_i$ defines a likelihood for our data, assuming an appropriate linking function. -->

<!-- Following @Franke2016, the linking function for this data treats each bin $n_{i}$ as a draw from a categorical distribution where the probability of bin $j$ is proportional to $\exp{(a\cdot P_i)}$, i.e., a soft-max choice from $P_{i}$.  -->
<!-- The higher parameter $a$, the more likely $n_{i}$ is the mode of $P_{i}$.  -->
<!-- For $a \rightarrow 0$, all bins become equiprobable. -->

<!-- \begin{eqnarray} -->
<!-- P_{i} &\sim& \text{Dirichlet}(1, ..., 1) \\  -->
<!-- a & \sim & \text{Gamma}(2,1) \\ -->
<!-- n_{i} &\sim & \text{Categorical}(\exp(a \cdot P_i)) -->
<!-- \end{eqnarray} -->

<!-- VIZ (as before)? -->

<!-- We assume each "binned" response is a sample from a multinomial distribution with unknown probability vector. -->
<!-- We put a Dirichlet-prior over this probability vector. -->

<!-- We built a Bayesian data analysis model for this prior elicitation task. -->
<!-- Question 1 elicits the proportion of people who have done an action before.  -->
<!-- We model this data as coming from a Beta distribution: $d_{1} \sim \text{Beta}(\gamma_{1}, \xi_{1})$.  -->
<!-- Question 2 elicits the rate, or relative frequency, with which a person does the action. -->
<!-- This was modeled by a log-normal distribution: $\ln d_{2} \sim \text{Gaussian}(\mu_{2}, \sigma_{2})$.  -->
<!-- Each item was modeled independently for each gender. -->
<!-- We implemented this model using the probabilistic programming language WebPPL \cite{dippl}, and found the credible values of the parameters by running MCMC for 100,000 iterations, discarding the first 50,000 for burnin. -->

<!-- The priors elicited cover a range of possible parameter values as intended (Figure \ref{fig:priorScatter}, scatter), resulting in parametrized distributions of dramatically different shapes (insets).   -->
<!-- We observe a correlation in our items between the mean \% of Americans who have \textsc{done action} before (Question 1) and the mean log-frequency  of action (Question 2) ($r_{1,2} = 0.74$). -->
<!-- Items that tend to be more popular actions also tend to be more frequent actions (e.g. \emph{wears socks}) and visa-versa (e.g. \emph{steals cars}), though there are notable exceptions (e.g. \emph{plays the banjo} is not popular but done frequently when done at all, as is \emph{smokes cigarettes}; \emph{goes to the movies} is a popular activity though not done very often).  -->
<!-- This diversity is relevant because the speaker model (Eq.~\ref{eq:S2}) will produce habitual sentences (e.g. \emph{Sam goes to the movies vs. the ballet.}) contingent on the shape of the prior distribution.  -->

<!-- From the inferred parameters and assumed functional forms, we get an inferred $P(p)$ modeled as a mixture of individuals with the possibility of carrying out the action and those without the possibility of doing it.  -->
<!-- That is, $P(p)$ was constructed by sampling $p$ as follows: -->

<!-- \begin{align} -->
<!-- \theta & \sim \text{Beta}(\gamma_{1}, \xi_{1}) \nonumber \\  -->
<!-- \ln \lambda & \sim \begin{cases} -->
<!-- 		\text{Gaussian}(\mu_{2}, \sigma_{2}) &\mbox{if } \text{Bernoulli}(\theta) = \textsc{t} \label{eq:priorModel}  \\ -->
<!-- 				\delta_{\lambda=-\infty} &\mbox{if } \text{Bernoulli}(\theta) = \textsc{f} \\ -->
<!-- 		\end{cases} -->
<!-- \end{align} -->

<!-- In addition to specifying the correct way to combine our two prior-elicitation questions, using this inferred prior in our language model resolves two technical difficulties: (1) It smooths effects that are clearly results of the response format^[ -->
<!-- For example, a very common rating is *1 time per year*. Presumably participants would be just as happy reporting *approximately* 1 time per year; the raw data does not reflect this due to demands of the dependent measure. -->
<!-- ] -->
<!-- and (2) it better captures the tails of the prior distribution which have relatively little data and need to be regularized by the analysis. -->
<!-- Figure \ref{fig:priorScatter} (right) shows example inferred priors. -->

<!-- Some items show substantial differences between the genders (e.g., *wears a bra*) and some show subtle differences (e.g., *watches professional football*).  -->
<!-- We will explore the possibility of different truth conditions for habituals of different gendered characters in Experiment 2, for select items with priors that differ substantially by gender. -->

## Experiment 2b: Habitual endorsements

In this experiment, we elicit human endorsements for generalizations about events (*habituals*; e.g., "Mary smokes cigarettes") while manipulating the frequency with which the referent-event occurs (e.g., how often Mary smokes cigarettes). 

```{r habituals-endorsement-gender-exploration}
load("cached_results/case2_endorsement_genderItemAnalysis.RData") #d.hab.bayes.gendered.wide, d.hab.bayes.gendered.subset.wide
load("cached_results/case2_endorsement_regressionResults.RData") #d.hab.endorse.regression.freq.mix, d.hab.endorse.regression.freq, d.hab.endorse.regression
load("cached_results/case2_endorsement_rsaModelResults.RData") #m.hab.endorse.rsa, hab.uncertain.threshold.s1opt, fixed.hab.params.posterior, md.hab.long, md.hab, hab.listener.predictions



gendered.items <- c("does cocaine", "drinks beer", "drinks coffee",
                    "wears a bra", "wears a watch", "wears a suit")

r2.habituals.gendered <- compute_r2(d.hab.bayes.gendered.wide, "map_female", "map_male")
n.habituals.gendered.analysis <- length(d.hab.bayes.gendered.wide$habitual)
r.habituals.gendered.analysis <- round(with(d.hab.bayes.gendered.wide, cor(map_female, map_male)), 3)

n.habituals.gendered.subset.analysis <- length(d.hab.bayes.gendered.subset.wide$habitual)
r.habituals.gendered.subset.analysis <- round(with(d.hab.bayes.gendered.subset.wide, 
                                                   cor(map_female, map_male)), 3)
```

```{r habituals-regression-models}

r2.hab.n <- length(d.hab.endorse.regression.freq$MAP_h)

r2.habituls.regression.freq <- compute_r2(d.hab.endorse.regression.freq,
                                          "MAP_h", "prediction")

mse.habituls.regression.freq <- compute_mse(d.hab.endorse.regression.freq,
                                          "MAP_h", "prediction")

r2.habituls.regression.infrequent.freq <- d.hab.endorse.regression.freq %>%
  filter(logAnnualRate < 1.1)

r2.hab.infrequent.n <- length(r2.habituls.regression.infrequent.freq$MAP_h)

r2.habituls.infrequent.regression.freq <- compute_r2(r2.habituls.regression.infrequent.freq,
                                          "MAP_h", "prediction")

mse.habituls.infrequent.regression.freq <- compute_mse(r2.habituls.regression.infrequent.freq,
                                          "MAP_h", "prediction")

     
r2.habituls.regression.freq.distinct <- compute_r2(d.hab.endorse.regression.freq.mix,
                                          "MAP_h", "prediction")

mse.habituls.regression.freq.distinct <- compute_mse(d.hab.endorse.regression.freq.mix,
                                          "MAP_h", "prediction")
```

```{r habituals-rsamodels}


## model parameters
fixed.threshold.hab.posterior <- filter(fixed.hab.params.posterior, B == "fixedThreshold")
fixed.noise.hab.posterior <- filter(fixed.hab.params.posterior, B == "noise")
hab.fixed.threshold.s1opt <- filter(fixed.hab.params.posterior, B == "speakerOptimality")


r2.habituals.rsa.fixed <- compute_r2(
  m.hab.endorse.rsa %>% filter(src == "some_model"),
                                          "MAP_h", "MAP")

mse.habituals.rsa.fixed <- compute_mse(
  m.hab.endorse.rsa %>% filter(src == "some_model"),
                                          "MAP_h", "MAP")


r2.habituals.rsa.uncertain <- compute_r2(
  m.hab.endorse.rsa %>% filter(src == "habituals_model"),
                                          "MAP_h", "MAP")

mse.habituals.rsa.uncertain <- compute_mse(
  m.hab.endorse.rsa %>% filter(src == "habituals_model"),
                                          "MAP_h", "MAP")


habituals.rsa.uncertain.infrequent <- m.hab.endorse.rsa %>% filter(src == "habituals_model", logAnnualRate < 1.1)

r2.hab.rsa.infrequent.n <- length(habituals.rsa.uncertain.infrequent$MAP_h)

r2.habituals.rsa.uncertain.infrequent<- compute_r2(habituals.rsa.uncertain.infrequent,
                                          "MAP_h", "MAP")

mse.habituals.rsa.uncertain.infrequent <- compute_mse(habituals.rsa.uncertain.infrequent,
                                          "MAP_h", "MAP")

example.habituals <- c( 
                       #"wears a watch", "wears socks", 
                       "writes poems", "writes novels",
                      "goes to the movies", "watches space launches",
                      "smokes cigarettes","drinks coffee",
                       "climbs mountains","hikes", "runs"
                       )
```



### Method

#### Participants

We recruited 150 participants from MTurk.
To arrive at this number, we performed a Bayesian precision analysis to determine the minimum sample size necessary to reliably ensure 95\% posterior credible intervals no larger than 0.3 for a parameter whose true value is 0.5 and for which the data is a 2-alternative forced choice.
This analysis revealed a minimum sample size of 50 per item; since participants only completed about one third of the items, we recruited 150 participants.
The experiment took 4 minutes on average and participants were compensated \$0.55 for their work.

#### Materials 

Each event from Expt. 2a was then paired with between two to four frequencies, for which the habitual statement would be evaluated. 
Frequencies were presented in terms of a character performing the action "three times in the past *time interval*".
We chose to always have the character perform the action three times to provide a strong test of a baseline hypothesis that the habitual encodes the person has done the action multiple times in the past.

Different time intervals were chosen for each event in order to maximize the variability of responses within each item. 
Specifically, we used the endorsement model to generate predictions based on the prior elicitation data (Expt. 2a) for each item, and chose between two and four time intervals across which maximal variability was predicted. 
For example, relatively high frequencies were chosen (e.g., time intervals of weeks and months) for items expected to occur rather often (e.g., *runs*); for an item was expected to occur infrequently (e.g., *climbs mountains*), lower frequencies were chosen (e.g., time intervals of years or longer) because the model predicted that much of the variability in endorsement would occur in that range.
In total, `r nrow(m.hab.endorse.rsa %>% distinct(habitual, time_period))` unique items were created by pairing frequencies with events. 
The full list of items and frequencies can be found in Appendix D.

#### Procedure

On each trial, participants were presented with a *past frequency statement* for a given event of the form: "In the past \{week, 2 weeks, month, 5 years, ...\}, *Person did X* 3 times".
For example, "In the past month, Bill smoked cigarettes 3 times".
Participants were asked whether they agreed or disagreed with the corresponding habitual sentence: "Person does X" (e.g.,"Bill smokes cigarettes.").
Participants completed thirty-seven trials, which were composed of the thirty-one items from the prior elicitation task randomly paired with either a male or female character name.
Six of these items were then also paired with a name of the opposite gender (e.g., participants rated both a female character and a male character who drank beer).
These were used for an exploratory analysis on differences in endorsements by gender of the target character.

<!-- The items were the same as in the prior elicitation task (Expt. 2a). -->

### Results 

<!-- The simplest alternative hypothesis for habitual language endorsement is that the degree to which the habitual statement is acceptable is given by the referent frequency (Figure \@ref(fig:habituals-endorsement-figure) top-left).  -->
<!-- Several features in our data argue against this hypothesis. -->
<!-- It is clear that not all habitual statements are strongly assented to, even though it is true that all the events occurred with a non-zero frequency (i.e., they occurred at least 3 times before). -->

Habitual sentences were endorsed for a wide range of frequencies.
When actions are very infrequent (3 times in a 5-year interval), habituals can receive strong agreement (e.g., \emph{writes novels}, \emph{climbs mountains}).
When actions are relatively frequent (e.g., 3 times in a one month interval), habitual sentences can receive less than full endorsement (e.g., \emph{wears socks}, \emph{drinks coffee}). 
In our data, actions completed with a high frequency (3 times in a one week interval) receive at a minimum 75\% endorsement, though there is still variability among them (e.g., between 10-25\% of people disagree that people who wore a watch or wore a bra 3 times in the past week \emph{wears a watch} or \emph{wears a bra} habitually).
In addition, we observe that none of our items receive less than 25\% endorsement (i.e., a maximum of about 75\% of participants disagree with the habitual utterances), reflecting the fact that these statements are not altogether *false* even though the action may be done very rarely.

### Endorsement model comparison


```{r habituals-endorsement-bars, fig.width = 8}
habituals.endorsement.bars <- ggplot(md.hab.long %>%
                                       mutate(src = factor(src, levels = c(
                                         "Human data", "Uncertain semantics",
                                         "Fixed semantics", "Referent frequency",
                                         "Distinctiveness +\n referent frequency"
                                       ))), aes ( y = prediction, ymin = prediction_lower, ymax = prediction_upper,
                  x = time_period, fill = logAnnualRate))+
  #geom_abline(intercept = 0, slope = 1, lty = 3)+
  #geom_linerange(alpha = 0.15)+
  geom_col(position = position_dodge(), color = 'black', width = 0.7)+
  geom_errorbar(alpha = 0.7, width = 0.3)+
  #geom_point(shape = 21, size = 3)+
  #scale_x_continuous(limits = c(-0.01, 1.01), breaks = c(0,  1))+
  scale_y_continuous(limits = c(-0.01, 1.01), breaks = c(0, 1))+
  scale_fill_viridis()+
  #scale_fill_continuous(low = "#2b83ba", high = "#d7191c")+
  coord_fixed()+
  ylab("Habitual endorsement")+
  xlab("Frequency")+
  facet_grid(src~habitual, scales = 'free')+
  theme(strip.text.y = element_text(hjust = 0, angle = 0),#, vjust = 1),
        axis.text.x = element_text(vjust = 1, hjust = 1, angle = 45),
        legend.position = "bottom") +
   guides(fill = F)
```

```{r habitual-endorsment-scatters}
habituals.endorsement.models <- md.hab %>% mutate(src = factor(src, levels = c(
                                      "regression_freq", 
                                      "regression_freq_distinct",
                                      "some_model",
                                      "habituals_model"
                                     ),
                      labels = c(
                                 "Referent frequency", 
                                 "Distinctiveness + \n referent frequency",
                                 "Fixed semantics",
                                 "Uncertain semantics"
                                 ) )) %>%
ggplot(., aes ( x = prediction, xmin = prediction_lower, xmax = prediction_upper,
                  y = MAP_h, ymin = low, ymax = high, fill = logAnnualRate))+
  geom_abline(intercept = 0, slope = 1, lty = 3)+
  geom_linerange(alpha = 0.15)+
  geom_errorbarh(alpha = 0.15)+
  geom_point(shape = 21, size = 3)+
  scale_x_continuous(limits = c(-0.01, 1.01), breaks = c(0,  1))+
  scale_y_continuous(limits = c(-0.01, 1.01), breaks = c(0, 1))+
  #scale_fill_continuous(low = "#2b83ba", high = "#d7191c")+
  scale_fill_viridis()+
  coord_fixed()+
  xlab("Model prediction")+
  ylab("Human endorsement")+
  facet_wrap(~src, nrow = 1, strip.position = "top")+
  guides(fill = guide_colorbar(title = 'log annual rate',
                               ticks = F,
                               title.position="top"))+ 
  theme(strip.text.y = element_blank(),# element_text(angle = 0, hjust= 0),
        legend.direction = 'horizontal',
        #legend.position = c(0.025,0.975),
        #legend.position =c(1,1),
        legend.justification = c(1,1)
        )
```

```{r habituals-model-insets, fig.width = 8, cache = T}
habituals.endorsement.insets <- hab.listener.predictions %>% 
    mutate(Parameter = factor(Parameter, levels = c("state_Prior",
                                                  "state_Posterior"
                                                  ),
                            labels = c("Prevalence Prior",
                                       "Posterior given Habitual")),
         action = factor(action, levels = example.habituals)
                                #"climbs mountains", "hikes", "runs",
                              #"goes to the movies", "smokes cigarettes"
                              ) %>%
  ggplot(., aes( x = value, fill = Parameter, color = Parameter, lty = Parameter, alpha = Parameter ))+
  geom_density(aes(y = ..scaled..), adjust = 1, size = 1)+
  facet_wrap(~action, nrow = 1)+
  # geom_text_repel(data = category.text.labels,
  #                 aes(label = category, x = x , y = y),
  #                 inherit.aes = F, color = "#2b83ba")+
  #scale_fill_manual(values = c("#636363", "#abdda4", "#2b83ba", "#d7191c"))+
  #scale_color_manual(values = c("#636363", "#abdda4", "#2b83ba", "#d7191c"))+
  scale_fill_manual(values = c("#636363", "#d7191c"))+#, "#2b83ba"))+
  scale_color_manual(values = c("#636363", "#d7191c"))+#, "#2b83ba"))+
  scale_alpha_manual(values = c(0.6, 0.4))+#, 0))+
  #scale_linetype_manual(values = c(3, 4, 2, 1))+
  #scale_linetype_manual(values = c(3, 4, 1))+
  #scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
  scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
  xlab("Frequency (log scale)") +
  ylab("Scaled probability density")+
  theme(#legend.position = "bottom", 
        legend.title = element_blank(),
        axis.text.x = element_text(angle = 90, vjust = 1),
        axis.title.y = element_text(hjust = 1))+
      scale_x_continuous(limits = c(-5, 8), 
                     breaks = c(-5, 0, 2.5, 4, 5.9),
                     labels = c("almost never", "annually", "monthly", "weekly", "daily"))
```

```{r habituals-endorsement-figure, fig.width = 16, fig.height = 12.5, fig.cap="Endorsing generalizations about events. A: Endorsements for nine events given different frequencies of action by human participants and four models. B: Human elicited endorsements for ninety-three habitual sentences as a function of the referent (log) frequency of action. Top right: Model fits for all ninety-three habitual sentences by each model. C: Nine example frequency priors and posteriors upon hearing the generalization. These distributions are inferred using both data sources from Expts. 2a & 2b.", cache = F}

hab.legend <- get_legend(habituals.endorsement.models)
fig.hab.endorse.bars <- habituals.endorsement.bars +
  theme(plot.margin = unit(c(6,0,6,0), "pt")) #+ ggtitle("prevalence priors")
fig.hab.endorse.scatters <- habituals.endorsement.models +
  theme(plot.margin = unit(c(6,0,6,0), "pt")) # + theme(legend.position="none") #+ ylab("")
fig.hab.endorse.priors <- habituals.endorsement.insets +
  theme(plot.margin = unit(c(18,2,6,6), "pt"))

#+ 
    #theme(strip.text.y = element_blank())#+ ggtitle("Interpreter model posterior")
#p4 <- s1.simulations.scatter + theme(plot.margin = unit(c(18,0,6,6), "pt"))
  
# fig.gen.endorse.toprow <- plot_grid(fig.gen.endorse.data,
#                                      fig.gen.endorse.models, 
#                                     # align = 'v',
#                                     #  axis = 'l',
#                                     labels = c("A", "B"),
#                                     rel_widths = c(0.9, 1))


#fig.hab.toprow <- 
  plot_grid(fig.hab.endorse.bars, fig.hab.endorse.scatters,
                            # plot_grid(fig.hab.endorse.scatters,hab.legend, nrow = 1, rel_widths = c(0.6,4),
                            #                                 axis = 't', align = 'h'),
                            fig.hab.endorse.priors,
                            labels = c("A", "B", "C"), nrow = 3,
                            axis = 'lr', 
                        align = 'v', 
                            rel_heights = c(3.1, 1.4, 1.2))

# plot_grid(fig.hab.toprow,
#           fig.hab.endorse.priors,
#           labels = c("", "C"), nrow = 2, rel_heights = c(3, 1.3)
#           )




#plot_grid( prow, legend, rel_widths = c(3, .3))

# grid.arrange(fig.habituals.endorsement.vs.freq, 
#              habituals.endorsement.models,
#              habituals.endorsement.insets, ncol = 2,
#              layout_matrix = cbind(c(1,1,3), c(2,2,3)))
```


In an exploratory analysis, we find no differences between endorsements of the habitual of characters with male and female names, and overall, the mean endorsements by gender are strongly correlated $r(`r n.habituals.gendered.analysis`) = `r r.habituals.gendered.analysis`$.
Endorsements are even more highly correlated for the six events we anticipated differences by referent-gender:  $r(`r n.habituals.gendered.subset.analysis`) = `r r.habituals.gendered.subset.analysis`$.
This lack of a difference may be because the felicity of habitual sentences depends on a comparison to individuals of both genders (i.e, in minimal contexts such as this, habituals are evaluated with respect to *other people*, not just other men or other women).
Less interestingly, the lack of a difference may be the result of gender being not very salient in our paradigm, perhaps because the names used were not sufficiently gendered.

We now turn to our model-based analyses to better understand the endorsement data and the contribution of our model.
For all analyses, we collapse across gender of the referent character for endorsement judgments.
Parallel to our analysis of generic language endorsements, we articulate a set of simple regression models and a fixed-threshold alternative to our uncertain-threshold endorsement model.
Analyses which use the prevalence prior distribution $P(p)$ (all models except "referent frequency" regression) use a 50\% mixture of the inferred priors for each gender to construct a single prevalence prior distribution.
Parallel to our analysis of generics model, we model uncertainty in the input measurements by bootstrapping the data for the regression models and constructing joint-inference, Bayesian data analytic models for the information-theoretic endorsement models (fixed-threshold and uncertain-threshold).

#### Referent frequency

To understand the role of frequency in habitual endorsement, we use the frequency supplied to the participants in our experiment as a predictor in a linear model. 
This model predicts the same endorsement level for two actions done with the same rate. 
Obvious counterexamples exist in our data set:
While participants are willing to endorse that a person "... climbs mountains" having done it 3 times in the past *year*, they are less willing to say that a person "... hikes" and not willing to say that a person "... runs".
Some actions done with a relative high rate (e.g., 3 times in a month) do not receive full endorsement (e.g., *smokes cigarettes*; Figure \@ref(fig:habituals-endorsement-figure)A).
Overall, the referent-frequency (in log-scale) predicts only a fraction of the variability in responses ($r^2(`r r2.hab.n`) = `r r2.habituls.regression.freq`$; MSE=$`r mse.habituls.regression.freq`$). 
In addition, for actions that are done on the time scale of years or longer (lower median of frequency), referent frequency no longer explains endorsements ($r^2(`r r2.hab.infrequent.n`) = `r r2.habituls.infrequent.regression.freq`$; MSE =$`r mse.habituls.infrequent.regression.freq`$).
The prevalence baseline does appreciably worse in this data set in comparison to the generics data set (Case Study 1) because we were able to independently manipulate the referent-frequency separate from the prevalence priors, which we could not do for generics about familiar categories. 

<!-- designed the stimulus set to be critical of a referent-frequency model. \ndg{i think it's not so much that we designed the stimuli to be critical, as that we were able to independently manipulate the referent-frequency from the prior, which we couldn't do for generics?} -->
<!-- An account purely based on the referent frequency does not explain endorsements in this data set. -->
<!-- Instead, we pursue an explanation where prior beliefs interact with an underspecified literal semantics to produce the pattern of habitual endorsement observed. -->

<!-- Before comparing models, we perform an exploratory analysis on a subset of our data, exploring whether or not endorsements for items whose priors differed by gender will also differ. -->

<!-- NOTE: Alternatively: We could not discuss this analysis (or, in a footnote). Since participants judged both genders for these items, we could only analyze their first gender and drop their second, for consistency with the other items. -->

<!-- For our six items that showed substantial variability in the elicited priors between genders, we explore differences in endorsements.  -->
<!-- Differences in endorsements could suggest that the *comparison class* of entities in the prior distribution used in our model might be gender-specific (i.e., when evaluating "Mary drinks beer." participants implicitly consider the distribution of frequencies for other women and not other men). -->
<!-- Endorsement probabilities and 95\% credible intervals for these six items at each time interavl for each gender are showed in Figure \ref{fig:habituals-endorsement-gender-exploration-scatter}. -->
<!-- We see no appreciable differences in endorsements for the two genders.  -->
<!-- This suggests that, at least the minimal experimental contexts we are exploring, the comparison class is not different for female vs. male target characters. -->
<!-- For all remaining analyses, we collapse across character gender when analyzing endorsements. -->

<!-- This is unlikely to be the case based on , which shows the correspondence between the frequency of the event (transformed to a log times-per-year scale) and the human-judged felicity of the corresponding habitual sentence.  -->

#### Distinctiveness and referent frequency

In our empirically elicited priors, items differ the proportion of people who have done the action before (the *mixture parameter* of the mixture model; Figure \@ref(fig:habituals-prior-figure)A x-axis).
This mixture parameter is a major contributor to the mean of the prevalence prior distribution, and thus is relates to the *cue validity* of a particular feature for a particular individual (see Appendix A for a derivation of the relationship between the prevalence prior and cue validity).
Thus, we take this parameter as an index of the *distinctiveness* of the action, analogous to *cue validity* in the case study of generic language.
We construct a regression model that treats endorsement as a linear combination of frequency and distinctiveness information. 
We use participants' responses to the question about the mixture parameter $\phi$ (i.e., the proportion of people who have done the action before) as well as frequency given to participants to predict habitual endorsement. 

```{r habituals-distinctiveness-counterexamples}

format_endorsment_ci <- function(
  df, estimate = "MAP_h", low = "low",
  high = "high", sigfigs = 2){
  return(paste(
    round(df[[1,estimate]], sigfigs),
    " [",
    round(df[[1,low]], sigfigs),
    ", ",
    round(df[[1,high]], sigfigs),
    "]", sep = ""))
}

movies.year.data <- format_endorsment_ci(filter(d.hab.endorse.regression.freq.mix, habitual == "goes to the movies", time_period == "year"))

movies.year.regression.distinct <- format_endorsment_ci(filter(d.hab.endorse.regression.freq.mix, habitual == "goes to the movies",
                                                        time_period == "year"),
                                                        estimate = "prediction",
                                                        low = "prediction_lower",
                                                        high = "prediction_upper")

banjo.2years.data <- format_endorsment_ci(filter(d.hab.endorse.regression.freq.mix, habitual == "plays the banjo", time_period == "2 years"))

banjo.2years.regression.distinct <- format_endorsment_ci(filter(d.hab.endorse.regression.freq.mix, habitual == "plays the banjo",
                                                        time_period == "2 years"),
                                                        estimate = "prediction",
                                                        low = "prediction_lower",
                                                        high = "prediction_upper")

```

This model is able to explain more of the variance in endorsements ($r^2(`r r2.hab.n`) = `r r2.habituls.regression.freq.distinct`$; MSE=$`r mse.habituls.regression.freq.distinct`$). 
It can differentiate events done with the same frequency (e.g., *writing poems* vs. *novels*, 3 times in the past 5 years) by increasing endorsement of the more rare action (*novels*). 
Still, this model fails to capture fine-grained differences in endorsement.
For example, going to the movies is a relatively nondistinctive action (many people do it) and going three times in a year is not very frequent, and yet people still strongly endorse the habitual ($`r movies.year.data`$), while this regression model predicts quite lower judgments ($`r movies.year.regression.distinct`$).
On the other hand, playing the banjo three times in the past two years is not strong evidence for the habitual, according to participants ($`r banjo.2years.data`$).
Nevertheless, because playing the banjo is a distinct skill, the regression model wants to endorse the habitual strongly in this case ($`r banjo.2years.regression.distinct`$).

#### Fixed-threshold endorsement model

We next examine an information-theoretic endorsement model based on a fixed-threshold semantics.
A fixed-threshold model commits the habitual to conveying literally that a person *does the action with some non-zero frequency*, and the threshold on frequency is the same for all actions.
As in Case Study 1, we incorporate this model into a Bayesian joint-inference model to infer the fixed-threshold and simultaneously predict both the priors data and the endorsement data (for more details on model implementation, see Appendix C).
We assume the referent-prevalence $p$ being conveyed by the endorsement model (Eq. \ref{eq:S1}) is the frequency provided to participants (e.g., 3 times in the past year).
Additionally, to account for statements that would be literally false under this model (frequencies that fall below the fixed threshold), we include an additional noise parameter, as we did for the fixed-threshold model in Case Study 1. 
To learn about the credible values of the model's parameters and generate predictions given those inferred parameter values, we collected 2 MCMC chains of 100,000 iterations, discarding the first 50,000 iterations for burn in.

The data analytic model infers that a low threshold is likely: the Maximum A-Posteriori threshold and 95\% credible interval in units of number of times per year is `r round(exp(fixed.threshold.hab.posterior[[1,"MAP"]]), 2)` [`r round(exp(fixed.threshold.hab.posterior[[1,"cred_lower"]]), 2)`,
`r round(exp(fixed.threshold.hab.posterior[[1,"cred_upper"]]), 2)`].
Compare this with the lowest referent-frequency used in our data set: $0.6$ times per year (3 times every 5 years).
Thus, all of the utterances evaluated under this fixed-threshold model were literally true.
As a result, the lowest endorsement this model can apply to an utterance is 0.5 (since both the habitual and silence are always true).
The fixed-threshold model exhibits a small dynamic range of endorsements, similar to the referent-frequency model (Figure\ \@ref(fig:habituals-endorsement-figure)B).

The fixed-threshold habitual updates the interpreter model's prior beliefs differentially depending on the item.
For instance, because of the distinctiveness of *climbs mountains*, the fixed-threshold endorsement model fully endorses the habitual ("Mary climbs mountains") even at a low frequency (Figure \@ref(fig:habituals-endorsement-figure)A).
However, the model cannot differentiate among different frequencies of doing the same action.
It is equally true that a person does an action with non-zero frequency for any frequency greater than zero, analogous to how "Some dogs are friendly" is equally true whether 20\% or 50\% or 80\% of dogs are friendly.

Overall, the fixed-threshold model is able to predict only a fraction of the variance in human endorsements ($r^2(`r r2.hab.n`) = `r r2.habituals.rsa.fixed`$; MSE=$`r mse.habituals.rsa.fixed`$).
The model does this by inferring that `r 100*round(fixed.noise.hab.posterior[[1,"MAP"]], 2)`% [`r 100*round(fixed.noise.hab.posterior[[1,"cred_lower"]], 2)`,
`r 100*round(fixed.noise.hab.posterior[[1,"cred_upper"]], 2)`] of the data is noise and that the speaker optimality parameter is `r round(hab.fixed.threshold.s1opt[[1,"MAP"]], 2)` [`r round(hab.fixed.threshold.s1opt[[1,"cred_lower"]], 2)`,
`r round(hab.fixed.threshold.s1opt[[1,"cred_upper"]], 2)`].

<!-- That is, the best way this model has to deal with the endorsement data is to say there is some arbitrary threshold and a large percentage of deviations are due to noise.  -->
<!-- Thus, our identical data analytic strategy paired with a fixed-threshold model is unable to predict the range of endorsements ($r^2(`r r2.hab.n`) = `r r2.habituals.rsa.fixed`$; MSE=$`r mse.habituals.rsa.fixed`$).^[ -->
<!--   An alternative formulation of this model could fix the threshold to be the lowest possible threshold (analagous to the quantifier "some").  -->
<!--   This model would explain even less data than the current model (if it was better, this situation would have been inferred). -->
<!--   That model would have trouble differentiating between different frequencies for the same habitual statement (as the current model does), analogous to how "some" is equally true of 20\% or 50\%. -->
<!-- ] -->

<!-- The fixed-threshold model is ill-defined for this data set. -->
<!-- The data-analytic model infers that `r 100*round(fixed.noise.hab.posterior[[1,"MAP"]], 2)`% [`r 100*round(fixed.noise.hab.posterior[[1,"cred_lower"]], 2)`, -->
<!-- `r 100*round(fixed.noise.hab.posterior[[1,"cred_upper"]], 2)`] of the data is noise. -->
<!-- The inferred (fixed) threshold in units of number of times per year is `r round(exp(fixed.threshold.hab.posterior[[1,"MAP"]]), 2)` [`r round(exp(fixed.threshold.hab.posterior[[1,"cred_lower"]]), 2)`, -->
<!-- `r round(exp(fixed.threshold.hab.posterior[[1,"cred_upper"]]), 2)`].  -->
<!-- That is, the best way this model has to deal with the endorsement data is to say there is some arbitrary threshold and a large percentage of deviations are due to noise.  -->
<!-- Thus, our identical data analytic strategy paired with a fixed-threshold model is unable to predict the range of endorsements ($r^2(`r r2.hab.n`) = `r r2.habituals.rsa.fixed`$; MSE=$`r mse.habituals.rsa.fixed`$).^[ -->
<!--   An alternative formulation of this model could fix the threshold to be the lowest possible threshold (analagous to the quantifier "some").  -->
<!--   This model would explain even less data than the current model (if it was better, this situation would have been inferred). -->
<!--   That model would have trouble differentiating between different frequencies for the same habitual statement (as the current model does), analogous to how "some" is equally true of 20\% or 50\%. -->
<!-- ] -->




<!-- statements (Figure \ref{fig:habituals-endorsement-figure}, top-right: upper-left facet). -->
<!-- The model exhibits a small dynamic range of endorsement, because the two options for the endorsement model (silence or a very low, fixed thresold) are only slightly different with respect to information conveyed to the interpreter. -->
<!-- Another shortcoming of this fixed threshold is illuminated by our manipulation of referent-frequency. -->
<!-- With a fixed threshold, the difference between the interpreter's prior and posterior distribution is  (whatever is below the threshold).  -->

<!-- Thus, this model makes the same predictions for different target frequencies in the same event. -->
<!-- For example, the model endorses somebody who hikes three times every two years the same as somebody who hikes three times every month.  -->

#### Uncertain-threshold endorsement model

We used the same data analytic approach for the uncertain threshold endorsement model and performed the same Bayesian statistical inference over the model to learn about its parameters and predictions. 
Again, this model has two fewer parameters than the fixed-threshold model (no fixed-threshold parameter and no extrinsic noise process).
As shown in Figure\ \@ref(fig:habituals-endorsement-figure)B, the uncertain-threshold endorsement model does a good job of accounting for the variability in responses ($r^2(`r r2.hab.n`) = `r r2.habituals.rsa.uncertain`$; MSE=$`r mse.habituals.rsa.uncertain`$), including actions done on the time scale of years or more  ($r^2(`r r2.hab.rsa.infrequent.n`) = `r r2.habituals.rsa.uncertain.infrequent`$; MSE=$`r mse.habituals.rsa.uncertain.infrequent`$).

### Discussion

Habitual language exhibits context-sensitivity directly parallel to that of generic language (Case Study 1).
Habituals are endorsed for a wide range of frequencies, but show systematic patterns relative to the prior distribution of frequencies, as formalized by the uncertain-threshold model.
Again, we articulated a number of alternative models and found that only the underspecified threshold model was able to explain the variability in endorsements. 

In this case study, we manipulated rather than measured the referent frequency (e.g., the frequency with which a person drinks coffee).
By manipulating the target frequency, we have shown that it is causally related to habitual endorsements in the way predicted by our model (and in a way that a fixed-threshold model cannot account for). 
The relationship is not linear (or log-linear), however; habitual endorsements vary in complex ways that reflect interpreters' prior knowledge about the event in question. 

Figure\ \@ref(fig:habituals-endorsement-figure)C provides insight into how the uncertain-threshold model is able to match human judgments. 
The endorsement model simulates how an interpreter would under the habitual sentence.
A habitual is interpreted relative the prior distribution over frequencies, and the comparison between the frequency implied by the habitual vs. staying silent results in different frequencies at which the generalization is good to assert. 
Climbing mountains three times in the past year is good evidence that you *climb mountains*; going for a hike three times in the past year is less convincing that you *hike*; and if you went for a run three times in the past year, you not a person who *runs*.
Only the uncertain-threshold model is able to draw these subtle distinctions.

In Expt. 2b, participants were given a statement about how often a person has done the action in the past and asked to judge the corresponding habitual statement.
This design potentially confounds an important distinction for the language of generalization: Does the prevalence communicated by a generalization indicate an objective, past frequency or a subjective, future expectation?
In Expt. 2c, we investigate this question by teasing apart *past* from *predictive frequency* and measuring its influence on habitual endorsement.

<!-- In Expt. 2b, we manipulated the frequency with which a person did an action in the past (e.g., three times in the past week vs. in the past year) for a wide range of events (e.g., drinks coffee vs. beer) and measured endorsements for the corresponding habitual statements. -->



<!-- The regression model that uses the distinctiveness of the action (as measured in Expt. 2a) and the frequency of action is the best alternative model to explain endorsements.  -->
<!-- This model is not a predictive model, however; rather, it is a re-description of the data and helps us understand what the *predictive* endorsement model is doing.  -->


<!-- People are used to learning new information about others, and thus habitual language about people doing actions is particularly amenable to this kind of manipulation.  -->



<!-- The speaker model endorses the statement when the observed frequency is relatively high, compared to the prior distribution over people doing the action.  -->

<!-- As another potential alternative hypothesis, we formulate a model of a speaker who judges the felicity of the habitual utterance based on simple summary statistics of the prior, such as the mean and variance. -->

<!-- Only our pragmatic speaker model who reasons about a listener is able to capture the quantitative variability in our data. -->


<!-- \begin{figure*}[t] -->
<!-- \centering -->
<!--   \includegraphics[width=\textwidth]{figs/expt3-4-scatters-camera.pdf} -->
<!--   \caption{Left: Predicted log frequency as a function of past log frequency given to the participant (Expt. 3a; CIs suppressed and jitter added for visual clarity). -->
<!--   Middle: Human endorsements of habitual sentences (Expt. 3b) vs. Predicted log frequency (Expt. 3a), with data for corresponding items from Expt. 2 (assumed to have the same predictive log frequency as baseline).  -->
<!--   Right: Endorsements (Expt. 3b) vs. Speaker $S_2$ model predictions using empirically elicited predictive frequencies (Expt. 3a).} -->
<!--   \label{fig:tj3} -->
<!--   \vspace{-7pt} -->
<!-- \end{figure*} -->


## Experiment 2c: What is prevalence?

While past frequency is often a good indicator of future tendency, the future is under no obligation to mimic the past.
Does habitual language communicate probabilities in terms of past frequency or future expectations?
On one hand, speakers can only be certain about what has happened in the past.
On the other hand, it is important for speakers to be able to convey their predictions of what they believe will be the case in the future.

People can change their behavior abruptly due to a variety outside events (e.g., developing an allergy) or intend to do an action without actually completing it (e.g., by making a resolution). 
We introduce these causal events into our experimental paradigm to measure their influence on endorsement.
To provide the appropriate model-based analysis, participants in one condition make a prediction about the future (*predictive frequency*).
In another condition, participants decide whether or not to endorse the habitual sentence (as in Expt. 2b).
<!-- In Expt. 2d, we examine endorsements of the habitual sentence (e.g., \emph{John smokes cigarettes.}; \emph{Susan eats peanut butter.}) in the presence of these causal modifiers. -->
We then compare two uncertain-thresholds models: one which uses participants' ratings of *predictive frequency* as the referent prevalence and one which uses the *past frequency* (as was done for Expt. 2b).
In addition, we compare to a baseline linear model that uses only the *predictive frequency* (no priors) to model endorsement. 

```{r loadCase2PredictiveData, cache =F}
load("cached_results/case2_predictive_data.RData")
#d.hab.predictive.endorsment.summary, d.hab.predictive.summary, n.subj.hab.pred.endorse, ave.seconds.hab.pred.endorse,n.subj.hab.pred, ave.seconds.hab.pred, d.hab.predictive.endorsment.condition.summary, lmer.rs.hab.predictive.summary
```


```{r, cache = F}
r.hab.pred.past.baseline.freq <- round(with(d.hab.predictive.summary %>% filter(condition == "baseline"), cor(logAnnualPastRate, mean)), 3)
n.hab.pred.past.baseline.freq <- length(filter(d.hab.predictive.summary, condition == "baseline")$mean)


r2.habituals.predictiveFreq <- compute_r2(d.hab.predictive.endorsment.summary,
                                          "logAnnualFutureRate", "MAP_h")
mse.habituals.predictiveFreq <- compute_mse(d.hab.predictive.endorsment.summary,
                                          "logAnnualFutureRate", "MAP_h")

d.hab.predictive.endorsment.baseline.summary <- d.hab.predictive.endorsment.condition.summary %>%
  filter(condition == 'baseline')

d.hab.predictive.endorsment.preventative.summary <- d.hab.predictive.endorsment.condition.summary %>%
  filter(condition == 'preventative')

d.hab.predictive.endorsment.enabling.summary <- d.hab.predictive.endorsment.condition.summary %>%
  filter(condition == 'enabling')
```


```{r fighabfreq_predictive_vs_past, cache = F}
fig.hab.freq.predictive.vs.past <- ggplot(d.hab.predictive.summary %>%
                                            ungroup() %>%
                                            mutate(condition = factor(condition,
                                                                      levels = c("preventative", "baseline", "enabling"))) %>%
                                            mutate(src = 'Manipulation check'),
       aes(x=logAnnualPastRate,
                              y = mean,
                              ymin = ci_lower, ymax = ci_upper,
                              fill = condition, shape=condition))+
  geom_jitter(position = position_jitter(width = .13), size = 4)+
  scale_shape_manual(values=c(21,22,23))+
  geom_abline(intercept = 0, slope = 1, lty =3, color = 'black')+
  # #geom_errorbar(width=0.1)+
  scale_x_continuous(limits = c(-3,6),
                     breaks = c(-3, 0, 2.5, 4, 5.9),
                     labels = c("almost never", "annually", "monthly", "weekly", "daily"))+
  scale_y_continuous(limits = c(-3,6),
                     breaks = c(-3, 0, 2.5, 4, 5.9),
                     labels = c("almost never", "annually", "monthly", "weekly", "daily"))+  coord_fixed()+
  facet_wrap(~src)+
  guides(fill=F, shape=F)+
  scale_fill_brewer(type = "seq", palette = 2)+
  xlab("Past frequency")+
  ylab("Predicted frequency")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
        axis.title.x = element_text(vjust = 1),
        axis.title.y = element_text(vjust = 1))
```


```{r habituals-predictive-endorsement, cache = F, eval = F}

fig.hab.endorse.vs.predfreq <- d.hab.predictive.endorsment.summary %>%
  ggplot(., aes(x=logAnnualFutureRate, y=MAP_h, 
                 fill=condition, shape=condition,
                 xmin = future_lower, xmax=future_upper,
                 ymin=low, ymax = high))+
  geom_errorbar(alpha=0.8)+
  geom_errorbarh(alpha=0.8)+
  scale_shape_manual(values=c(21,22,23,24))+
  geom_point(alpha = 1, size = 4)+
  xlab("Predicted frequency (log scale)")+
  ylab("Human endorsement")+
  scale_x_continuous(limits = c(-4.2,7.2), 
                     breaks = c(-3, 0, 2.5, 4, 5.9),
                     labels = c("almost never", "annually", "monthly", "weekly", "daily"))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  coord_fixed(ratio=11.4)+
  scale_fill_solarized()+
  guides(fill = F, shape = F)+
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
    legend.position="bottom",
    legend.direction="horizontal"
    )
```

```{r habituals-predictive-rsa, cache = T, eval=T}
n_chains <- 3
# n_samples <- 500000
# burn <- n_samples / 2
# lg <- 150
model_prefix <- "results-habituals-predictive-jointModel-S1-smtncs_habitual-"
model_prefix <- "results-habituals-predictive-meanPredictive-min-5-jointModel-S1-smtncs_habitual-"
n_samples <- 100000
burn <- n_samples / 2
lg <- 50


m.hab.pred.samp <- data.frame()
m.hab.past.samp <- data.frame()


# for (i in seq(1, n_chains)){
#   mi <- fread(paste(project.path,  "models/habituals/results/",
#                     model_prefix, "targetH_predictive-",
#                     n_samples, "_burn", burn, "_lag", lg, "_chain", i, ".csv", sep = ""))
#   m.hab.pred.samp <- bind_rows(m.hab.pred.samp, mi %>% mutate(chain = i))

  # mi.past <- fread(paste(project.path,  "models/habituals/results/",
  #                   model_prefix, "targetH_past-",
  #                   n_samples, "_burn", burn, "_lag", lg, "_chain", i, ".csv", sep = ""))
  # m.hab.past.samp <- bind_rows(m.hab.past.samp, mi.past %>% mutate(chain = i))

#}
# 
# 
# # 
# save(m.hab.pred.samp,
#      file = paste(project.path,  "models/habituals/results/", model_prefix, "predictive-",
#                     n_samples, "_burn", burn, "_lag", lg, "_",n_chains , "chains.RData", sep = ""))
# # # 
# save(m.hab.past.samp,
#      file = paste(project.path,  "models/habituals/results/", model_prefix, "past-",
#                     n_samples, "_burn", burn, "_lag", lg, "_",n_chains , "chains.RData", sep = ""))


load(file = paste(project.path,  "models/habituals/results/results-habituals-predictive-jointModel-S1-smtncs_habitual-predictive-100000_burn50000_lag50_3chains.RData", sep = ""))


load(file = paste(project.path,  "models/habituals/results/results-habituals-predictive-jointModel-S1-smtncs_habitual-past-100000_burn50000_lag20_3chains.RData", sep = ""))



m.hab.pastmodel.endorsement <- m.hab.past.samp %>%
  filter(type == 'predictive', C == "endorsement") %>%
  rename(item = B, condition = D, binned_freq = E) %>%
  group_by(item, condition) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))

m.hab.predictive.endorsement <- m.hab.pred.samp %>%
  filter(type == 'predictive', C == "endorsement") %>%
  rename(item = B, condition = D, binned_freq = E) %>%
  group_by(item, condition) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = hdi_upper(val),
            cred_lower = hdi_lower(val))

m.hab.predictive.endorse.rsa <- bind_rows(
  left_join(
    d.hab.predictive.endorsment.summary,
    m.hab.predictive.endorsement
  ) %>%
    mutate(src = "predictive_model"),
  left_join(
    d.hab.predictive.endorsment.summary,
    m.hab.pastmodel.endorsement
  ) %>%
    mutate(src = "past_model")
)

r2.habituals.rsa.past <- compute_r2(
  m.hab.predictive.endorse.rsa %>% filter(src == "past_model"),
                                          "MAP_h", "MAP")

mse.habituals.rsa.past <- compute_mse(
  m.hab.predictive.endorse.rsa %>% filter(src == "past_model"),
                                          "MAP_h", "MAP")


r2.habituals.rsa.predictive <- compute_r2(
  m.hab.predictive.endorse.rsa %>% filter(src == "predictive_model"),
                                          "MAP_h", "MAP")

mse.habituals.rsa.predictive <- compute_mse(
  m.hab.predictive.endorse.rsa %>% filter(src == "predictive_model"),
                                          "MAP_h", "MAP")

r2.hab.rsa.predictive.n <- length(d.hab.predictive.endorsment.summary$MAP_h)

habituals.predictive.past.model <- m.hab.predictive.endorse.rsa %>%
  mutate(src = factor(src, levels = c( 
                                      "past_model",
                                      "predictive_model"
                                     ),
                      labels = c(
                                 "Past-frequency endorsement model",
                                 "Predictive-frequency endorsement model"
                                 
                                 ))) %>%
  filter(src == "Past-frequency endorsement model") %>%
  ggplot(., aes ( x = MAP, xmin = cred_lower, xmax = cred_upper,
                  y = MAP_h, ymin = low, ymax = high, fill = condition,
                  shape = condition))+
  geom_abline(intercept = 0, slope = 1, lty = 3)+
  geom_linerange(alpha = 0.7)+
  geom_errorbarh(alpha = 0.7)+
  geom_point(size = 4)+
  scale_shape_manual(values=c(21,22,23,24))+
  scale_x_continuous(limits = c(-0.01, 1.01), breaks = c(0,  1))+
  scale_y_continuous(limits = c(-0.01, 1.01), breaks = c(0, 1))+
  #scale_fill_continuous(low = "#2b83ba", high = "#d7191c")+
  coord_fixed()+
  xlab("Model prediction")+
  ylab("Human habitual endorsement")+
  facet_wrap(~src, nrow = 1)+
  guides(fill = F)+
  #theme(legend.position = "bottom")+
  scale_fill_solarized()

habituals.predictive.predictive.model <- m.hab.predictive.endorse.rsa %>%
  mutate(src = factor(src, levels = c( 
                                      "past_model",
                                      "predictive_model"
                                     ),
                      labels = c(
                                 "Past-frequency endorsement model",
                                 "Predictive-frequency endorsement model"
                                
                                 
                                 ))) %>%
  #filter(src == "Predictive-frequency endorsement model") %>%
  ggplot(., aes ( x = MAP, xmin = cred_lower, xmax = cred_upper,
                  y = MAP_h, ymin = low, ymax = high, fill = condition, shape = condition))+
  geom_abline(intercept = 0, slope = 1, lty = 3)+
  geom_linerange(alpha = 0.7)+
  geom_errorbarh(alpha = 0.7)+
  geom_point(size = 4)+
  scale_shape_manual(values=c(21,22,23,24))+
  scale_x_continuous(limits = c(-0.01, 1.01), breaks = c(0,  1))+
  scale_y_continuous(limits = c(-0.01, 1.01), breaks = c(0, 1))+
  #scale_fill_continuous(low = "#2b83ba", high = "#d7191c")+
  coord_fixed()+
  xlab("\n Model prediction")+
  ylab("Human habitual endorsement")+
  facet_wrap(~src, nrow = 1)+
  #guides(fill = F)+
  #theme(legend.position = "bottom")+
  scale_fill_solarized()
#habituals.predictive.predictive.model
```

```{r habituals-predictive-endorsement-bars}
pred.items.of.interest <- c("climbs mountains","steals cars", "steals chewing gum",
                            "writes novels", "hikes","plays the banjo",
                            "smokes marijuana","runs","watches space launches",
                            "smokes cigarettes","drinks coffee", "writes poems")

habituals.endorsement.pred.bars <- m.hab.predictive.endorse.allModels %>%
  mutate(condition = factor(condition, levels = c("preventative", "baseline", "enabling")),
         # src =factor(src , 
         #             levels = c("human data","predictive_model", "regression_pastfreq",
         #                              "regression_futurefreq","past_model"),
         #             labels= c("Human data", "Uncertain semantics  \n(Predictive frequency)", "regression_pastfreq",
         #                       "Regression              \n(Predictive frequency)", "Uncertain semantics\n(Past frequency)      ")),
         src =factor(src , 
                     levels = c("regression_pastfreq",
                                "regression_futurefreq",
                                "past_model", 
                                "predictive_model","human data"),
                     labels= c(
                               "Past frequency",
                                "Predictive frequency",
                                "Uncertain semantics\n(Past frequency)",
                               "Uncertain semantics \n(Predictive frequency)", 
                               "Human data")),
         hab_time = gsub("-","\n(", hab_time),
         hab_time = paste(hab_time, ")", sep = ""),
         hab_time = factor(hab_time, 
                           levels = c("climbs mountains\n(5 years)","hikes\n(year)","runs\n(2 months)",
                            "watches space launches\n(2 years)", 
                                      "steals cars\n(5 years)",
                                      "steals chewing gum\n(5 years)",
                            "writes novels\n(5 years)", 
                            "writes poems\n(year)","plays the banjo\n(year)",
                            "smokes marijuana\n(year)",
                            "smokes cigarettes\n(month)",
                            "drinks coffee\n(month)"))) %>%
  filter(item %in% pred.items.of.interest) %>%
ggplot(., 
       aes (y = MAP, ymin = cred_lower, ymax = cred_upper, 
            x = src, fill = condition))+
  #geom_abline(intercept = 0, slope = 1, lty = 3)+
  #geom_linerange(alpha = 0.15)+
  geom_col(position = position_dodge(0.7), color = 'black', width = 0.7)+
  geom_linerange(alpha = 0.7, size = 1, position = position_dodge(0.7))+
  #geom_point(shape = 21, size = 3)+
  #scale_x_continuous(limits = c(-0.01, 1.01), breaks = c(0,  1))+
  scale_y_continuous(limits = c(-0.01, 1.01), breaks = c(0, 1))+
  scale_fill_brewer(type = "seq", palette = 2)+
  #scale_fill_continuous(low = "#2b83ba", high = "#d7191c")+
  #coord_fixed()+
  ylab("Habitual endorsement")+
  xlab("Model")+
  #facet_grid(src~., scales = 'free')+
  facet_wrap(~hab_time, nrow = 3)+
  theme(strip.text.y = element_text(hjust = 0, angle = 0),#, vjust = 1),
        axis.text.x = element_text(vjust = 1, hjust = 1, angle = 45,
                                   lineheight = 0.7),
        legend.position = "bottom")
```

```{r habituals-predictive-endorsement-scatters, cache = F}
habituals.endorsement.pred.scatters <- m.hab.predictive.endorse.allModels %>%
  mutate(condition = factor(condition, levels = c("preventative", "baseline", "enabling")),
         src =factor(src , levels = c("human data", "regression_pastfreq",
                                     "past_model", "regression_futurefreq",
                                      "predictive_model"),
                     labels= c("Human data","regression_pastfreq", 
                                "Uncertain semantics\n(Past)", "Linear model\n(Predictive frequency)","Uncertain semantics\n(Predictive)"))) %>%
  filter(!(src %in% c("Human data", "regression_pastfreq"))) %>%
ggplot(., 
       aes (x = MAP, xmin = cred_lower, xmax = cred_upper, 
            y = MAP_h, ymin= low, ymax = high, fill = condition,
            shape = condition))+
  geom_abline(intercept = 0, slope = 1, lty = 3)+
  #geom_linerange(alpha = 0.15)+
  #geom_col(position = position_dodge(0.7), color = 'black', width = 0.7)+
  geom_linerange(alpha = 0.7, size = 1)+
  geom_errorbarh()+
  geom_point(size = 3)+
  scale_x_continuous(limits = c(-0.01, 1.01), breaks = c(0,  1))+
  scale_shape_manual(values=c(21,22,23))+
  scale_y_continuous(limits = c(-0.01, 1.01), breaks = c(0, 1))+
  scale_fill_brewer(type = "seq", palette = 2)+
  #scale_fill_continuous(low = "#2b83ba", high = "#d7191c")+
  coord_fixed()+
  ylab("Habitual endorsement")+
  xlab("Model prediction")+
  facet_grid(.~src)+
  theme(legend.position = "right")
```


```{r habituals-predictive-figure, fig.cap="A: Example empirical and predicted endorsements for habituals in three conditions. B: Predicted frequency as a function of past frequency and condition manipulation (enabling, preventative, and baseline). C: Model fits for the uncertain threshold model using past frequency, linear model based on future frequency, and uncertain threshold model using future frequency.", fig.width = 12,  fig.asp = 0.7, cache=F}
plot_grid(
  habituals.endorsement.pred.bars +
    theme(plot.margin = unit(c(6,6,6,6), "pt"), legend.position="none",
          axis.title.x = element_blank()),
  plot_grid(
    fig.hab.freq.predictive.vs.past + 
     theme(plot.margin = unit(c(0,6,0,0), "pt"), legend.position="none",
           axis.text = element_text(size = 8),
           axis.title = element_text(size = 9)),
    habituals.endorsement.pred.scatters +
      theme(plot.margin = unit(c(0,0,12,12), "pt")),
    labels = c("B", "C"),
    nrow = 1,  axis = 'tblr', rel_widths = c(1, 2.3)), 
  labels = c("A", ""), 
  nrow = 2, align = 'v', axis = 'lr', rel_heights = c(3.1, 2))
```


### Method

#### Participants

We recruited `r n.subj.hab.pred + n.subj.hab.pred.endorse` participants from MTurk, using the same criterion as Expt. 2b.
`r n.subj.hab.pred` were assigned to the *predictive frequency* condition and `r n.subj.hab.pred.endorse` were assigned to the *habitual endorsement* condition.
The experiment took on average `r round(ave.seconds.hab.pred$total/60,1)` minutes (*predictive frequency*) and `r round(ave.seconds.hab.pred.endorse$total / 60)` minutes (*habitual endorsement*).
Participants were compensated \$0.40.

#### Materials

The events used were a subset of those used in Expts. 2a \& b (21 of the original 31).
In addition, we crafted statements that were intended to either increase the frequency (*enabling*; e.g., "Yesterday, Bill bought a pack of cigarettes.") or decrease the frequency (*preventative*; "Yesterday, Bill quit smoking.") of the event in the future.
In order to increase the potential variability of responses across the experimental conditions, participants only saw the frequencies that led to the most intermediate endorsement of the habitual in Expt. 2b.
We did not include separate trials for both male and female names for the select items we did in Expt. 2b, since we saw no differences in their endorsements of the habitual.
See Appendix D for a full list of the items and frequencies used, as well as the enabling and preventative information.

#### Procedure

The procedure was identical to Expt. 2b except for the inclusion of a second sentence on a subset of trials (*preventative* and *enabling* trials).
On all trials, participants were presented with a \emph{past frequency sentence} (same as Expt. 2b).
Additionally, trials either included \emph{preventative} information, \emph{enabling} information, or no additional information (identical to Expt. 2b), in equal proportions.
See Table 1 for example trials.

```{r table-habitual-predictive-items, results="asis"}
hab.pred.item.table <- xtable::xtable(data.frame(
  Habitual = c("John smokes cigarettes.", "Tina volunteers at soup kitchens."),
  Baseline = c("In the past month, John smoked cigarettes 3 times.",
               "In the past five years, Tina volunteered for soup kitchens 3 times."),
  Preventative = c("In the past month, John smoked cigarettes 3 times. Yesterday, John quit smoking cigarettes.",
                   
               "In the past five years, Tina volunteered for soup kitchens 3 times. Yesterday, Tina grew disillusioned with the soup kitchen system and wants nothing to do with it anymore."),
  Enabling = c("In the past month, John smoked cigarettes 3 times. Yesterday, John wanted a smoke and bought a pack of cigarettes.",
               
               "In the past five years, Tina volunteered for soup kitchens 3 times. Yesterday, Tina researched a new soup kitchen in the area and is going to volunteer with them.")
), align=c('p{1in}',' |p{1in}|', 'p{1.75in}| ','p{1.75in}|','p{1.75in} |'),
  caption = "Example stimuli used in Expt. 2c.")
bold <- function(x){
paste0('{\\bfseries ', x, '}') }

print(hab.pred.item.table,
    type="latex", 
      comment = F,
       #table.placement = "h", 
      #tabular.environment = "longtable",
      hline.after=-1:nrow(hab.pred.item.table),
      size="\\fontsize{9pt}{10pt}\\selectfont", 
      include.rownames=FALSE,
      sanitize.colnames.function = bold
      #add.to.row = addtorow
      )
```


In the *predictive frequency* condition, participants were asked ``In the next *time interval*, how many times do you think *person does action*?'', where the *time interval* was the same as given in the past frequency statement.
In the *habitual endorsement* condition, participants were asked if they agreed or disagreed with the corresponding habitual sentence (as in Expt. 2b).


### Results

<!-- We analyze this data by building a simple Bayesian model of the measurement so that it may be integrated seamlessly into a joint model with the endorsement data (as we have done for all previous case studies). -->
<!-- As we assumed for the prior elicitation task (Expt. 2a), we assume participant's responses for each event $i$ and condition $c$ are generated from a log-normal distribution with unknown mean and variance. -->
<!-- $$ -->
<!-- \ln d_{ic} \sim \text{Gaussian}(\mu_{ic}, \sigma_{ic}) -->
<!-- $$ -->

#### Predictive frequency

Figure \ref{fig:habituals-predictive-figure}B shows the mean predicted future rate as a function of the past rate given to the participant and the type of causal information given. 
We observe in the baseline condition that future frequency perfectly tracks past frequency ($r(`r n.hab.pred.past.baseline.freq`) = `r format(r.hab.pred.past.baseline.freq, digits = 3)`$). 
That is, participants believe if a person smoked cigarettes 3 times last month, they will smoke cigarettes 3 times next month. 
This result implies that our model makes identical predictions for Expt. 2b whether the referent is past frequency or expected future frequency (indicating, as expected, that we must look to the new data to distinguish these models).
Critically, we observe the preventative information strongly decreases and the enabling information slightly increases predicted frequency (Figure \ref{fig:habituals-predictive-figure}B, yellow and purple dots).


```{r habituals-predictive-elicitation-predictions, eval=F}
lmer.rs.hab.predictive <- lmer(data = d.hab.predictive, 
     logAnnualPredictiveRate ~ logAnnualPastRate + condition + 
       (1 + condition | workerid) + 
       (1 + condition | item)
)
lmer.rs.hab.predictive.summary <- summary(lmer.rs.hab.predictive)
```


We confirm these observations using a linear mixed-effects model, predicting the log-transformed responses from the log-transformed past frequency and the experimental condition (baseline, preventative, enabling).
To account for participant and item variability in this analysis, we also include random effects of intercept and condition for both participants and items.
Confirming that our manipulation worked as intended, the preventative information led to significantly lower predictions for future frequency, relative to the baseline condition (
$\beta = `r round(lmer.rs.hab.predictive.summary[["coefficients"]]["conditionpreventative","Estimate"],2)`$;
$SE = `r round(lmer.rs.hab.predictive.summary[["coefficients"]]["conditionpreventative","Std. Error"],2)`;$
$t = `r round(lmer.rs.hab.predictive.summary[["coefficients"]]["conditionpreventative","t value"],2)`$
).
There was also tendency for the enabling information to lead to higher predictions for future frequency, relative to baseline (
$\beta = `r round(lmer.rs.hab.predictive.summary[["coefficients"]]["conditionenabling","Estimate"],2)`$;
$SE = `r round(lmer.rs.hab.predictive.summary[["coefficients"]]["conditionenabling","Std. Error"],2)`;$
$t = `r round(lmer.rs.hab.predictive.summary[["coefficients"]]["conditionenabling","t value"],2)`$
).
Finally, past frequency was a significant predictor of predicted future frequency (
$\beta = `r round(lmer.rs.hab.predictive.summary[["coefficients"]]["logAnnualPastRate","Estimate"],2)`$;
$SE = `r round(lmer.rs.hab.predictive.summary[["coefficients"]]["logAnnualPastRate","Std. Error"],2)`;$
$t = `r round(lmer.rs.hab.predictive.summary[["coefficients"]]["logAnnualPastRate","t value"],2)`$
).

#### Habitual endorsement

There is a clear and consistent negative effect of preventative information on endorsements for the habitual sentence (Figure \ref{fig:habituals-predictive-figure}B; green points).
When collapsing across items, the Bayesian Maximum A-Posteriori estimate and 95\% highest probability density interval for the true endorsement probabilities per condition are: baseline = 
`r round(d.hab.predictive.endorsment.baseline.summary[[1,"MAP_h"]], 2)` [`r round(d.hab.predictive.endorsment.baseline.summary[[1,"low"]], 2)`,
`r round(d.hab.predictive.endorsment.baseline.summary[[1,"high"]], 2)`
], enabling = 
`r round(d.hab.predictive.endorsment.enabling.summary[[1,"MAP_h"]], 2)` [`r round(d.hab.predictive.endorsment.enabling.summary[[1,"low"]], 2)`,
`r round(d.hab.predictive.endorsment.enabling.summary[[1,"high"]], 2)`
], preventative = `r round(d.hab.predictive.endorsment.preventative.summary[[1,"MAP_h"]], 2)` [`r round(d.hab.predictive.endorsment.preventative.summary[[1,"low"]], 2)`,
`r round(d.hab.predictive.endorsment.preventative.summary[[1,"high"]], 2)`
].
Still, frequency --- even predictive frequency --- does not perfectly explain the endorsements ($r^2(`r r2.hab.rsa.predictive.n`) = `r r2.habituals.predictiveFreq`$; MSE = $`r mse.habituals.predictiveFreq`$). 

<!-- Interestingly, we observe endorsements in this experiment that are appreciably higher than in Expt. 2 for the same items (Figure \ref{fig:tj3}, middle; red vs. purple points). -->
<!-- This may be due, in part, to an effect of the experimental context on participants: -->
<!-- in this experiment the overall population of frequencies is much lower (both because we selected moderate frequencies from Expt. 2 and because of the preventative information) and participants may infer that the experimenter believes this to be a representative range and adjust judgments accordingly. -->
<!-- Future investigation into this issue is warranted. -->

We use our formal model to test whether past or predictive frequency matters for endorsement.
To formalize the predictive frequency speaker model, we use the mean predictive frequency as the referent-prevalence $p_{fk}$ that the endorsement model (Eq. \ref{eq:S1}) aims to convey. 
The past frequency model is constructed using the past frequency supplied to participants as the referent-prevalence.
We analyze this model in the same Bayesian data analysis regime as for our previous models.
We use the same priors over the parameters as before and learn about the posterior distribution by collecting three independent MCMC chains of 100,000 iterations (removing the first 50,000 for burn-in). 
Figure \ref{fig:habituals-predictive-figure}C shows the resulting model predictions for the past frequency and the predictive frequency endorsement models. 
Participants' judgments of the habitual statements was indeed influenced by the causal manipulations in the way predicted by the endorsement model that uses the predictive frequency as the referent prevalence ($r^2(`r r2.hab.rsa.predictive.n`) = `r r2.habituals.rsa.predictive`$; MSE = $`r mse.habituals.rsa.predictive`$).
The model based on past frequency does not make different predictions for the different causal manipulation conditions and does a poor job at explaining the endorsements ($r^2(`r r2.hab.rsa.predictive.n`) = `r r2.habituals.rsa.past`$; MSE = $`r mse.habituals.rsa.past`$). 


<!-- We use the mean predicted log frequency from Expt. 3a as the input to the speaker $S_1$ model to predict the felicity judgments measured in Expt. 3b. -->
<!-- We infer the two model parameters using the same analysis approach in Expt. 2.  -->
<!-- The model matches the data well ($r^2(63) = 0.91$; Figure \ref{fig:tj3}, right). -->
<!-- The same model using the past frequency as the object of communication does not match the data at all ($r^2(63) = 0.02$). -->
<!-- These results suggest that the felicity of habituals is based on an underlying scale of predicted future propensity, not merely the observed frequency in the past. -->



## Discussion

Habitual language conveys generalizations about events.
Our model decides if a habitual sentence is a pragmatically useful way to describe the rate at which a person does an action, taking into account a naive interpreter's prior beliefs about the event (measured in Expt. 2a). 
Our computational model endorses statements that communicate generalizations about events with the same sensitivity to context and frequency that people exhibit (Expt. 2b \& c).
In Expt. 2b, we varied the type of event and the past frequency with which the person did the action, and found graded endorsements of the corresponding habitual sentences.
By manipulating (rather than measuring) the referent frequency, we showed how alternative models were unable to account for the gradience in endorsement.
In particular, we show that prior knowledge in an information-theoretic, communicative model is not sufficient to produce gradience in endorsement: The fixed-threshold model, which has these components, does not make different predictions for different frequencies.
Only our uncertain threshold model was able to precisely account for the wide range of endorsements.

<!-- Naive baseline models could not explain these endorsements judgments and our communicative model provided the most parsimonious account of the data in comparison to non-Bayesian models.  -->
<!-- In Experiment 3, we looked at interpretations of habitual statements about different events and again found substantial gradedness that our theory provided the most parsimonious account of. -->
<!-- The theory defines a pair of interpretation and endorsement models and these models can simultaneously account for both data sets. -->
In Expt. 2c, we further investigated the nature of the underlying prevalence scale by introducing causal information that enabled and prevented future occurrences of the action.
We used the empirically-measured predicted future frequency as the object of communication for our endorsement model.
We found that the endorsement model that seeks to communicate its predictions (rather than its observations) is a better model of habitual endorsements under these situations.
That is, habitual language (and generalization language more generally) is fundementally about conveying people's predictive beliefs, not what has actually happened.

<!-- To our knowledge, the experiments presented here are first empirical investigations into endorsements of habitual sentences and the first test of a formal psychological model of such generalizations in language. -->

In these experiments, we introduced participants to novel actors and, by doing so, were able to directly manipulate the frequency of a person doing the action.
The kinds of events we used were familiar to participants (e.g., *running*) and thus we measured the prevalence priors for those events.
In our final case study, we experimentally manipulate the prevalence priors, testing their causal influence over endorsements.
In addition, we further we extend our theory to the language of causal relationships.

<!-- Thus, it is still possible that the feature-probability priors that are central to our communicative theory of generalizations are somehow epiphenomenal, correlationally associated with generic and habitual endorsement but not directly causally related. -->

<!-- To anticipate our results, we find that only a model that uses the *predictive* feature-probabilities as the object of communication is able to explain the endorsement data, thus tying the phenomenology of *genericity* even more to subjective beliefs. -->


<!-- Habituals are a particularly convenient case study for manipulating referent feature-probability. -->
<!-- In the domain -->

<!-- From a methodological standpoint, habituals are a particularly convenient domain for manipulating the referent feature-probability that a speaker aims to communicate.^[ -->
<!--   While referent feature-probability can be manipulated for generalizations about categories (e.g., by stating the prevalence of feature in a kind), observers may have strongly constraining domain knowledge that heavily guides what feature-probabilities are likely (e.g., it's *a priori* very unlikely that 90% of a hypothetical novel kind---lorches---have broken wings). -->
<!-- ] -->
<!-- Due to the sociality of our species, we are often in the situation of being introduced to new people and learning new information about them. -->
<!-- In addition, feature-probabilities of people doing actions can be expressed in an easily-interpretable rate information (e.g., "Last month, Mary smoked cigarettes 3 times.").^[ -->
<!-- In this case study, we use frequencies or rates of people doing actions (e.g., *smoking cigarettes 3 times a month*) as our feature-probabilities for ease of interpretability.  -->
<!-- A rate can be converted into a probability of the event occuring in some time window by computing the area under the curve of a Poisson distribution parametrized by the rate. -->
<!-- ] -->





